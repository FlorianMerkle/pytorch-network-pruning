{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7068495e",
   "metadata": {},
   "source": [
    "# Adversarial-Aware Pruning Experiment\n",
    "Inspired by our own work on the adversarial robustness of pruned neural networks, we try to find subnets that are less prone to adversarial attacks then their unpruned counterparts. Therefore it is sensible to choose a selection criterium that is related to adversarial attacks. We could therefore naively prune the neurons that have a high activation for adversarial examples, or prune the weights that have high gradients when passing adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111379bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.custom_modules import MaskedLinearLayer,MaskedConvLayer\n",
    "from src.data_loader import load_torchvision_dataset\n",
    "from src.training import _fit\n",
    "from src.helpers import _evaluate_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551fbcf",
   "metadata": {},
   "source": [
    "# Create prunable Model and conduct standard training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c35b2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer with pruning mask\"\"\"\n",
    "    def __init__(self, shape, bias=True, activation='relu'):\n",
    "        super(MaskedLinearLayer, self).__init__()\n",
    "        self.b, self.a = bias, activation\n",
    "        weights = torch.empty(shape)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        mask = torch.ones(shape)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        if self.b == True:\n",
    "            bias = torch.zeros(self.weights.shape[-1])\n",
    "            self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        \n",
    "        self.activations = torch.zeros((shape[1]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = torch.mm(inputs, self.weights*self.mask)\n",
    "        if self.b == True:\n",
    "            x = torch.add(x, self.bias)\n",
    "        if self.a == 'relu':\n",
    "            x = F.relu(x)\n",
    "        self.activations = x\n",
    "        return x\n",
    "\n",
    "class MaskedConvLayer(nn.Module):\n",
    "    \"\"\" Custom Conv layer with pruning mask\"\"\"\n",
    "    def __init__(self, shape, bias=True, stride=1, padding=0, activation=None):\n",
    "        super(MaskedConvLayer, self).__init__()\n",
    "        self.b, self.s, self.p, self.a = bias, stride, padding, activation\n",
    "        weights = torch.empty(shape)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        mask = torch.ones(shape)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "        if self.b == True:\n",
    "            bias = torch.zeros(self.weights.shape[0])\n",
    "            self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        \n",
    "        self.activations = torch.zeros((shape[1]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.conv2d(inputs, self.weights*self.mask, bias=None, stride=self.s, padding=self.p)\n",
    "        if self.b == True:\n",
    "            #reshape the bias\n",
    "            b = self.bias.reshape((1, self.bias.shape[0], 1,1))\n",
    "            x = torch.add(x, b)\n",
    "        if self.a =='relu':\n",
    "            x = F.relu(x)\n",
    "        self.activations = x\n",
    "        return x\n",
    "\n",
    "class AAP_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AAP_model, self).__init__()\n",
    "        self.c1 = MaskedConvLayer((6, 1, 3, 3), padding=0, activation='relu')\n",
    "        self.c2 = MaskedConvLayer((16, 6, 3, 3), padding=0, activation='relu')\n",
    "        self.p1 = nn.AvgPool2d(2)\n",
    "        self.p2 = nn.AvgPool2d(2)\n",
    "        self.l1 = MaskedLinearLayer((400,500))\n",
    "\n",
    "        self.l3 = MaskedLinearLayer((500,10))\n",
    "        self.train_stats = pd.DataFrame()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.c1(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "        x = x.view(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3])\n",
    "#        x = torch.flatten(x, 1)\n",
    "        x = self.l1(x)\n",
    "#        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, train_data, val_data, epochs, device, eps = 8/255, number_of_replays=7, patience=None, evaluate_robustness=False):\n",
    "        return _fit(self, train_data, val_data, epochs, device, patience=patience, evaluate_robustness=evaluate_robustness)\n",
    "    \n",
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.c1 = MaskedConvLayer((6, 1, 5, 5), padding=0, activation='relu')\n",
    "        self.c2 = MaskedConvLayer((16, 6, 5, 5), padding=0, activation='relu')\n",
    "        self.p1 = nn.AvgPool2d(2)\n",
    "        self.p2 = nn.AvgPool2d(2)\n",
    "        self.fc1 = MaskedLinearLayer((256, 128))\n",
    "        self.fc2 = MaskedLinearLayer((128, 84))\n",
    "        self.fc3 = MaskedLinearLayer((84, 10), activation=None)\n",
    "        self.conv_weights, self.conv_masks, self.fully_connected_weights, self.fully_connected_masks = None, None, None, None\n",
    "\n",
    "        self.train_stats = pd.DataFrame(columns=('epoch', 'train_loss', 'train_accuracy', 'validation_loss', 'validation_accuracy', 'duration', 'criterion', 'optimizer', 'method', 'learning_rate', 'batchsize'))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.c1(inputs)\n",
    "        x = self.p1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "        x = x.view(x.shape[0], x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b62b40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a9e7e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AAP_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b4d2f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_torchvision_dataset('MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7c133094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNIST_CNN' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1698014/787362364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/PT/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MNIST_CNN' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(train_dl, test_dl, 1, device,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a54c8",
   "metadata": {},
   "source": [
    "# Strip Down Training Function to Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5bb60dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration: 2.4367828369140625  - validation accuracy:  90.85  - validation loss:  0.3142783366143703\n",
      "duration: 2.390641927719116  - validation accuracy:  93.95  - validation loss:  0.19823764488101006\n",
      "duration: 2.401247024536133  - validation accuracy:  95.69  - validation loss:  0.13836825862526894\n",
      "duration: 2.4281771183013916  - validation accuracy:  96.57  - validation loss:  0.10654706154018641\n",
      "duration: 2.379926919937134  - validation accuracy:  97.09  - validation loss:  0.09151599630713463\n",
      "duration: 2.380399465560913  - validation accuracy:  97.65  - validation loss:  0.07506840350106359\n",
      "duration: 2.3980917930603027  - validation accuracy:  97.86  - validation loss:  0.07229888085275889\n",
      "duration: 2.4158570766448975  - validation accuracy:  97.74  - validation loss:  0.06877827066928148\n",
      "duration: 2.3753881454467773  - validation accuracy:  98.1  - validation loss:  0.06101765092462301\n",
      "duration: 2.3956823348999023  - validation accuracy:  98.15  - validation loss:  0.059845586260780695\n",
      "duration: 2.4616262912750244  - validation accuracy:  98.47  - validation loss:  0.048426401987671855\n",
      "duration: 2.402012825012207  - validation accuracy:  98.54  - validation loss:  0.04731449098326266\n",
      "duration: 2.4006049633026123  - validation accuracy:  98.47  - validation loss:  0.04735684380866587\n",
      "duration: 2.387551784515381  - validation accuracy:  98.65  - validation loss:  0.04385656737722456\n",
      "duration: 2.3905465602874756  - validation accuracy:  98.64  - validation loss:  0.040259117959067225\n",
      "duration: 2.390471935272217  - validation accuracy:  98.59  - validation loss:  0.044636795041151346\n",
      "duration: 2.3930375576019287  - validation accuracy:  98.33  - validation loss:  0.053211139084305616\n",
      "duration: 2.4135172367095947  - validation accuracy:  98.75  - validation loss:  0.0412329453160055\n",
      "duration: 2.4451534748077393  - validation accuracy:  98.55  - validation loss:  0.04270935792592354\n",
      "duration: 2.3770532608032227  - validation accuracy:  98.77  - validation loss:  0.03909390470944345\n",
      "duration: 2.4142556190490723  - validation accuracy:  98.7  - validation loss:  0.039889122813474384\n",
      "duration: 2.456591844558716  - validation accuracy:  98.7  - validation loss:  0.03961177871096879\n",
      "duration: 2.387186288833618  - validation accuracy:  98.77  - validation loss:  0.04029096473241225\n",
      "duration: 2.4903862476348877  - validation accuracy:  98.59  - validation loss:  0.043964890064671636\n",
      "duration: 2.391777515411377  - validation accuracy:  98.51  - validation loss:  0.045532423187978566\n",
      "duration: 2.3962414264678955  - validation accuracy:  98.79  - validation loss:  0.038692002545576545\n",
      "duration: 2.4323813915252686  - validation accuracy:  98.76  - validation loss:  0.038722434191731735\n",
      "duration: 2.387608051300049  - validation accuracy:  98.8  - validation loss:  0.036118542589247224\n",
      "duration: 2.4374825954437256  - validation accuracy:  98.75  - validation loss:  0.038009165017865595\n",
      "duration: 2.4347636699676514  - validation accuracy:  98.76  - validation loss:  0.042391430423595014\n",
      "duration: 2.4599359035491943  - validation accuracy:  98.71  - validation loss:  0.041571017960086463\n",
      "duration: 2.3736250400543213  - validation accuracy:  98.79  - validation loss:  0.036589422583347186\n",
      "duration: 2.379741668701172  - validation accuracy:  98.63  - validation loss:  0.04424901534803212\n",
      "duration: 2.397245168685913  - validation accuracy:  98.68  - validation loss:  0.04016784947598353\n",
      "duration: 2.409945487976074  - validation accuracy:  98.82  - validation loss:  0.03861119218054228\n",
      "duration: 2.4062209129333496  - validation accuracy:  98.9  - validation loss:  0.03709819605574012\n",
      "duration: 2.3948237895965576  - validation accuracy:  98.68  - validation loss:  0.045343295019119975\n",
      "duration: 2.4106078147888184  - validation accuracy:  98.59  - validation loss:  0.04803072984213941\n",
      "duration: 2.5672028064727783  - validation accuracy:  98.66  - validation loss:  0.053310914500616494\n",
      "duration: 2.3977200984954834  - validation accuracy:  98.54  - validation loss:  0.051822734787128864\n",
      "duration: 2.4287967681884766  - validation accuracy:  98.8  - validation loss:  0.040061120456084606\n",
      "duration: 2.3885140419006348  - validation accuracy:  98.86  - validation loss:  0.04217357189045288\n",
      "duration: 2.39198899269104  - validation accuracy:  98.69  - validation loss:  0.04680028411094099\n",
      "duration: 2.504331111907959  - validation accuracy:  98.76  - validation loss:  0.043791949818842116\n",
      "duration: 2.3777806758880615  - validation accuracy:  98.86  - validation loss:  0.04175630870740861\n",
      "duration: 2.4228403568267822  - validation accuracy:  98.73  - validation loss:  0.05274773384444416\n",
      "duration: 2.4256646633148193  - validation accuracy:  98.9  - validation loss:  0.04286487969802692\n",
      "duration: 2.4767706394195557  - validation accuracy:  98.83  - validation loss:  0.042798189679160714\n",
      "duration: 2.478193998336792  - validation accuracy:  98.92  - validation loss:  0.04175127326598158\n",
      "duration: 2.3938980102539062  - validation accuracy:  98.76  - validation loss:  0.046848743443842975\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = AAP_model().to(device)\n",
    "model = MNIST_CNN().to(device)\n",
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    t0 = time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        if i>=i:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            batchsize = labels.size(0)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = 100 * correct / batchsize\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    t1 = time()\n",
    "    accuracy, loss = _evaluate_model(model, val_loader, device, criterion)\n",
    "    print('duration:', t1-t0,' - validation accuracy: ', accuracy,' - validation loss: ', loss)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b3ffeee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2559, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(val_loader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "adv_inputs = pgd_attack(model, inputs, labels, eps=16/255)\n",
    "optimizer.zero_grad()\n",
    "# forward + backward + optimize\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f9eaabbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 88,   8, 116,  82,  78,  77,  13,  17,  48,  91,  83,   4,  72, 129,\n",
       "         96, 149,  98, 139,  93, 134, 121,  87,  19,   9,  86, 109,   3, 144,\n",
       "         42, 141,  21, 127, 111,  14,  45, 110,   2, 146,  81, 114,  18,  22,\n",
       "         73,  99,  76,  41,  71, 132,  92,  46, 105,  12, 118,   7,  67,  97,\n",
       "         43,  52,  24, 123,  94, 104,  53, 136, 124,  68, 126, 137,  47, 145,\n",
       "        120, 115, 131,  89,  84, 117,  79,  16,  23, 128, 119, 122,  95,  37,\n",
       "         20,  27,   1, 106,  57, 133, 100, 113, 142,  66,  49,  51,  36,  85,\n",
       "         62,  32, 112,  90,  75,  11,  40,  80, 138, 140,   6,  26,  63,  31,\n",
       "         58,  28,  38, 143, 125,  44,  56,  70, 135, 130,  54,  15, 147,  74,\n",
       "         33,  61, 148,   0, 101,  69, 107,  10, 108,  35,   5,  25,  30,  65,\n",
       "         39,  29, 103,  59, 102,  34,  50,  64,  60,  55], device='cuda:0')"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_c1 = model.c1.weights.grad.flatten().abs().argsort(); s_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "12149d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([550, 551, 640, 641, 649, 648, 660, 661, 663, 662, 664, 665, 643, 642,\n",
       "        646, 647, 667, 666, 669, 668, 645, 644, 748, 749, 554, 555, 556, 557,\n",
       "        553, 552, 559, 558, 741, 740, 746, 747, 745, 744, 742, 743, 363, 362,\n",
       "        332, 333, 394, 395, 399, 398, 390, 391, 396, 397, 393, 392, 467, 466,\n",
       "        461, 460,  23,  22,  29,  28,  20,  21, 136, 137, 132, 133, 139, 138,\n",
       "        135, 134, 130, 131,  24,  25,  27,  26, 337, 336, 335, 334, 330, 331,\n",
       "        281, 280, 282, 283, 287, 286, 284, 285, 288, 289, 338, 339, 368, 369,\n",
       "        360, 361, 365, 364, 366, 367, 469, 468, 464, 465, 462, 463, 573, 571,\n",
       "        574, 577, 653, 578,  34, 575, 576, 572, 651,  36, 470, 652, 473, 475,\n",
       "        654, 657, 658,  31, 472, 579, 570, 471, 476,  30, 200, 659, 686, 477,\n",
       "        411, 655, 387, 346,  39,  37, 833, 416, 170, 685, 706, 681, 415, 700,\n",
       "         38, 294, 520, 752, 702, 156,  32, 800, 813, 117, 406, 820, 684, 597,\n",
       "        796, 413,  13, 729, 619, 754, 526, 804, 714, 150, 316, 487, 354, 417,\n",
       "        736, 542, 127, 689, 832, 600, 299,  96, 827,  77, 544, 806,  46, 327,\n",
       "        370, 610, 124, 812, 120,   9, 296, 722, 104, 176, 230, 715, 414, 400,\n",
       "        810, 710,  57, 412, 145, 831, 386, 478,  82, 767, 418, 502, 162, 381,\n",
       "        291, 540, 680, 547, 505, 507, 617, 815, 814, 534, 341, 807,  42, 220,\n",
       "        433, 703, 705, 510, 719,  70, 836, 683, 687, 123, 147, 780, 298, 353,\n",
       "        688, 410, 694, 769, 631, 197,  40, 490,  80,  90, 179, 782, 790,  10,\n",
       "        431, 310, 824, 307,  85, 454, 727, 770, 357, 182, 389, 726, 227,  52,\n",
       "        838, 697, 207, 650, 656, 419, 434, 839, 560, 340, 206, 479, 277, 215,\n",
       "        219, 599, 358, 247, 817, 730, 427, 451, 734,  59, 835, 214, 760, 493,\n",
       "        795, 457, 300, 384, 290, 237, 675, 194, 787, 343, 320, 376, 272,  75,\n",
       "        180,  67, 682, 474, 232, 203, 404, 494, 192, 377, 199, 440, 344,  19,\n",
       "        224,   4, 380, 709, 516, 522, 830, 484, 485, 630, 236,  61, 496, 213,\n",
       "        324, 122, 594, 125, 171,  43, 322, 525, 112,  12, 195, 100, 345, 242,\n",
       "         47, 721, 422, 297, 639, 797,  35,  33, 716, 437, 107, 530, 314, 420,\n",
       "        750, 707, 565,  68, 482, 604,  73, 819, 549, 757, 210, 713, 312, 222,\n",
       "        442, 359, 720, 751, 497, 432, 786, 635, 480, 699, 602, 446, 580,  74,\n",
       "        517, 421, 388, 254,  49, 178, 724, 582, 164,  87, 533, 217,  62, 537,\n",
       "        781,  15, 837, 106, 372, 355, 692,  45, 174, 774, 536, 252, 250, 504,\n",
       "        723, 834, 293,  83, 612, 809, 382, 257, 244, 193,  14, 670, 245, 385,\n",
       "        532, 295, 383, 159, 737, 586,  84, 175,  69, 777, 634, 512, 829, 140,\n",
       "        119, 259, 500, 154, 160, 255, 531, 202, 348, 609, 243, 351, 121,  54,\n",
       "        292, 240, 349, 274, 495, 144,   0,  55, 677, 562, 606, 561,  63, 221,\n",
       "        262, 430, 793, 309, 590, 481, 235, 142, 728, 772, 499, 264, 260, 305,\n",
       "        110, 302, 152, 816, 211, 784, 535, 636, 614, 114, 347, 425,  60, 523,\n",
       "        186, 632, 356, 764, 545, 225, 725, 109, 509, 270, 304, 205, 731, 350,\n",
       "         44, 674, 452,  94, 637, 566, 326, 624, 633, 672, 352, 679,  17,  97,\n",
       "        696, 564, 218, 486, 126, 407, 762, 792, 435, 102, 129,  64, 279, 342,\n",
       "        315, 690, 755, 753, 313, 805, 321, 567, 450,  65, 190, 785, 256,  50,\n",
       "        187, 591, 311,  95, 155, 251, 216, 776, 759,  76,  89, 732, 405, 543,\n",
       "        436, 673, 444, 627, 184, 229, 458, 424, 717, 783, 607, 712,  72,  53,\n",
       "        803, 613, 638, 514, 172, 373, 459, 629, 143, 271, 488, 563, 584, 489,\n",
       "        620, 691, 167, 802, 587, 527, 592,   2, 766, 234, 622, 439, 483,  16,\n",
       "        153, 826, 111, 161, 248, 447, 822, 456, 429, 173, 704, 266, 149, 811,\n",
       "        169, 258, 616,  41, 317, 615, 775, 596, 212, 249, 511, 789, 539,  92,\n",
       "          7, 763, 449, 177, 538, 325, 128, 226, 402, 267, 621,  93, 541, 515,\n",
       "        223, 423, 698, 105, 492, 735, 401, 794, 253, 204, 371, 157, 801, 529,\n",
       "        374, 788, 524,  48, 756, 546, 201, 329, 375,  99, 513, 278, 671, 101,\n",
       "        196,  71, 568, 233, 569, 141, 103, 275, 438,   6, 598, 799, 765, 108,\n",
       "        328, 246, 779,  79, 595, 208, 241, 453, 693, 191, 503, 708, 818, 676,\n",
       "        589, 276, 269, 228, 198, 261, 601, 626, 548, 165,   5, 209, 758, 158,\n",
       "         86, 426, 118, 519, 239, 146,   1, 455, 625, 738,  66, 733, 603, 166,\n",
       "        695, 605, 303, 739, 323, 521, 825, 445, 403, 115, 798, 306, 518, 273,\n",
       "        151, 498, 701, 506,  78,  81,   8, 379, 116, 585, 265, 428,   3,  56,\n",
       "        528, 318, 301, 189, 773, 791, 808,  11, 593, 491, 409, 581, 319,  88,\n",
       "         98,  51, 408, 163, 185, 718, 181, 443, 501,  18, 148, 441, 263,  91,\n",
       "        711, 588, 821, 823, 611, 448, 678, 583, 268, 113, 308, 623, 771, 378,\n",
       "        608, 231, 761, 768, 628, 168, 183, 508, 238, 828,  58, 618, 188, 778],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_fc3 = model.fc3.weights.grad.flatten().abs().argsort(); s_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e49571fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2617, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "adv_inputs = pgd_attack(model, inputs, labels, eps=16/255)\n",
    "optimizer.zero_grad()\n",
    "# forward + backward + optimize\n",
    "outputs = model(adv_inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0792aab8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 13,  88,  78,  17,  83,   8,  91, 109, 121, 111, 118,  98, 146,  82,\n",
       "         77,  93, 149,  96,  72,  48,  86, 139,   4,  19,   9, 123, 116,  21,\n",
       "        144, 134,  87, 129,  81,   2,  42,  14,   3,  76,  45, 105, 127,  73,\n",
       "         71, 104,  99, 141, 117, 110,  67,  18,  12,  46,  22,  52,   7,  92,\n",
       "        132, 122,  41,  97,  24,  43, 126,  53,  68, 137,  94, 113, 136, 114,\n",
       "        131,  47,  16,  79,  84, 112, 100, 106,  89, 115,  23, 120, 142,  95,\n",
       "        145,  20,  57, 128,  66,  62,  27,  51,   1, 124,  49,  75,  85,  37,\n",
       "        133,  80,  11, 138,  36,  90,  26,  32,   6,  40, 119,  63,  58, 143,\n",
       "        147,  31,  56, 140, 125, 108, 107,  28,  61, 148, 101,  15,  38,  54,\n",
       "         70,  44, 130,  74, 135,   0,  33,  10,  69,  25,   5,  35, 103,  65,\n",
       "         30, 102,  39,  59,  29,  50,  64,  34,  60,  55], device='cuda:0')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_c1 = model.c1.weights.grad.flatten().abs().argsort(); a_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "aa0a4ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([550, 551, 640, 641, 649, 648, 660, 661, 663, 662, 664, 665, 643, 642,\n",
       "        646, 647, 667, 666, 669, 668, 645, 644, 748, 749, 554, 555, 556, 557,\n",
       "        553, 552, 559, 558, 741, 740, 746, 747, 745, 744, 742, 743, 363, 362,\n",
       "        332, 333, 394, 395, 399, 398, 390, 391, 396, 397, 393, 392, 467, 466,\n",
       "        461, 460,  23,  22,  29,  28,  20,  21, 136, 137, 132, 133, 139, 138,\n",
       "        135, 134, 130, 131,  24,  25,  27,  26, 337, 336, 335, 334, 330, 331,\n",
       "        281, 280, 282, 283, 287, 286, 284, 285, 288, 289, 338, 339, 368, 369,\n",
       "        360, 361, 365, 364, 366, 367, 469, 468, 464, 465, 462, 463, 573, 571,\n",
       "        574, 577, 653, 578,  34, 575, 576, 572,  36, 651, 652, 473, 475, 470,\n",
       "        654, 657, 658,  31, 579, 570, 472, 471,  30, 476, 200, 659, 686, 477,\n",
       "        411, 655, 387, 346, 833,  39, 416, 706, 685, 700, 681,  37, 415, 170,\n",
       "        813, 502, 714,  38, 520, 752, 162, 156, 702, 413, 800,  32, 294, 736,\n",
       "        820, 684,  13, 796, 729, 354, 117, 406, 150, 316, 526, 487, 600, 417,\n",
       "        127,  46, 299, 804, 434,  96, 599, 507, 832,  77, 145, 597, 544, 370,\n",
       "        542, 120, 418, 812, 296, 689, 806, 291, 414, 827, 412, 610, 715, 176,\n",
       "        810, 710, 230, 540, 722, 478, 619,  57,  82, 327, 754, 104, 680, 703,\n",
       "        381, 386, 547, 767, 433,  42, 814, 197, 705, 400, 341, 534, 688, 719,\n",
       "        410, 358, 182, 220, 510, 807, 617, 683, 227, 831,  70, 697, 687, 215,\n",
       "        780, 815, 675,  90, 353, 179, 836,  80, 431,  52, 224, 490, 310, 839,\n",
       "        123,  40, 307,   9, 790, 505, 147, 838, 207,  10, 726, 277, 769, 650,\n",
       "        656, 782, 419, 727, 770, 560, 427, 389, 219, 206, 493, 340, 247, 479,\n",
       "        290, 357, 787, 760, 835, 730, 631, 254, 298, 214, 300, 817, 384, 451,\n",
       "        237, 682, 343, 522, 795, 324, 180, 377, 320, 272, 124, 376, 594, 203,\n",
       "         67, 194, 474,   4,  85, 440, 457, 485, 494, 454, 344, 192, 199, 380,\n",
       "        322, 484,  75, 830, 824,  61,  19, 630, 232, 694, 236, 516, 171, 122,\n",
       "        100, 125, 819, 107, 442,  47, 345, 213, 639, 829, 297, 496, 112, 525,\n",
       "         12,  35,  33, 750, 242, 482, 530,  43, 721, 437, 244, 797, 420, 716,\n",
       "        195, 707, 517, 222, 720, 565, 210,  59, 497, 549, 709, 217, 582, 635,\n",
       "        432, 359, 713, 480, 786, 312, 580, 422, 724, 252, 355, 781, 809, 692,\n",
       "         62, 164, 757,  73, 388, 734, 751, 446, 144, 609, 250, 723, 533,  87,\n",
       "        699, 602,  14, 257, 106,  15, 537, 837,  45,  68, 372, 536, 532, 178,\n",
       "        834, 174, 385, 193, 295, 670, 777, 512,  83,  49, 382, 293, 612, 383,\n",
       "        140, 604, 634,  55, 245, 504,  84, 175, 404, 274, 255, 500, 421, 202,\n",
       "        160, 531, 586, 159, 737, 259, 674,  69, 262, 562, 240,   0, 349, 545,\n",
       "        495, 154, 677, 679, 351, 221, 606, 142,  54, 243, 305, 260, 561, 225,\n",
       "        348, 292, 430, 590, 119, 535,  63, 509, 152, 110, 235, 632, 114, 499,\n",
       "         60, 302, 121, 304,  74, 816, 624, 270, 725, 755, 636, 347, 350, 356,\n",
       "        731, 211, 637, 728, 452,  17, 481, 205, 109, 186, 784,  97, 793, 772,\n",
       "        633, 774, 523, 352, 102, 566, 792,  64, 435, 696, 309, 326, 672, 315,\n",
       "        342, 762, 458, 126, 129, 425, 407, 218, 486, 690, 187, 314, 764, 753,\n",
       "        759, 805, 450, 313, 785, 321, 311, 567,  50, 614,  95, 190, 279,  44,\n",
       "        155, 256, 629, 543,  65, 216, 229, 424, 591,  76, 717, 776, 172,  94,\n",
       "        607, 405,  72, 564, 627, 436,  89, 673, 783, 732, 712,  53, 251, 803,\n",
       "        373, 459, 264, 587, 429, 143, 489, 620, 169, 613, 167, 488,   2, 563,\n",
       "        444, 271, 638, 691, 527, 149, 249, 802, 447, 766, 258, 538, 439, 592,\n",
       "        153, 826, 483, 615,  16, 266, 456, 822, 775, 622, 111, 212, 128, 616,\n",
       "        248, 161, 173,  92, 515, 449, 596,  41, 811,   7, 317, 789, 539, 698,\n",
       "        511, 325, 267, 763, 177, 514, 704, 105, 226,  93, 402, 423, 492, 735,\n",
       "        223, 788, 541, 584, 278,  48, 234, 184, 621, 438, 371, 329, 401, 157,\n",
       "        375, 253, 328, 529, 568, 208,  99, 546, 598, 756, 275, 201, 801, 513,\n",
       "        794, 196, 524, 101, 765,  79, 103, 671, 233, 228,   6, 569, 779, 108,\n",
       "         71, 141, 595, 799, 246, 374, 758, 453, 693, 269, 818, 241, 738, 518,\n",
       "        191, 708, 503, 204, 165, 676, 118, 589, 276, 548, 601, 261, 455, 626,\n",
       "        158,   5, 519, 198, 426,  86, 625, 209, 239, 146,  78, 695,   1,  66,\n",
       "        733, 115, 605, 318, 166, 739, 603, 825, 445, 798, 323, 303, 521, 403,\n",
       "        306, 379, 265, 151, 273, 506, 585,   8, 428, 701, 498, 116,  81, 189,\n",
       "         56,   3, 528, 773, 408, 808,  98, 409, 301, 791, 593, 491, 581, 588,\n",
       "        319,  88, 185,  11, 148, 163, 268,  51, 181, 443, 263,  91, 441, 501,\n",
       "        378, 718, 678, 711,  18, 448, 821, 823, 608, 308, 611, 583, 113, 623,\n",
       "        168, 771, 768, 628, 761, 231, 183, 508, 238, 828, 188, 618,  58, 778],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_fc3 = model.fc3.weights.grad.flatten().abs().argsort();a_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "af3b2bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3153, device='cuda:0')"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((a_c1-s_c1)/len(a_c1)).abs().sum()/len(a_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "3b85c4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2682, device='cuda:0')"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((a_fc3-s_fc3)/len(a_fc3)).abs()).sum()/len(a_fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ae87ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from foolbox import PyTorchModel, attacks\n",
    "def pgd_attack(model, images, labels, eps=8/255):\n",
    "    model.eval()\n",
    "    fmodel = PyTorchModel(model, bounds=(0, 1))\n",
    "    attack = attacks.LinfPGD()\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=eps)\n",
    "    model.train()\n",
    "    print(success.sum()/len(success))\n",
    "    return clipped_advs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "59ccec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2617, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "adv_inputs = pgd_attack(model, inputs, labels, eps=16/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e752b227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 13,  88,  78,  17,  83,   8,  91, 109, 121, 111, 118,  98, 146,  82,\n",
       "         77,  93, 149,  96,  72,  48,  86, 139,   4,  19,   9, 123, 116,  21,\n",
       "        144, 134,  87, 129,  81,   2,  42,  14,   3,  76,  45, 105, 127,  73,\n",
       "         71, 104,  99, 141, 117, 110,  67,  18,  12,  46,  22,  52,   7,  92,\n",
       "        132, 122,  41,  97,  24,  43, 126,  53,  68, 137,  94, 113, 136, 114,\n",
       "        131,  47,  16,  79,  84, 112, 100, 106,  89, 115,  23, 120, 142,  95,\n",
       "        145,  20,  57, 128,  66,  62,  27,  51,   1, 124,  49,  75,  85,  37,\n",
       "        133,  80,  11, 138,  36,  90,  26,  32,   6,  40, 119,  63,  58, 143,\n",
       "        147,  31,  56, 140, 125, 108, 107,  28,  61, 148, 101,  15,  38,  54,\n",
       "         70,  44, 130,  74, 135,   0,  33,  10,  69,  25,   5,  35, 103,  65,\n",
       "         30, 102,  39,  59,  29,  50,  64,  34,  60,  55], device='cuda:0')"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "e593d589",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'prod'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1698014/17833742.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'prod'"
     ]
    }
   ],
   "source": [
    "model.c1.mask.view(model.c1.mask.shape.prod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908ce840",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12981/1687379662.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.c1.mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d80c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
