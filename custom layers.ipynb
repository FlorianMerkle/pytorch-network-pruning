{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.models import ResNet, CNN\n",
    "from src.helpers import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "train_path='/Users/florianmerkle/Downloads/imagenette2-320/train'\n",
    "imagenette_train = torchvision.datasets.ImageFolder(\n",
    "    root=train_path,\n",
    "    transform=transforms\n",
    ")\n",
    "val_path='/Users/florianmerkle/Downloads/imagenette2-320/val'\n",
    "imagenette_val = torchvision.datasets.ImageFolder(\n",
    "    root=val_path,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(imagenette_train,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(imagenette_val,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2.89801\n",
      "[1,     1] test_accuracy: 0.06\n",
      "[1,     2] loss: 81.60330\n",
      "[1,     2] test_accuracy: 0.06\n",
      "[1,     3] loss: 39.95670\n",
      "[1,     3] test_accuracy: 0.12\n",
      "[1,     4] loss: 35.93241\n",
      "[1,     4] test_accuracy: 0.03\n",
      "[1,     5] loss: 17.38429\n",
      "[1,     5] test_accuracy: 0.22\n",
      "[1,     6] loss: 28.82063\n",
      "[1,     6] test_accuracy: 0.22\n",
      "[1,     7] loss: 32.61719\n",
      "[1,     7] test_accuracy: 0.22\n",
      "[1,     8] loss: 37.27567\n",
      "[1,     8] test_accuracy: 0.06\n",
      "[1,     9] loss: 30.90833\n",
      "[1,     9] test_accuracy: 0.09\n",
      "[1,    10] loss: 14.82520\n",
      "[1,    10] test_accuracy: 0.06\n",
      "[1,    11] loss: 14.69095\n",
      "[1,    11] test_accuracy: 0.12\n",
      "[1,    12] loss: 15.15463\n",
      "[1,    12] test_accuracy: 0.09\n",
      "[1,    13] loss: 13.24937\n",
      "[1,    13] test_accuracy: 0.12\n",
      "[1,    14] loss: 11.63730\n",
      "[1,    14] test_accuracy: 0.16\n",
      "[1,    15] loss: 11.95995\n",
      "[1,    15] test_accuracy: 0.12\n",
      "[1,    16] loss: 7.46261\n",
      "[1,    16] test_accuracy: 0.09\n",
      "[1,    17] loss: 11.29519\n",
      "[1,    17] test_accuracy: 0.06\n",
      "[1,    18] loss: 7.55978\n",
      "[1,    18] test_accuracy: 0.19\n",
      "[1,    19] loss: 11.01834\n",
      "[1,    19] test_accuracy: 0.19\n",
      "[1,    20] loss: 8.82500\n",
      "[1,    20] test_accuracy: 0.16\n",
      "[1,    21] loss: 12.78983\n",
      "[1,    21] test_accuracy: 0.19\n",
      "[1,    22] loss: 11.31503\n",
      "[1,    22] test_accuracy: 0.09\n",
      "[1,    23] loss: 7.46522\n",
      "[1,    23] test_accuracy: 0.12\n",
      "[1,    24] loss: 11.25437\n",
      "[1,    24] test_accuracy: 0.06\n",
      "[1,    25] loss: 9.38241\n",
      "[1,    25] test_accuracy: 0.16\n",
      "[1,    26] loss: 14.10714\n",
      "[1,    26] test_accuracy: 0.19\n",
      "[1,    27] loss: 8.52006\n",
      "[1,    27] test_accuracy: 0.19\n",
      "[1,    28] loss: 7.03608\n",
      "[1,    28] test_accuracy: 0.16\n",
      "[1,    29] loss: 9.12871\n",
      "[1,    29] test_accuracy: 0.28\n",
      "[1,    30] loss: 7.45833\n",
      "[1,    30] test_accuracy: 0.16\n",
      "[1,    31] loss: 6.25111\n",
      "[1,    31] test_accuracy: 0.09\n",
      "[1,    32] loss: 3.86708\n",
      "[1,    32] test_accuracy: 0.19\n",
      "[1,    33] loss: 3.03732\n",
      "[1,    33] test_accuracy: 0.28\n",
      "[1,    34] loss: 12.61059\n",
      "[1,    34] test_accuracy: 0.12\n",
      "[1,    35] loss: 7.31480\n",
      "[1,    35] test_accuracy: 0.16\n",
      "[1,    36] loss: 10.74798\n",
      "[1,    36] test_accuracy: 0.09\n",
      "[1,    37] loss: 3.18823\n",
      "[1,    37] test_accuracy: 0.16\n",
      "[1,    38] loss: 10.47415\n",
      "[1,    38] test_accuracy: 0.06\n",
      "[1,    39] loss: 4.67616\n",
      "[1,    39] test_accuracy: 0.16\n",
      "[1,    40] loss: 2.91597\n",
      "[1,    40] test_accuracy: 0.16\n",
      "[1,    41] loss: 5.54440\n",
      "[1,    41] test_accuracy: 0.22\n",
      "[1,    42] loss: 3.26076\n",
      "[1,    42] test_accuracy: 0.25\n",
      "[1,    43] loss: 6.12245\n",
      "[1,    43] test_accuracy: 0.06\n",
      "[1,    44] loss: 3.14417\n",
      "[1,    44] test_accuracy: 0.12\n",
      "[1,    45] loss: 3.08786\n",
      "[1,    45] test_accuracy: 0.19\n",
      "[1,    46] loss: 5.64353\n",
      "[1,    46] test_accuracy: 0.09\n",
      "[1,    47] loss: 7.77543\n",
      "[1,    47] test_accuracy: 0.09\n",
      "[1,    48] loss: 5.13306\n",
      "[1,    48] test_accuracy: 0.09\n",
      "[1,    49] loss: 5.79273\n",
      "[1,    49] test_accuracy: 0.19\n",
      "[1,    50] loss: 9.31353\n",
      "[1,    50] test_accuracy: 0.16\n",
      "[1,    51] loss: 2.63550\n",
      "[1,    51] test_accuracy: 0.28\n",
      "[1,    52] loss: 4.10073\n",
      "[1,    52] test_accuracy: 0.16\n",
      "[1,    53] loss: 3.62377\n",
      "[1,    53] test_accuracy: 0.12\n",
      "[1,    54] loss: 2.75473\n",
      "[1,    54] test_accuracy: 0.19\n",
      "[1,    55] loss: 3.52458\n",
      "[1,    55] test_accuracy: 0.12\n",
      "[1,    56] loss: 5.30375\n",
      "[1,    56] test_accuracy: 0.25\n",
      "[1,    57] loss: 2.75559\n",
      "[1,    57] test_accuracy: 0.25\n",
      "[1,    58] loss: 5.85760\n",
      "[1,    58] test_accuracy: 0.12\n",
      "[1,    59] loss: 3.16750\n",
      "[1,    59] test_accuracy: 0.06\n",
      "[1,    60] loss: 4.51309\n",
      "[1,    60] test_accuracy: 0.09\n",
      "[1,    61] loss: 4.50196\n",
      "[1,    61] test_accuracy: 0.06\n",
      "[1,    62] loss: 4.26133\n",
      "[1,    62] test_accuracy: 0.06\n",
      "[1,    63] loss: 2.56958\n",
      "[1,    63] test_accuracy: 0.12\n",
      "[1,    64] loss: 3.35829\n",
      "[1,    64] test_accuracy: 0.09\n",
      "[1,    65] loss: 4.37198\n",
      "[1,    65] test_accuracy: 0.12\n",
      "[1,    66] loss: 3.29839\n",
      "[1,    66] test_accuracy: 0.25\n",
      "[1,    67] loss: 3.81526\n",
      "[1,    67] test_accuracy: 0.31\n",
      "[1,    68] loss: 3.57815\n",
      "[1,    68] test_accuracy: 0.09\n",
      "[1,    69] loss: 2.86184\n",
      "[1,    69] test_accuracy: 0.12\n",
      "[1,    70] loss: 3.50250\n",
      "[1,    70] test_accuracy: 0.12\n",
      "[1,    71] loss: 3.48324\n",
      "[1,    71] test_accuracy: 0.19\n",
      "[1,    72] loss: 3.06966\n",
      "[1,    72] test_accuracy: 0.12\n",
      "[1,    73] loss: 2.43303\n",
      "[1,    73] test_accuracy: 0.34\n",
      "[1,    74] loss: 2.86593\n",
      "[1,    74] test_accuracy: 0.25\n",
      "[1,    75] loss: 2.45710\n",
      "[1,    75] test_accuracy: 0.19\n",
      "[1,    76] loss: 2.60183\n",
      "[1,    76] test_accuracy: 0.31\n",
      "[1,    77] loss: 2.41410\n",
      "[1,    77] test_accuracy: 0.28\n",
      "[1,    78] loss: 2.66565\n",
      "[1,    78] test_accuracy: 0.19\n",
      "[1,    79] loss: 3.05844\n",
      "[1,    79] test_accuracy: 0.22\n",
      "[1,    80] loss: 3.00295\n",
      "[1,    80] test_accuracy: 0.22\n",
      "[1,    81] loss: 3.71904\n",
      "[1,    81] test_accuracy: 0.19\n",
      "[1,    82] loss: 4.02367\n",
      "[1,    82] test_accuracy: 0.19\n",
      "[1,    83] loss: 4.78623\n",
      "[1,    83] test_accuracy: 0.38\n",
      "[1,    84] loss: 3.90387\n",
      "[1,    84] test_accuracy: 0.19\n",
      "[1,    85] loss: 4.58064\n",
      "[1,    85] test_accuracy: 0.16\n",
      "[1,    86] loss: 2.47978\n",
      "[1,    86] test_accuracy: 0.22\n",
      "[1,    87] loss: 2.88742\n",
      "[1,    87] test_accuracy: 0.22\n",
      "[1,    88] loss: 5.34791\n",
      "[1,    88] test_accuracy: 0.19\n",
      "[1,    89] loss: 2.71743\n",
      "[1,    89] test_accuracy: 0.16\n",
      "[1,    90] loss: 3.07894\n",
      "[1,    90] test_accuracy: 0.19\n",
      "[1,    91] loss: 2.59754\n",
      "[1,    91] test_accuracy: 0.34\n",
      "[1,    92] loss: 4.74766\n",
      "[1,    92] test_accuracy: 0.12\n",
      "[1,    93] loss: 3.87849\n",
      "[1,    93] test_accuracy: 0.22\n",
      "[1,    94] loss: 5.33379\n",
      "[1,    94] test_accuracy: 0.16\n",
      "[1,    95] loss: 3.56761\n",
      "[1,    95] test_accuracy: 0.16\n",
      "[1,    96] loss: 2.91275\n",
      "[1,    96] test_accuracy: 0.25\n",
      "[1,    97] loss: 2.97747\n",
      "[1,    97] test_accuracy: 0.28\n",
      "[1,    98] loss: 2.74411\n",
      "[1,    98] test_accuracy: 0.31\n",
      "[1,    99] loss: 2.99622\n",
      "[1,    99] test_accuracy: 0.09\n",
      "[1,   100] loss: 2.43583\n",
      "[1,   100] test_accuracy: 0.25\n",
      "[1,   101] loss: 3.58416\n",
      "[1,   101] test_accuracy: 0.19\n",
      "[1,   102] loss: 5.30445\n",
      "[1,   102] test_accuracy: 0.06\n",
      "[1,   103] loss: 3.45062\n",
      "[1,   103] test_accuracy: 0.28\n",
      "[1,   104] loss: 2.36010\n",
      "[1,   104] test_accuracy: 0.12\n",
      "[1,   105] loss: 3.70557\n",
      "[1,   105] test_accuracy: 0.16\n",
      "[1,   106] loss: 4.09747\n",
      "[1,   106] test_accuracy: 0.09\n",
      "[1,   107] loss: 3.29366\n",
      "[1,   107] test_accuracy: 0.12\n",
      "[1,   108] loss: 2.17715\n",
      "[1,   108] test_accuracy: 0.22\n",
      "[1,   109] loss: 2.77076\n",
      "[1,   109] test_accuracy: 0.09\n",
      "[1,   110] loss: 2.54451\n",
      "[1,   110] test_accuracy: 0.22\n",
      "[1,   111] loss: 2.58462\n",
      "[1,   111] test_accuracy: 0.19\n",
      "[1,   112] loss: 2.63885\n",
      "[1,   112] test_accuracy: 0.25\n",
      "[1,   113] loss: 3.75560\n",
      "[1,   113] test_accuracy: 0.12\n",
      "[1,   114] loss: 4.33442\n",
      "[1,   114] test_accuracy: 0.12\n",
      "[1,   115] loss: 3.35845\n",
      "[1,   115] test_accuracy: 0.25\n",
      "[1,   116] loss: 2.09012\n",
      "[1,   116] test_accuracy: 0.25\n",
      "[1,   117] loss: 4.36065\n",
      "[1,   117] test_accuracy: 0.16\n",
      "[1,   118] loss: 4.48355\n",
      "[1,   118] test_accuracy: 0.16\n",
      "[1,   119] loss: 2.76047\n",
      "[1,   119] test_accuracy: 0.09\n",
      "[1,   120] loss: 2.71907\n",
      "[1,   120] test_accuracy: 0.19\n",
      "[1,   121] loss: 4.91646\n",
      "[1,   121] test_accuracy: 0.19\n",
      "[1,   122] loss: 3.63125\n",
      "[1,   122] test_accuracy: 0.25\n",
      "[1,   123] loss: 3.34366\n",
      "[1,   123] test_accuracy: 0.09\n",
      "[1,   124] loss: 3.17676\n",
      "[1,   124] test_accuracy: 0.19\n",
      "[1,   125] loss: 3.66954\n",
      "[1,   125] test_accuracy: 0.16\n",
      "[1,   126] loss: 3.67049\n",
      "[1,   126] test_accuracy: 0.22\n",
      "[1,   127] loss: 3.41595\n",
      "[1,   127] test_accuracy: 0.19\n",
      "[1,   128] loss: 3.73218\n",
      "[1,   128] test_accuracy: 0.16\n",
      "[1,   129] loss: 2.70643\n",
      "[1,   129] test_accuracy: 0.19\n",
      "[1,   130] loss: 3.78414\n",
      "[1,   130] test_accuracy: 0.09\n",
      "[1,   131] loss: 2.64710\n",
      "[1,   131] test_accuracy: 0.22\n",
      "[1,   132] loss: 3.45537\n",
      "[1,   132] test_accuracy: 0.19\n",
      "[1,   133] loss: 2.63395\n",
      "[1,   133] test_accuracy: 0.31\n",
      "[1,   134] loss: 2.74590\n",
      "[1,   134] test_accuracy: 0.22\n",
      "[1,   135] loss: 3.47750\n",
      "[1,   135] test_accuracy: 0.19\n",
      "[1,   136] loss: 3.60695\n",
      "[1,   136] test_accuracy: 0.16\n",
      "[1,   137] loss: 3.10591\n",
      "[1,   137] test_accuracy: 0.25\n",
      "[1,   138] loss: 3.58026\n",
      "[1,   138] test_accuracy: 0.06\n",
      "[1,   139] loss: 2.54477\n",
      "[1,   139] test_accuracy: 0.28\n",
      "[1,   140] loss: 2.64263\n",
      "[1,   140] test_accuracy: 0.22\n",
      "[1,   141] loss: 2.65895\n",
      "[1,   141] test_accuracy: 0.16\n",
      "[1,   142] loss: 3.17767\n",
      "[1,   142] test_accuracy: 0.19\n",
      "[1,   143] loss: 2.23851\n",
      "[1,   143] test_accuracy: 0.12\n",
      "[1,   144] loss: 2.23036\n",
      "[1,   144] test_accuracy: 0.22\n",
      "[1,   145] loss: 2.93828\n",
      "[1,   145] test_accuracy: 0.09\n",
      "[1,   146] loss: 2.69275\n",
      "[1,   146] test_accuracy: 0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   147] loss: 3.26933\n",
      "[1,   147] test_accuracy: 0.12\n",
      "[1,   148] loss: 2.90882\n",
      "[1,   148] test_accuracy: 0.16\n",
      "[1,   149] loss: 2.30887\n",
      "[1,   149] test_accuracy: 0.16\n",
      "[1,   150] loss: 3.18380\n",
      "[1,   150] test_accuracy: 0.19\n",
      "[1,   151] loss: 2.20584\n",
      "[1,   151] test_accuracy: 0.22\n",
      "[1,   152] loss: 3.23148\n",
      "[1,   152] test_accuracy: 0.22\n",
      "[1,   153] loss: 2.84157\n",
      "[1,   153] test_accuracy: 0.28\n",
      "[1,   154] loss: 2.65667\n",
      "[1,   154] test_accuracy: 0.16\n",
      "[1,   155] loss: 2.26723\n",
      "[1,   155] test_accuracy: 0.16\n",
      "[1,   156] loss: 2.15284\n",
      "[1,   156] test_accuracy: 0.25\n",
      "[1,   157] loss: 2.23918\n",
      "[1,   157] test_accuracy: 0.19\n",
      "[1,   158] loss: 2.03692\n",
      "[1,   158] test_accuracy: 0.28\n",
      "[1,   159] loss: 2.22358\n",
      "[1,   159] test_accuracy: 0.22\n",
      "[1,   160] loss: 2.12073\n",
      "[1,   160] test_accuracy: 0.31\n",
      "[1,   161] loss: 2.29489\n",
      "[1,   161] test_accuracy: 0.22\n",
      "[1,   162] loss: 1.85255\n",
      "[1,   162] test_accuracy: 0.28\n",
      "[1,   163] loss: 2.20065\n",
      "[1,   163] test_accuracy: 0.25\n",
      "[1,   164] loss: 2.70935\n",
      "[1,   164] test_accuracy: 0.31\n",
      "[1,   165] loss: 2.15945\n",
      "[1,   165] test_accuracy: 0.25\n",
      "[1,   166] loss: 1.99042\n",
      "[1,   166] test_accuracy: 0.34\n",
      "[1,   167] loss: 2.09142\n",
      "[1,   167] test_accuracy: 0.22\n",
      "[1,   168] loss: 2.15138\n",
      "[1,   168] test_accuracy: 0.22\n",
      "[1,   169] loss: 2.48015\n",
      "[1,   169] test_accuracy: 0.25\n",
      "[1,   170] loss: 2.01359\n",
      "[1,   170] test_accuracy: 0.28\n",
      "[1,   171] loss: 2.57114\n",
      "[1,   171] test_accuracy: 0.34\n",
      "[1,   172] loss: 2.14940\n",
      "[1,   172] test_accuracy: 0.28\n",
      "[1,   173] loss: 2.60023\n",
      "[1,   173] test_accuracy: 0.22\n",
      "[1,   174] loss: 2.73241\n",
      "[1,   174] test_accuracy: 0.28\n",
      "[1,   175] loss: 2.04416\n",
      "[1,   175] test_accuracy: 0.28\n",
      "[1,   176] loss: 2.12115\n",
      "[1,   176] test_accuracy: 0.28\n",
      "[1,   177] loss: 2.10409\n",
      "[1,   177] test_accuracy: 0.28\n",
      "[1,   178] loss: 2.14571\n",
      "[1,   178] test_accuracy: 0.25\n",
      "[1,   179] loss: 3.13031\n",
      "[1,   179] test_accuracy: 0.22\n",
      "[1,   180] loss: 1.96309\n",
      "[1,   180] test_accuracy: 0.34\n",
      "[1,   181] loss: 2.14787\n",
      "[1,   181] test_accuracy: 0.19\n",
      "[1,   182] loss: 2.08882\n",
      "[1,   182] test_accuracy: 0.31\n",
      "[1,   183] loss: 2.47487\n",
      "[1,   183] test_accuracy: 0.12\n",
      "[1,   184] loss: 2.08230\n",
      "[1,   184] test_accuracy: 0.28\n",
      "[1,   185] loss: 1.88996\n",
      "[1,   185] test_accuracy: 0.28\n",
      "[1,   186] loss: 2.30635\n",
      "[1,   186] test_accuracy: 0.22\n",
      "[1,   187] loss: 2.27209\n",
      "[1,   187] test_accuracy: 0.22\n",
      "[1,   188] loss: 2.25992\n",
      "[1,   188] test_accuracy: 0.25\n",
      "[1,   189] loss: 2.01954\n",
      "[1,   189] test_accuracy: 0.34\n",
      "[1,   190] loss: 2.86632\n",
      "[1,   190] test_accuracy: 0.16\n",
      "[1,   191] loss: 1.77836\n",
      "[1,   191] test_accuracy: 0.44\n",
      "[1,   192] loss: 2.45848\n",
      "[1,   192] test_accuracy: 0.28\n",
      "[1,   193] loss: 2.20560\n",
      "[1,   193] test_accuracy: 0.38\n",
      "[1,   194] loss: 2.87173\n",
      "[1,   194] test_accuracy: 0.25\n",
      "[1,   195] loss: 2.37439\n",
      "[1,   195] test_accuracy: 0.19\n",
      "[1,   196] loss: 2.53074\n",
      "[1,   196] test_accuracy: 0.09\n",
      "[1,   197] loss: 2.35593\n",
      "[1,   197] test_accuracy: 0.25\n",
      "[1,   198] loss: 2.20167\n",
      "[1,   198] test_accuracy: 0.28\n",
      "[1,   199] loss: 2.06016\n",
      "[1,   199] test_accuracy: 0.31\n",
      "[1,   200] loss: 2.52761\n",
      "[1,   200] test_accuracy: 0.25\n",
      "[1,   201] loss: 1.86567\n",
      "[1,   201] test_accuracy: 0.31\n",
      "[1,   202] loss: 2.59424\n",
      "[1,   202] test_accuracy: 0.22\n",
      "[1,   203] loss: 2.12556\n",
      "[1,   203] test_accuracy: 0.28\n",
      "[1,   204] loss: 2.14187\n",
      "[1,   204] test_accuracy: 0.31\n",
      "[1,   205] loss: 2.26258\n",
      "[1,   205] test_accuracy: 0.22\n",
      "[1,   206] loss: 2.33070\n",
      "[1,   206] test_accuracy: 0.22\n",
      "[1,   207] loss: 2.24745\n",
      "[1,   207] test_accuracy: 0.22\n",
      "[1,   208] loss: 2.43132\n",
      "[1,   208] test_accuracy: 0.12\n",
      "[1,   209] loss: 2.39741\n",
      "[1,   209] test_accuracy: 0.25\n",
      "[1,   210] loss: 2.13269\n",
      "[1,   210] test_accuracy: 0.22\n",
      "[1,   211] loss: 2.58131\n",
      "[1,   211] test_accuracy: 0.34\n",
      "[1,   212] loss: 1.97078\n",
      "[1,   212] test_accuracy: 0.25\n",
      "[1,   213] loss: 2.23087\n",
      "[1,   213] test_accuracy: 0.19\n",
      "[1,   214] loss: 2.15289\n",
      "[1,   214] test_accuracy: 0.31\n",
      "[1,   215] loss: 1.97442\n",
      "[1,   215] test_accuracy: 0.31\n",
      "[1,   216] loss: 3.25677\n",
      "[1,   216] test_accuracy: 0.06\n",
      "[1,   217] loss: 2.34189\n",
      "[1,   217] test_accuracy: 0.34\n",
      "[1,   218] loss: 2.31542\n",
      "[1,   218] test_accuracy: 0.28\n",
      "[1,   219] loss: 2.54075\n",
      "[1,   219] test_accuracy: 0.25\n",
      "[1,   220] loss: 2.45848\n",
      "[1,   220] test_accuracy: 0.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1748b09c7f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/pytorch-implementation/src/helpers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, test_data, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/master-thesis/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "train(model, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, epochs):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_data, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            #if i == 0:\n",
    "            inputs, labels = data\n",
    "                \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(F.softmax(outputs, dim=1),dim=1)\n",
    "            accuracy = int(sum(([pred == labels[i] for i, pred in enumerate(preds)])))/len(preds)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "            running_loss += loss.item()\n",
    "            print('[%d, %5d] loss: %.5f' %(epoch + 1, i + 1, running_loss))\n",
    "            print('[%d, %5d] test_accuracy: %.2f' %(epoch + 1, i + 1, accuracy))\n",
    "            running_loss = 0.0\n",
    "        accuracy = evaluate_model(model, test_data)\n",
    "        print('val_accuracy: ', accuracy)\n",
    "        \n",
    "    print('Finished Training')\n",
    "    return True\n",
    "def evaluate_model(model, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            print(i)\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            #print(outputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST('./data', train=True, transform=transforms, download=True)\n",
    "MNIST_test = torchvision.datasets.MNIST('./data', train=False, transform=transforms, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model(MNIST_test[0][0].view((1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, epochs):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            #if i == 0:\n",
    "            inputs, labels = data\n",
    "                \n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                #return labels, outputs\n",
    "        eval\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, train_loader, test_loader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, test_loader, optimizer):\n",
    "    for epoch in range(args.num_pre_epochs):\n",
    "        print('Pre epoch: {}'.format(epoch + 1))\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = regularized_nll_loss(args, model, output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
