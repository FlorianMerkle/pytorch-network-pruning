{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import PGD, FGSM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from src.models  import CifarResNet, MNIST_CNN, CIFAR_CNN\n",
    "from src.helpers import evaluate_rob_accuracy, evaluate_clean_accuracy, load_model, safe_model,_evaluate_model\n",
    "from src.data_loader import load_torchvision_dataset, load_imagenette\n",
    "#from src.pruning import identify_layers, _evaluate_sparsity\n",
    "\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD(model, data_loader, criterion, steps, max_stepsize, eps, device):\n",
    "    model.eval()\n",
    "    advs = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        if i == i:\n",
    "            inputs, labels = data\n",
    "            inputs, labels =inputs.to(device), labels.to(device)\n",
    "\n",
    "            adv_examples = inputs\n",
    "            adv_examples.requires_grad = True\n",
    "            adv_examples.retain_grad()\n",
    "            for step in range(steps):\n",
    "                #print(torch.max(adv_examples[0]-inputs[0][0]))\n",
    "                adv_examples, pert = FGSM_step(model, adv_examples, labels, criterion, max_stepsize, device)\n",
    "                pert = adv_examples - inputs\n",
    "                pert.clamp_(-eps, eps)\n",
    "                adv_examples = inputs + pert\n",
    "                adv_examples.clamp_(0,1)\n",
    "            advs.append(adv_examples)\n",
    "            preds = model(adv_examples)\n",
    "            #pred_labels = \n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            total += len(predicted)\n",
    "            #correct += (pred_labels == labels).sum().item()\n",
    "            correct += (predicted != labels).sum().item()\n",
    "    return advs, correct/total\n",
    "        \n",
    "\n",
    "def FGSM_step(model, inputs, labels, criterion, eps, device):\n",
    "\n",
    "    inputs.retain_grad()\n",
    "    perturbation = torch.zeros_like(inputs).to(device)\n",
    "    preds = model(inputs)\n",
    "    loss = criterion(preds, labels)\n",
    "    loss.backward(retain_graph=True)\n",
    "    perturbation = torch.sign(inputs.grad).clamp_(-eps, eps)\n",
    "    adv_examples = inputs + perturbation\n",
    "    adv_examples.clamp_(0,1)\n",
    "    return adv_examples, perturbation\n",
    "    \n",
    "\n",
    "def FGSM(model, data_loader, criterion, eps, device):\n",
    "    model.eval()\n",
    "    #mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "    #mean = torch.tensor(mean).view(3,1,1).expand(3,32,32).to(device)\n",
    "    #std = torch.tensor(std).view(3,1,1).expand(3,32,32).to(device)\n",
    "    advs = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i,data in enumerate(data_loader):\n",
    "        if i < 10:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs.requires_grad = True\n",
    "            adv_examples, perturbation = FGSM_step(model, inputs, labels, criterion, eps, device)\n",
    "\n",
    "            advs.append(adv_examples)\n",
    "            preds = model(adv_examples)\n",
    "            #pred_labels = \n",
    "            _, predicted = torch.max(preds.data, 1)\n",
    "            total += len(predicted)\n",
    "            #correct += (pred_labels == labels).sum().item()\n",
    "            correct += (predicted != labels).sum().item()\n",
    "\n",
    "    \n",
    "    return advs, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torchvision_dataset(dataset, batchsize=512, data_augmentation=False, shuffle=True):\n",
    "    if data_augmentation == True:\n",
    "        train_transforms = torchvision.transforms.Compose([\n",
    "            #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomRotation(20),\n",
    "            torchvision.transforms.Resize(40),\n",
    "            torchvision.transforms.RandomResizedCrop(32),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            #torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),        \n",
    "        ])\n",
    "    if data_augmentation == False:\n",
    "        train_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            #torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),        \n",
    "        ])\n",
    "    val_transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        #torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    if dataset == 'MNIST':\n",
    "        train = torchvision.datasets.MNIST('./data', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "        test = torchvision.datasets.MNIST('./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "        attack = torchvision.datasets.MNIST('./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    if dataset == 'CIFAR10':\n",
    "        train = torchvision.datasets.CIFAR10('./data', train=True, transform=train_transforms, download=True)\n",
    "        test = torchvision.datasets.CIFAR10('./data', train=False, transform=val_transforms, download=True)\n",
    "        attack = torchvision.datasets.CIFAR10('./data', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    attack_loader = torch.utils.data.DataLoader(\n",
    "        attack,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifying layers\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = CifarResNet()\n",
    "model.to(device)\n",
    "train_loader, test_loader = load_torchvision_dataset('CIFAR10', data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast adversarial training\n",
      "fast adv. train.\n",
      "[1,     1] loss: 6.24548, adv_train_accuracy: 0.00, clean_train_accuracy : 18.36\n",
      "[1,    11] loss: 7.48917, adv_train_accuracy: 11.52, clean_train_accuracy : 11.13\n",
      "[1,    21] loss: 4.45007, adv_train_accuracy: 11.13, clean_train_accuracy : 15.04\n",
      "[1,    31] loss: 3.13092, adv_train_accuracy: 14.06, clean_train_accuracy : 18.95\n",
      "[1,    41] loss: 2.87374, adv_train_accuracy: 8.01, clean_train_accuracy : 16.99\n",
      "[1,    51] loss: 2.75230, adv_train_accuracy: 15.23, clean_train_accuracy : 20.31\n",
      "[1,    61] loss: 3.35220, adv_train_accuracy: 13.28, clean_train_accuracy : 23.44\n",
      "[1,    71] loss: 2.42029, adv_train_accuracy: 11.13, clean_train_accuracy : 20.31\n",
      "[1,    81] loss: 2.41274, adv_train_accuracy: 10.74, clean_train_accuracy : 20.31\n",
      "[1,    91] loss: 2.36847, adv_train_accuracy: 15.43, clean_train_accuracy : 25.00\n",
      "0.840625\n",
      "0.8283203125\n",
      "duration: 193 s - train loss: 5.10343 - train accuracy: 11.33 - validation loss: 2.09287 - validation accuracy: 28.52 \n",
      "[2,     1] loss: 2.34185, adv_train_accuracy: 15.04, clean_train_accuracy : 22.66\n",
      "[2,    11] loss: 2.43748, adv_train_accuracy: 14.65, clean_train_accuracy : 20.31\n",
      "[2,    21] loss: 2.26317, adv_train_accuracy: 17.77, clean_train_accuracy : 24.61\n",
      "[2,    31] loss: 2.14766, adv_train_accuracy: 17.58, clean_train_accuracy : 29.49\n",
      "[2,    41] loss: 2.20217, adv_train_accuracy: 15.04, clean_train_accuracy : 22.85\n",
      "[2,    51] loss: 2.31548, adv_train_accuracy: 13.87, clean_train_accuracy : 25.98\n",
      "[2,    61] loss: 2.21647, adv_train_accuracy: 16.02, clean_train_accuracy : 25.39\n",
      "[2,    71] loss: 2.20296, adv_train_accuracy: 16.80, clean_train_accuracy : 26.76\n",
      "[2,    81] loss: 2.14689, adv_train_accuracy: 19.34, clean_train_accuracy : 28.32\n",
      "[2,    91] loss: 2.20985, adv_train_accuracy: 17.38, clean_train_accuracy : 25.39\n",
      "0.814453125\n",
      "0.7796875\n",
      "duration: 193 s - train loss: 2.26388 - train accuracy: 16.59 - validation loss: 1.96038 - validation accuracy: 31.10 \n",
      "[3,     1] loss: 2.22952, adv_train_accuracy: 16.02, clean_train_accuracy : 25.00\n",
      "[3,    11] loss: 2.25618, adv_train_accuracy: 13.28, clean_train_accuracy : 22.27\n",
      "[3,    21] loss: 2.24873, adv_train_accuracy: 17.38, clean_train_accuracy : 27.54\n",
      "[3,    31] loss: 2.19067, adv_train_accuracy: 20.51, clean_train_accuracy : 32.62\n",
      "[3,    41] loss: 2.16340, adv_train_accuracy: 18.95, clean_train_accuracy : 26.95\n",
      "[3,    51] loss: 2.18879, adv_train_accuracy: 17.58, clean_train_accuracy : 26.76\n",
      "[3,    61] loss: 2.19755, adv_train_accuracy: 19.92, clean_train_accuracy : 27.93\n",
      "[3,    71] loss: 2.14488, adv_train_accuracy: 19.92, clean_train_accuracy : 26.37\n",
      "[3,    81] loss: 2.18408, adv_train_accuracy: 17.97, clean_train_accuracy : 25.78\n",
      "[3,    91] loss: 2.13663, adv_train_accuracy: 19.34, clean_train_accuracy : 31.64\n",
      "0.81015625\n",
      "0.7537109375\n",
      "duration: 193 s - train loss: 2.19036 - train accuracy: 18.75 - validation loss: 1.88060 - validation accuracy: 33.09 \n",
      "[4,     1] loss: 2.22783, adv_train_accuracy: 20.31, clean_train_accuracy : 25.98\n",
      "[4,    11] loss: 2.14568, adv_train_accuracy: 20.12, clean_train_accuracy : 31.05\n",
      "[4,    21] loss: 2.14192, adv_train_accuracy: 20.51, clean_train_accuracy : 29.30\n",
      "[4,    31] loss: 2.12902, adv_train_accuracy: 20.51, clean_train_accuracy : 29.10\n",
      "[4,    41] loss: 2.25220, adv_train_accuracy: 19.73, clean_train_accuracy : 22.85\n",
      "[4,    51] loss: 2.18964, adv_train_accuracy: 19.92, clean_train_accuracy : 26.17\n",
      "[4,    61] loss: 2.14871, adv_train_accuracy: 15.62, clean_train_accuracy : 25.20\n",
      "[4,    71] loss: 2.23045, adv_train_accuracy: 18.75, clean_train_accuracy : 28.12\n",
      "[4,    81] loss: 2.11226, adv_train_accuracy: 21.29, clean_train_accuracy : 31.25\n",
      "[4,    91] loss: 2.15476, adv_train_accuracy: 19.14, clean_train_accuracy : 27.34\n",
      "0.8033203125\n",
      "0.754296875\n",
      "duration: 193 s - train loss: 2.15741 - train accuracy: 19.71 - validation loss: 1.83330 - validation accuracy: 36.78 \n",
      "[5,     1] loss: 2.14281, adv_train_accuracy: 19.73, clean_train_accuracy : 29.10\n",
      "[5,    11] loss: 2.07054, adv_train_accuracy: 24.80, clean_train_accuracy : 33.98\n",
      "[5,    21] loss: 2.19523, adv_train_accuracy: 21.88, clean_train_accuracy : 31.05\n",
      "[5,    31] loss: 2.10963, adv_train_accuracy: 20.51, clean_train_accuracy : 28.52\n",
      "[5,    41] loss: 2.13639, adv_train_accuracy: 21.09, clean_train_accuracy : 28.91\n",
      "[5,    51] loss: 2.19436, adv_train_accuracy: 18.95, clean_train_accuracy : 26.76\n",
      "[5,    61] loss: 2.13600, adv_train_accuracy: 21.09, clean_train_accuracy : 30.08\n",
      "[5,    71] loss: 2.13235, adv_train_accuracy: 21.29, clean_train_accuracy : 31.25\n",
      "[5,    81] loss: 2.08775, adv_train_accuracy: 22.27, clean_train_accuracy : 30.27\n",
      "[5,    91] loss: 2.03066, adv_train_accuracy: 25.20, clean_train_accuracy : 36.91\n",
      "0.818359375\n",
      "0.7759765625\n",
      "duration: 193 s - train loss: 2.12644 - train accuracy: 20.56 - validation loss: 1.88453 - validation accuracy: 34.18 \n",
      "[6,     1] loss: 2.17665, adv_train_accuracy: 17.77, clean_train_accuracy : 27.93\n",
      "[6,    11] loss: 2.15549, adv_train_accuracy: 16.99, clean_train_accuracy : 30.86\n",
      "[6,    21] loss: 2.12211, adv_train_accuracy: 22.07, clean_train_accuracy : 30.66\n",
      "[6,    31] loss: 2.10282, adv_train_accuracy: 21.88, clean_train_accuracy : 34.38\n",
      "[6,    41] loss: 2.08220, adv_train_accuracy: 21.48, clean_train_accuracy : 30.08\n",
      "[6,    51] loss: 2.11139, adv_train_accuracy: 19.73, clean_train_accuracy : 29.88\n",
      "[6,    61] loss: 2.06692, adv_train_accuracy: 22.66, clean_train_accuracy : 33.59\n",
      "[6,    71] loss: 2.09479, adv_train_accuracy: 24.22, clean_train_accuracy : 30.08\n",
      "[6,    81] loss: 2.10255, adv_train_accuracy: 20.70, clean_train_accuracy : 29.10\n",
      "[6,    91] loss: 2.08575, adv_train_accuracy: 21.48, clean_train_accuracy : 33.01\n",
      "0.796875\n",
      "0.738671875\n",
      "duration: 193 s - train loss: 2.11074 - train accuracy: 21.11 - validation loss: 1.79539 - validation accuracy: 37.40 \n",
      "[7,     1] loss: 2.08202, adv_train_accuracy: 21.48, clean_train_accuracy : 32.23\n",
      "[7,    11] loss: 2.12184, adv_train_accuracy: 20.90, clean_train_accuracy : 28.12\n",
      "[7,    21] loss: 2.10116, adv_train_accuracy: 20.70, clean_train_accuracy : 30.47\n",
      "[7,    31] loss: 2.08521, adv_train_accuracy: 25.00, clean_train_accuracy : 34.96\n",
      "[7,    41] loss: 2.09434, adv_train_accuracy: 20.31, clean_train_accuracy : 31.84\n",
      "[7,    51] loss: 2.06726, adv_train_accuracy: 23.63, clean_train_accuracy : 33.20\n",
      "[7,    61] loss: 2.09945, adv_train_accuracy: 22.46, clean_train_accuracy : 33.59\n",
      "[7,    71] loss: 2.12788, adv_train_accuracy: 20.31, clean_train_accuracy : 31.84\n",
      "[7,    81] loss: 2.06851, adv_train_accuracy: 22.85, clean_train_accuracy : 33.79\n",
      "[7,    91] loss: 2.06917, adv_train_accuracy: 21.48, clean_train_accuracy : 31.45\n",
      "0.817578125\n",
      "0.7525390625\n",
      "duration: 193 s - train loss: 2.08950 - train accuracy: 21.76 - validation loss: 1.77607 - validation accuracy: 36.85 \n",
      "[8,     1] loss: 2.08119, adv_train_accuracy: 20.70, clean_train_accuracy : 33.40\n",
      "[8,    11] loss: 2.11606, adv_train_accuracy: 21.48, clean_train_accuracy : 31.05\n",
      "[8,    21] loss: 2.07043, adv_train_accuracy: 22.07, clean_train_accuracy : 34.38\n",
      "[8,    31] loss: 2.10340, adv_train_accuracy: 20.90, clean_train_accuracy : 32.81\n",
      "[8,    41] loss: 2.09970, adv_train_accuracy: 22.66, clean_train_accuracy : 33.40\n",
      "[8,    51] loss: 2.06118, adv_train_accuracy: 22.85, clean_train_accuracy : 31.25\n",
      "[8,    61] loss: 2.12520, adv_train_accuracy: 21.29, clean_train_accuracy : 33.20\n",
      "[8,    71] loss: 2.08367, adv_train_accuracy: 22.27, clean_train_accuracy : 31.25\n",
      "[8,    81] loss: 2.08975, adv_train_accuracy: 22.07, clean_train_accuracy : 31.25\n",
      "[8,    91] loss: 2.14555, adv_train_accuracy: 20.70, clean_train_accuracy : 33.20\n",
      "0.780859375\n",
      "0.7271484375\n",
      "duration: 193 s - train loss: 2.07883 - train accuracy: 21.95 - validation loss: 1.73578 - validation accuracy: 36.86 \n",
      "[9,     1] loss: 2.09885, adv_train_accuracy: 23.44, clean_train_accuracy : 34.96\n",
      "[9,    11] loss: 2.11888, adv_train_accuracy: 24.02, clean_train_accuracy : 33.79\n",
      "[9,    21] loss: 2.04991, adv_train_accuracy: 19.92, clean_train_accuracy : 31.05\n",
      "[9,    31] loss: 2.10023, adv_train_accuracy: 22.07, clean_train_accuracy : 33.01\n",
      "[9,    41] loss: 2.09756, adv_train_accuracy: 19.73, clean_train_accuracy : 32.23\n",
      "[9,    51] loss: 2.01264, adv_train_accuracy: 24.61, clean_train_accuracy : 34.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,    61] loss: 2.04419, adv_train_accuracy: 24.02, clean_train_accuracy : 35.74\n",
      "[9,    71] loss: 2.10378, adv_train_accuracy: 20.90, clean_train_accuracy : 29.69\n",
      "[9,    81] loss: 2.10470, adv_train_accuracy: 19.53, clean_train_accuracy : 28.52\n",
      "[9,    91] loss: 2.04133, adv_train_accuracy: 21.29, clean_train_accuracy : 32.42\n",
      "0.779296875\n",
      "0.7130859375\n",
      "duration: 193 s - train loss: 2.07987 - train accuracy: 22.29 - validation loss: 1.68458 - validation accuracy: 40.38 \n",
      "[10,     1] loss: 2.02710, adv_train_accuracy: 25.39, clean_train_accuracy : 33.20\n",
      "[10,    11] loss: 2.05175, adv_train_accuracy: 21.09, clean_train_accuracy : 33.59\n",
      "[10,    21] loss: 2.08253, adv_train_accuracy: 23.44, clean_train_accuracy : 35.35\n",
      "[10,    31] loss: 1.99495, adv_train_accuracy: 25.39, clean_train_accuracy : 38.28\n",
      "[10,    41] loss: 2.07223, adv_train_accuracy: 20.31, clean_train_accuracy : 32.62\n",
      "[10,    51] loss: 2.01737, adv_train_accuracy: 24.41, clean_train_accuracy : 37.30\n",
      "[10,    61] loss: 2.13436, adv_train_accuracy: 20.70, clean_train_accuracy : 34.38\n",
      "[10,    71] loss: 2.09158, adv_train_accuracy: 20.31, clean_train_accuracy : 32.23\n",
      "[10,    81] loss: 2.04288, adv_train_accuracy: 25.00, clean_train_accuracy : 35.94\n",
      "[10,    91] loss: 2.07176, adv_train_accuracy: 22.85, clean_train_accuracy : 34.77\n",
      "0.7953125\n",
      "0.7166015625\n",
      "duration: 192 s - train loss: 2.05746 - train accuracy: 23.03 - validation loss: 1.65905 - validation accuracy: 43.07 \n",
      "[11,     1] loss: 2.02169, adv_train_accuracy: 25.00, clean_train_accuracy : 35.74\n",
      "[11,    11] loss: 2.05701, adv_train_accuracy: 22.27, clean_train_accuracy : 33.01\n",
      "[11,    21] loss: 1.97671, adv_train_accuracy: 25.39, clean_train_accuracy : 40.23\n",
      "[11,    31] loss: 1.98933, adv_train_accuracy: 21.29, clean_train_accuracy : 37.70\n",
      "[11,    41] loss: 2.08114, adv_train_accuracy: 23.83, clean_train_accuracy : 33.40\n",
      "[11,    51] loss: 2.09497, adv_train_accuracy: 21.48, clean_train_accuracy : 32.03\n",
      "[11,    61] loss: 2.01478, adv_train_accuracy: 24.22, clean_train_accuracy : 35.55\n",
      "[11,    71] loss: 2.10658, adv_train_accuracy: 20.12, clean_train_accuracy : 33.59\n",
      "[11,    81] loss: 2.06731, adv_train_accuracy: 21.29, clean_train_accuracy : 34.57\n",
      "[11,    91] loss: 2.08732, adv_train_accuracy: 22.07, clean_train_accuracy : 31.84\n",
      "0.7919921875\n",
      "0.7232421875\n",
      "duration: 192 s - train loss: 2.04872 - train accuracy: 23.08 - validation loss: 1.67930 - validation accuracy: 42.87 \n",
      "[12,     1] loss: 2.07695, adv_train_accuracy: 23.63, clean_train_accuracy : 37.11\n",
      "[12,    11] loss: 1.98974, adv_train_accuracy: 25.98, clean_train_accuracy : 37.89\n",
      "[12,    21] loss: 2.03263, adv_train_accuracy: 21.29, clean_train_accuracy : 33.01\n",
      "[12,    31] loss: 2.04481, adv_train_accuracy: 23.24, clean_train_accuracy : 32.62\n",
      "[12,    41] loss: 2.03245, adv_train_accuracy: 23.44, clean_train_accuracy : 33.20\n",
      "[12,    51] loss: 2.03149, adv_train_accuracy: 24.61, clean_train_accuracy : 33.40\n",
      "[12,    61] loss: 2.05979, adv_train_accuracy: 23.83, clean_train_accuracy : 32.23\n",
      "[12,    71] loss: 2.07449, adv_train_accuracy: 19.53, clean_train_accuracy : 29.69\n",
      "[12,    81] loss: 2.00217, adv_train_accuracy: 25.00, clean_train_accuracy : 35.94\n",
      "[12,    91] loss: 2.03549, adv_train_accuracy: 22.66, clean_train_accuracy : 33.01\n",
      "0.7787109375\n",
      "0.7142578125\n",
      "duration: 192 s - train loss: 2.04099 - train accuracy: 23.72 - validation loss: 1.63820 - validation accuracy: 41.93 \n",
      "[13,     1] loss: 2.04245, adv_train_accuracy: 22.85, clean_train_accuracy : 35.35\n",
      "[13,    11] loss: 2.02576, adv_train_accuracy: 21.68, clean_train_accuracy : 34.77\n",
      "[13,    21] loss: 2.04899, adv_train_accuracy: 22.46, clean_train_accuracy : 36.91\n",
      "[13,    31] loss: 2.08421, adv_train_accuracy: 19.14, clean_train_accuracy : 33.20\n",
      "[13,    41] loss: 2.00884, adv_train_accuracy: 20.51, clean_train_accuracy : 36.91\n",
      "[13,    51] loss: 2.06248, adv_train_accuracy: 24.22, clean_train_accuracy : 31.64\n",
      "[13,    61] loss: 2.01196, adv_train_accuracy: 27.93, clean_train_accuracy : 40.23\n",
      "[13,    71] loss: 2.05716, adv_train_accuracy: 23.63, clean_train_accuracy : 37.11\n",
      "[13,    81] loss: 2.03328, adv_train_accuracy: 24.22, clean_train_accuracy : 37.11\n",
      "[13,    91] loss: 1.99365, adv_train_accuracy: 23.24, clean_train_accuracy : 36.13\n",
      "0.784375\n",
      "0.7251953125\n",
      "duration: 192 s - train loss: 2.02951 - train accuracy: 23.79 - validation loss: 1.64369 - validation accuracy: 42.47 \n",
      "[14,     1] loss: 2.05586, adv_train_accuracy: 24.41, clean_train_accuracy : 36.52\n",
      "[14,    11] loss: 1.95446, adv_train_accuracy: 26.17, clean_train_accuracy : 39.26\n",
      "[14,    21] loss: 2.02213, adv_train_accuracy: 25.20, clean_train_accuracy : 36.13\n",
      "[14,    31] loss: 2.03662, adv_train_accuracy: 24.22, clean_train_accuracy : 33.59\n",
      "[14,    41] loss: 2.02074, adv_train_accuracy: 21.48, clean_train_accuracy : 34.18\n",
      "[14,    51] loss: 1.99879, adv_train_accuracy: 24.80, clean_train_accuracy : 37.50\n",
      "[14,    61] loss: 2.00440, adv_train_accuracy: 24.02, clean_train_accuracy : 35.74\n",
      "[14,    71] loss: 2.01293, adv_train_accuracy: 25.00, clean_train_accuracy : 38.48\n",
      "[14,    81] loss: 2.07151, adv_train_accuracy: 23.44, clean_train_accuracy : 33.98\n",
      "[14,    91] loss: 1.97705, adv_train_accuracy: 26.56, clean_train_accuracy : 37.70\n",
      "0.7734375\n",
      "0.7048828125\n",
      "duration: 191 s - train loss: 2.02190 - train accuracy: 24.14 - validation loss: 1.61968 - validation accuracy: 43.13 \n",
      "[15,     1] loss: 1.99215, adv_train_accuracy: 27.54, clean_train_accuracy : 39.84\n",
      "[15,    11] loss: 2.01966, adv_train_accuracy: 26.56, clean_train_accuracy : 36.13\n",
      "[15,    21] loss: 1.99021, adv_train_accuracy: 24.61, clean_train_accuracy : 42.38\n",
      "[15,    31] loss: 2.02535, adv_train_accuracy: 25.00, clean_train_accuracy : 37.70\n",
      "[15,    41] loss: 2.01031, adv_train_accuracy: 27.15, clean_train_accuracy : 39.65\n",
      "[15,    51] loss: 2.03848, adv_train_accuracy: 21.68, clean_train_accuracy : 33.40\n",
      "[15,    61] loss: 1.96609, adv_train_accuracy: 24.22, clean_train_accuracy : 43.55\n",
      "[15,    71] loss: 2.02152, adv_train_accuracy: 24.61, clean_train_accuracy : 38.09\n",
      "[15,    81] loss: 2.00757, adv_train_accuracy: 25.00, clean_train_accuracy : 36.52\n",
      "[15,    91] loss: 2.01742, adv_train_accuracy: 23.44, clean_train_accuracy : 40.62\n",
      "0.759375\n",
      "0.7021484375\n",
      "duration: 191 s - train loss: 2.01994 - train accuracy: 24.51 - validation loss: 1.57945 - validation accuracy: 46.22 \n",
      "[16,     1] loss: 2.04603, adv_train_accuracy: 21.09, clean_train_accuracy : 33.40\n",
      "[16,    11] loss: 1.98165, adv_train_accuracy: 27.54, clean_train_accuracy : 40.43\n",
      "[16,    21] loss: 2.00392, adv_train_accuracy: 25.20, clean_train_accuracy : 39.26\n",
      "[16,    31] loss: 1.96159, adv_train_accuracy: 25.20, clean_train_accuracy : 39.84\n",
      "[16,    41] loss: 2.03488, adv_train_accuracy: 23.83, clean_train_accuracy : 39.84\n",
      "[16,    51] loss: 2.00169, adv_train_accuracy: 26.37, clean_train_accuracy : 38.09\n",
      "[16,    61] loss: 2.01289, adv_train_accuracy: 23.05, clean_train_accuracy : 34.96\n",
      "[16,    71] loss: 2.02857, adv_train_accuracy: 24.22, clean_train_accuracy : 37.50\n",
      "[16,    81] loss: 2.02131, adv_train_accuracy: 24.80, clean_train_accuracy : 35.94\n",
      "[16,    91] loss: 2.04690, adv_train_accuracy: 23.44, clean_train_accuracy : 35.35\n",
      "0.7634765625\n",
      "0.7171875\n",
      "duration: 191 s - train loss: 2.01423 - train accuracy: 24.59 - validation loss: 1.64373 - validation accuracy: 42.43 \n",
      "[17,     1] loss: 2.05926, adv_train_accuracy: 22.66, clean_train_accuracy : 35.55\n",
      "[17,    11] loss: 2.01425, adv_train_accuracy: 26.56, clean_train_accuracy : 40.82\n",
      "[17,    21] loss: 2.06254, adv_train_accuracy: 19.34, clean_train_accuracy : 34.96\n",
      "[17,    31] loss: 2.00285, adv_train_accuracy: 26.76, clean_train_accuracy : 40.04\n",
      "[17,    41] loss: 1.98997, adv_train_accuracy: 24.02, clean_train_accuracy : 37.50\n",
      "[17,    51] loss: 1.99488, adv_train_accuracy: 27.15, clean_train_accuracy : 41.99\n",
      "[17,    61] loss: 2.01412, adv_train_accuracy: 24.02, clean_train_accuracy : 39.26\n",
      "[17,    71] loss: 1.97679, adv_train_accuracy: 27.54, clean_train_accuracy : 40.82\n",
      "[17,    81] loss: 2.01421, adv_train_accuracy: 22.07, clean_train_accuracy : 37.11\n",
      "[17,    91] loss: 1.98808, adv_train_accuracy: 25.00, clean_train_accuracy : 41.21\n",
      "0.77109375\n",
      "0.6982421875\n",
      "duration: 191 s - train loss: 2.00235 - train accuracy: 25.01 - validation loss: 1.56577 - validation accuracy: 45.97 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,     1] loss: 1.99763, adv_train_accuracy: 25.00, clean_train_accuracy : 38.87\n",
      "[18,    11] loss: 1.98450, adv_train_accuracy: 25.00, clean_train_accuracy : 36.72\n",
      "[18,    21] loss: 2.01204, adv_train_accuracy: 22.66, clean_train_accuracy : 36.91\n",
      "[18,    31] loss: 1.97325, adv_train_accuracy: 27.15, clean_train_accuracy : 42.77\n",
      "[18,    41] loss: 1.97644, adv_train_accuracy: 25.39, clean_train_accuracy : 40.62\n",
      "[18,    51] loss: 1.96699, adv_train_accuracy: 27.15, clean_train_accuracy : 40.82\n",
      "[18,    61] loss: 2.02902, adv_train_accuracy: 22.85, clean_train_accuracy : 37.30\n",
      "[18,    71] loss: 1.96984, adv_train_accuracy: 26.37, clean_train_accuracy : 38.48\n",
      "[18,    81] loss: 1.99101, adv_train_accuracy: 26.17, clean_train_accuracy : 38.28\n",
      "[18,    91] loss: 1.97518, adv_train_accuracy: 27.73, clean_train_accuracy : 40.82\n",
      "0.770703125\n",
      "[19,    61] loss: 2.05349, adv_train_accuracy: 20.90, clean_train_accuracy : 32.81\n",
      "[19,    71] loss: 1.99580, adv_train_accuracy: 26.17, clean_train_accuracy : 37.11\n",
      "[19,    81] loss: 2.04421, adv_train_accuracy: 22.85, clean_train_accuracy : 36.52\n",
      "[19,    91] loss: 1.98488, adv_train_accuracy: 20.12, clean_train_accuracy : 37.30\n",
      "0.7900390625\n",
      "0.738671875\n",
      "duration: 193 s - train loss: 2.09186 - train accuracy: 22.22 - validation loss: 1.63061 - validation accuracy: 41.32 \n",
      "[20,     1] loss: 2.04426, adv_train_accuracy: 23.05, clean_train_accuracy : 38.09\n",
      "[20,    11] loss: 1.99129, adv_train_accuracy: 26.76, clean_train_accuracy : 39.84\n",
      "[20,    21] loss: 2.07052, adv_train_accuracy: 22.85, clean_train_accuracy : 37.70\n",
      "[20,    31] loss: 1.98456, adv_train_accuracy: 26.76, clean_train_accuracy : 36.33\n",
      "[20,    41] loss: 2.00021, adv_train_accuracy: 27.15, clean_train_accuracy : 40.04\n",
      "[20,    51] loss: 1.97803, adv_train_accuracy: 24.61, clean_train_accuracy : 39.26\n",
      "[20,    61] loss: 2.02716, adv_train_accuracy: 20.51, clean_train_accuracy : 36.13\n",
      "[20,    71] loss: 2.00728, adv_train_accuracy: 22.66, clean_train_accuracy : 37.70\n",
      "[20,    81] loss: 2.02509, adv_train_accuracy: 25.78, clean_train_accuracy : 39.65\n",
      "[20,    91] loss: 1.98812, adv_train_accuracy: 26.95, clean_train_accuracy : 41.02\n",
      "0.7673828125\n",
      "0.693359375\n",
      "duration: 192 s - train loss: 2.01817 - train accuracy: 24.42 - validation loss: 1.58792 - validation accuracy: 46.87 \n",
      "[21,     1] loss: 2.01717, adv_train_accuracy: 23.83, clean_train_accuracy : 41.02\n",
      "[21,    11] loss: 2.03249, adv_train_accuracy: 25.20, clean_train_accuracy : 41.80\n",
      "[21,    21] loss: 2.00191, adv_train_accuracy: 25.39, clean_train_accuracy : 39.26\n",
      "[21,    31] loss: 1.98315, adv_train_accuracy: 24.02, clean_train_accuracy : 39.84\n",
      "[21,    41] loss: 1.96473, adv_train_accuracy: 25.20, clean_train_accuracy : 38.67\n",
      "[21,    51] loss: 2.01800, adv_train_accuracy: 21.68, clean_train_accuracy : 37.89\n",
      "[21,    61] loss: 2.02991, adv_train_accuracy: 26.76, clean_train_accuracy : 38.48\n",
      "[21,    71] loss: 1.97453, adv_train_accuracy: 26.17, clean_train_accuracy : 42.58\n",
      "[21,    81] loss: 2.01126, adv_train_accuracy: 26.56, clean_train_accuracy : 39.26\n",
      "[21,    91] loss: 2.00235, adv_train_accuracy: 23.83, clean_train_accuracy : 41.41\n",
      "0.77578125\n",
      "0.708984375\n",
      "duration: 193 s - train loss: 1.99418 - train accuracy: 25.45 - validation loss: 1.55615 - validation accuracy: 44.56 \n",
      "[22,     1] loss: 1.99882, adv_train_accuracy: 23.05, clean_train_accuracy : 39.06\n",
      "[22,    11] loss: 1.97952, adv_train_accuracy: 24.61, clean_train_accuracy : 40.04\n",
      "[22,    21] loss: 1.97557, adv_train_accuracy: 25.78, clean_train_accuracy : 43.36\n",
      "[22,    31] loss: 2.02233, adv_train_accuracy: 25.00, clean_train_accuracy : 38.67\n",
      "[22,    41] loss: 2.00223, adv_train_accuracy: 26.37, clean_train_accuracy : 40.43\n",
      "[22,    51] loss: 2.00112, adv_train_accuracy: 25.39, clean_train_accuracy : 39.26\n",
      "[22,    61] loss: 1.99146, adv_train_accuracy: 26.95, clean_train_accuracy : 39.06\n",
      "[22,    71] loss: 2.01220, adv_train_accuracy: 23.63, clean_train_accuracy : 37.70\n",
      "[22,    81] loss: 1.98068, adv_train_accuracy: 26.37, clean_train_accuracy : 38.48\n",
      "[22,    91] loss: 1.90135, adv_train_accuracy: 27.73, clean_train_accuracy : 42.19\n",
      "0.7630859375\n",
      "0.7025390625\n",
      "duration: 193 s - train loss: 1.98859 - train accuracy: 25.31 - validation loss: 1.50648 - validation accuracy: 47.52 \n",
      "[23,     1] loss: 1.96297, adv_train_accuracy: 25.20, clean_train_accuracy : 41.02\n",
      "[23,    11] loss: 1.99694, adv_train_accuracy: 23.44, clean_train_accuracy : 40.23\n",
      "[23,    21] loss: 1.98617, adv_train_accuracy: 26.95, clean_train_accuracy : 41.60\n",
      "[23,    31] loss: 2.02882, adv_train_accuracy: 27.15, clean_train_accuracy : 42.58\n",
      "[23,    41] loss: 1.95564, adv_train_accuracy: 27.15, clean_train_accuracy : 40.62\n",
      "[23,    51] loss: 1.95040, adv_train_accuracy: 26.56, clean_train_accuracy : 43.36\n",
      "[23,    61] loss: 1.96010, adv_train_accuracy: 22.07, clean_train_accuracy : 35.94\n",
      "[23,    71] loss: 2.01410, adv_train_accuracy: 29.10, clean_train_accuracy : 41.21\n",
      "[23,    81] loss: 2.02687, adv_train_accuracy: 24.41, clean_train_accuracy : 38.87\n",
      "[23,    91] loss: 1.94261, adv_train_accuracy: 27.93, clean_train_accuracy : 40.43\n",
      "0.817578125\n",
      "0.76953125\n",
      "duration: 194 s - train loss: 1.99203 - train accuracy: 25.45 - validation loss: 1.84228 - validation accuracy: 35.94 \n",
      "[24,     1] loss: 2.18103, adv_train_accuracy: 18.16, clean_train_accuracy : 31.84\n",
      "[24,    11] loss: 2.12991, adv_train_accuracy: 22.85, clean_train_accuracy : 34.96\n",
      "[24,    21] loss: 2.12191, adv_train_accuracy: 18.16, clean_train_accuracy : 36.33\n",
      "[24,    31] loss: 2.06002, adv_train_accuracy: 21.48, clean_train_accuracy : 35.35\n",
      "[24,    41] loss: 2.96819, adv_train_accuracy: 13.28, clean_train_accuracy : 17.97\n",
      "[24,    51] loss: 2.39991, adv_train_accuracy: 15.04, clean_train_accuracy : 21.48\n",
      "[24,    61] loss: 2.20952, adv_train_accuracy: 19.14, clean_train_accuracy : 29.69\n",
      "[24,    71] loss: 2.11713, adv_train_accuracy: 22.46, clean_train_accuracy : 32.23\n",
      "[24,    81] loss: 2.05297, adv_train_accuracy: 23.05, clean_train_accuracy : 35.35\n",
      "[24,    91] loss: 2.03295, adv_train_accuracy: 24.41, clean_train_accuracy : 37.50\n",
      "0.7794921875\n",
      "0.7259765625\n",
      "duration: 193 s - train loss: 2.31292 - train accuracy: 20.08 - validation loss: 1.61222 - validation accuracy: 43.96 \n",
      "[25,     1] loss: 2.07792, adv_train_accuracy: 20.90, clean_train_accuracy : 34.18\n",
      "[25,    11] loss: 1.98598, adv_train_accuracy: 25.39, clean_train_accuracy : 40.82\n",
      "[25,    21] loss: 1.96734, adv_train_accuracy: 26.17, clean_train_accuracy : 39.45\n",
      "[25,    31] loss: 2.00832, adv_train_accuracy: 27.34, clean_train_accuracy : 41.80\n",
      "[25,    41] loss: 1.96876, adv_train_accuracy: 27.34, clean_train_accuracy : 43.55\n",
      "[25,    51] loss: 2.02699, adv_train_accuracy: 26.56, clean_train_accuracy : 38.28\n",
      "[25,    61] loss: 2.04859, adv_train_accuracy: 25.78, clean_train_accuracy : 39.45\n",
      "[25,    71] loss: 2.03325, adv_train_accuracy: 22.46, clean_train_accuracy : 36.72\n",
      "[25,    81] loss: 2.04441, adv_train_accuracy: 25.78, clean_train_accuracy : 39.65\n",
      "[25,    91] loss: 1.94248, adv_train_accuracy: 25.39, clean_train_accuracy : 42.58\n",
      "0.7619140625\n",
      "0.694921875\n",
      "duration: 193 s - train loss: 2.00912 - train accuracy: 24.61 - validation loss: 1.55463 - validation accuracy: 47.51 \n",
      "[26,     1] loss: 1.98824, adv_train_accuracy: 25.00, clean_train_accuracy : 38.87\n",
      "[26,    11] loss: 1.98200, adv_train_accuracy: 25.78, clean_train_accuracy : 39.06\n",
      "[26,    21] loss: 1.98355, adv_train_accuracy: 25.98, clean_train_accuracy : 38.67\n",
      "[26,    31] loss: 1.97572, adv_train_accuracy: 26.37, clean_train_accuracy : 41.99\n",
      "[26,    41] loss: 1.96299, adv_train_accuracy: 25.59, clean_train_accuracy : 39.45\n",
      "[26,    51] loss: 1.98146, adv_train_accuracy: 25.59, clean_train_accuracy : 41.80\n",
      "[26,    61] loss: 1.96547, adv_train_accuracy: 26.17, clean_train_accuracy : 43.55\n",
      "[26,    71] loss: 1.95400, adv_train_accuracy: 25.20, clean_train_accuracy : 40.82\n",
      "[26,    81] loss: 2.00426, adv_train_accuracy: 26.56, clean_train_accuracy : 38.67\n",
      "[26,    91] loss: 1.96001, adv_train_accuracy: 26.37, clean_train_accuracy : 42.38\n",
      "0.7578125\n",
      "0.66640625\n",
      "duration: 192 s - train loss: 1.97869 - train accuracy: 25.75 - validation loss: 1.47734 - validation accuracy: 49.15 \n",
      "[27,     1] loss: 1.99677, adv_train_accuracy: 24.80, clean_train_accuracy : 41.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27,    11] loss: 1.99124, adv_train_accuracy: 26.37, clean_train_accuracy : 42.19\n",
      "[27,    21] loss: 1.98278, adv_train_accuracy: 27.93, clean_train_accuracy : 41.80\n",
      "[27,    31] loss: 2.00439, adv_train_accuracy: 25.59, clean_train_accuracy : 41.60\n",
      "[27,    41] loss: 1.97521, adv_train_accuracy: 25.78, clean_train_accuracy : 42.77\n",
      "[27,    51] loss: 1.90919, adv_train_accuracy: 32.62, clean_train_accuracy : 42.77\n",
      "[27,    61] loss: 1.93826, adv_train_accuracy: 25.59, clean_train_accuracy : 44.73\n",
      "[27,    71] loss: 1.94733, adv_train_accuracy: 29.88, clean_train_accuracy : 45.12\n",
      "[27,    81] loss: 2.01273, adv_train_accuracy: 24.80, clean_train_accuracy : 39.06\n",
      "[27,    91] loss: 1.95912, adv_train_accuracy: 27.93, clean_train_accuracy : 44.34\n",
      "0.759375\n",
      "0.685546875\n",
      "duration: 190 s - train loss: 1.96762 - train accuracy: 26.21 - validation loss: 1.45326 - validation accuracy: 50.34 \n",
      "[28,     1] loss: 1.95100, adv_train_accuracy: 27.34, clean_train_accuracy : 47.27\n",
      "[28,    11] loss: 2.04450, adv_train_accuracy: 24.02, clean_train_accuracy : 39.84\n",
      "[28,    21] loss: 1.93235, adv_train_accuracy: 30.86, clean_train_accuracy : 46.29\n",
      "[28,    31] loss: 1.95087, adv_train_accuracy: 24.61, clean_train_accuracy : 42.77\n",
      "[28,    41] loss: 1.89840, adv_train_accuracy: 28.52, clean_train_accuracy : 48.24\n",
      "[28,    51] loss: 1.97345, adv_train_accuracy: 23.44, clean_train_accuracy : 39.26\n",
      "[28,    61] loss: 1.92230, adv_train_accuracy: 27.34, clean_train_accuracy : 40.82\n",
      "[28,    71] loss: 1.95715, adv_train_accuracy: 27.15, clean_train_accuracy : 44.73\n",
      "[28,    81] loss: 2.04602, adv_train_accuracy: 23.05, clean_train_accuracy : 40.04\n",
      "[28,    91] loss: 1.97038, adv_train_accuracy: 27.93, clean_train_accuracy : 42.58\n",
      "0.75546875\n",
      "0.68671875\n",
      "duration: 193 s - train loss: 1.95856 - train accuracy: 26.62 - validation loss: 1.45388 - validation accuracy: 49.59 \n",
      "[29,     1] loss: 1.96852, adv_train_accuracy: 22.66, clean_train_accuracy : 37.30\n",
      "[29,    11] loss: 1.99410, adv_train_accuracy: 26.17, clean_train_accuracy : 42.19\n",
      "[29,    21] loss: 1.95291, adv_train_accuracy: 27.15, clean_train_accuracy : 44.53\n",
      "[29,    31] loss: 1.91746, adv_train_accuracy: 28.32, clean_train_accuracy : 44.73\n",
      "[29,    41] loss: 1.95354, adv_train_accuracy: 27.93, clean_train_accuracy : 42.77\n",
      "[29,    51] loss: 1.91230, adv_train_accuracy: 28.12, clean_train_accuracy : 45.12\n",
      "[29,    61] loss: 1.97250, adv_train_accuracy: 24.02, clean_train_accuracy : 37.89\n",
      "[29,    71] loss: 1.93271, adv_train_accuracy: 28.32, clean_train_accuracy : 43.75\n",
      "[29,    81] loss: 1.91856, adv_train_accuracy: 27.73, clean_train_accuracy : 45.90\n",
      "[29,    91] loss: 2.01895, adv_train_accuracy: 25.59, clean_train_accuracy : 41.41\n",
      "0.7482421875\n",
      "0.6705078125\n",
      "duration: 187 s - train loss: 1.95669 - train accuracy: 26.58 - validation loss: 1.48424 - validation accuracy: 49.58 \n",
      "[30,     1] loss: 1.97353, adv_train_accuracy: 23.83, clean_train_accuracy : 42.77\n",
      "[30,    11] loss: 1.93567, adv_train_accuracy: 26.17, clean_train_accuracy : 42.58\n",
      "[30,    21] loss: 1.93195, adv_train_accuracy: 28.91, clean_train_accuracy : 42.97\n",
      "[30,    31] loss: 1.95662, adv_train_accuracy: 26.76, clean_train_accuracy : 46.29\n",
      "[30,    41] loss: 1.90262, adv_train_accuracy: 32.03, clean_train_accuracy : 45.90\n",
      "[30,    51] loss: 1.92404, adv_train_accuracy: 28.32, clean_train_accuracy : 44.14\n",
      "[30,    61] loss: 1.89965, adv_train_accuracy: 29.88, clean_train_accuracy : 46.48\n",
      "[30,    71] loss: 1.96656, adv_train_accuracy: 26.95, clean_train_accuracy : 43.55\n",
      "[30,    81] loss: 1.91906, adv_train_accuracy: 28.71, clean_train_accuracy : 43.95\n",
      "[30,    91] loss: 1.89594, adv_train_accuracy: 24.22, clean_train_accuracy : 42.77\n",
      "0.752734375\n",
      "0.6525390625\n",
      "duration: 185 s - train loss: 1.95107 - train accuracy: 26.75 - validation loss: 1.44059 - validation accuracy: 51.74 \n",
      "[31,     1] loss: 1.92019, adv_train_accuracy: 28.32, clean_train_accuracy : 41.02\n",
      "[31,    11] loss: 1.95179, adv_train_accuracy: 26.37, clean_train_accuracy : 42.19\n",
      "[31,    21] loss: 1.87804, adv_train_accuracy: 25.98, clean_train_accuracy : 44.92\n",
      "[31,    31] loss: 1.89939, adv_train_accuracy: 29.30, clean_train_accuracy : 46.09\n",
      "[31,    41] loss: 1.97070, adv_train_accuracy: 25.78, clean_train_accuracy : 45.12\n",
      "[31,    51] loss: 1.97511, adv_train_accuracy: 26.95, clean_train_accuracy : 44.14\n",
      "[31,    61] loss: 1.95814, adv_train_accuracy: 28.52, clean_train_accuracy : 46.09\n",
      "[31,    71] loss: 1.92809, adv_train_accuracy: 25.78, clean_train_accuracy : 42.38\n",
      "[31,    81] loss: 1.94773, adv_train_accuracy: 22.66, clean_train_accuracy : 44.14\n",
      "[31,    91] loss: 2.01240, adv_train_accuracy: 26.95, clean_train_accuracy : 40.82\n",
      "0.741015625\n",
      "0.6705078125\n",
      "duration: 185 s - train loss: 1.94274 - train accuracy: 27.21 - validation loss: 1.41747 - validation accuracy: 51.20 \n",
      "[32,     1] loss: 1.91447, adv_train_accuracy: 28.32, clean_train_accuracy : 43.36\n",
      "[32,    11] loss: 1.94633, adv_train_accuracy: 27.93, clean_train_accuracy : 44.73\n",
      "[32,    21] loss: 1.95483, adv_train_accuracy: 27.15, clean_train_accuracy : 44.14\n",
      "[32,    31] loss: 1.90118, adv_train_accuracy: 25.98, clean_train_accuracy : 43.36\n",
      "[32,    41] loss: 1.87543, adv_train_accuracy: 29.10, clean_train_accuracy : 44.92\n",
      "[32,    51] loss: 1.91809, adv_train_accuracy: 26.17, clean_train_accuracy : 43.36\n",
      "[32,    61] loss: 1.96851, adv_train_accuracy: 26.95, clean_train_accuracy : 41.60\n",
      "[32,    71] loss: 1.94693, adv_train_accuracy: 23.63, clean_train_accuracy : 38.09\n",
      "[32,    81] loss: 1.95260, adv_train_accuracy: 26.37, clean_train_accuracy : 44.53\n",
      "[32,    91] loss: 1.98793, adv_train_accuracy: 26.56, clean_train_accuracy : 46.88\n",
      "0.75078125\n",
      "0.6833984375\n",
      "duration: 185 s - train loss: 1.93665 - train accuracy: 27.35 - validation loss: 1.41683 - validation accuracy: 50.17 \n",
      "[33,     1] loss: 1.93383, adv_train_accuracy: 26.76, clean_train_accuracy : 46.68\n",
      "[33,    11] loss: 1.90362, adv_train_accuracy: 28.32, clean_train_accuracy : 45.31\n",
      "[33,    21] loss: 1.95743, adv_train_accuracy: 26.37, clean_train_accuracy : 41.21\n",
      "[33,    31] loss: 1.90783, adv_train_accuracy: 28.52, clean_train_accuracy : 44.92\n",
      "[33,    41] loss: 1.85305, adv_train_accuracy: 29.49, clean_train_accuracy : 44.53\n",
      "[33,    51] loss: 2.01362, adv_train_accuracy: 23.05, clean_train_accuracy : 38.67\n",
      "[33,    61] loss: 1.93279, adv_train_accuracy: 26.95, clean_train_accuracy : 40.23\n",
      "[33,    71] loss: 1.95359, adv_train_accuracy: 26.17, clean_train_accuracy : 40.23\n",
      "[33,    81] loss: 1.95377, adv_train_accuracy: 29.49, clean_train_accuracy : 42.58\n",
      "[33,    91] loss: 1.98708, adv_train_accuracy: 23.24, clean_train_accuracy : 39.26\n",
      "0.748046875\n",
      "0.684375\n",
      "duration: 185 s - train loss: 1.93273 - train accuracy: 27.58 - validation loss: 1.39702 - validation accuracy: 51.38 \n",
      "[34,     1] loss: 1.92004, adv_train_accuracy: 29.49, clean_train_accuracy : 45.51\n",
      "[34,    11] loss: 1.93707, adv_train_accuracy: 27.54, clean_train_accuracy : 49.02\n",
      "[34,    21] loss: 1.89964, adv_train_accuracy: 27.73, clean_train_accuracy : 47.07\n",
      "[34,    31] loss: 1.91325, adv_train_accuracy: 24.61, clean_train_accuracy : 42.38\n",
      "[34,    41] loss: 1.90566, adv_train_accuracy: 29.69, clean_train_accuracy : 45.51\n",
      "[34,    51] loss: 1.93126, adv_train_accuracy: 28.32, clean_train_accuracy : 43.36\n",
      "[34,    61] loss: 1.98537, adv_train_accuracy: 25.39, clean_train_accuracy : 43.36\n",
      "[34,    71] loss: 1.90125, adv_train_accuracy: 26.17, clean_train_accuracy : 47.07\n",
      "[34,    81] loss: 1.97483, adv_train_accuracy: 26.95, clean_train_accuracy : 40.43\n",
      "[34,    91] loss: 1.84495, adv_train_accuracy: 29.10, clean_train_accuracy : 45.51\n",
      "0.75234375\n",
      "0.6654296875\n",
      "duration: 185 s - train loss: 1.92634 - train accuracy: 27.23 - validation loss: 1.37605 - validation accuracy: 54.21 \n",
      "[35,     1] loss: 1.98166, adv_train_accuracy: 24.02, clean_train_accuracy : 42.77\n",
      "[35,    11] loss: 1.93168, adv_train_accuracy: 26.56, clean_train_accuracy : 46.29\n",
      "[35,    21] loss: 1.95447, adv_train_accuracy: 25.39, clean_train_accuracy : 46.68\n",
      "[35,    31] loss: 1.88304, adv_train_accuracy: 29.49, clean_train_accuracy : 41.80\n",
      "[35,    41] loss: 1.92102, adv_train_accuracy: 26.56, clean_train_accuracy : 45.70\n",
      "[35,    51] loss: 1.93347, adv_train_accuracy: 28.52, clean_train_accuracy : 43.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,    61] loss: 1.89965, adv_train_accuracy: 31.84, clean_train_accuracy : 48.05\n",
      "[35,    71] loss: 1.90523, adv_train_accuracy: 31.45, clean_train_accuracy : 46.09\n",
      "[35,    81] loss: 1.95820, adv_train_accuracy: 27.93, clean_train_accuracy : 42.19\n",
      "[35,    91] loss: 1.97237, adv_train_accuracy: 27.73, clean_train_accuracy : 47.07\n",
      "0.740234375\n",
      "0.65\n",
      "duration: 185 s - train loss: 1.92473 - train accuracy: 27.97 - validation loss: 1.36718 - validation accuracy: 54.26 \n",
      "[36,     1] loss: 1.90254, adv_train_accuracy: 28.12, clean_train_accuracy : 45.70\n",
      "[36,    11] loss: 1.93038, adv_train_accuracy: 28.91, clean_train_accuracy : 45.51\n",
      "[36,    21] loss: 1.88921, adv_train_accuracy: 30.27, clean_train_accuracy : 46.88\n",
      "[36,    31] loss: 1.92242, adv_train_accuracy: 28.32, clean_train_accuracy : 47.27\n",
      "[36,    41] loss: 1.92661, adv_train_accuracy: 27.73, clean_train_accuracy : 44.92\n",
      "[36,    51] loss: 1.88766, adv_train_accuracy: 29.49, clean_train_accuracy : 49.61\n",
      "[36,    61] loss: 1.92109, adv_train_accuracy: 26.95, clean_train_accuracy : 43.55\n",
      "[36,    71] loss: 1.92981, adv_train_accuracy: 28.91, clean_train_accuracy : 45.31\n",
      "[36,    81] loss: 1.93291, adv_train_accuracy: 25.20, clean_train_accuracy : 43.55\n",
      "[36,    91] loss: 1.99189, adv_train_accuracy: 22.07, clean_train_accuracy : 42.58\n",
      "0.732421875\n",
      "0.6515625\n",
      "duration: 185 s - train loss: 1.91748 - train accuracy: 28.03 - validation loss: 1.40384 - validation accuracy: 53.64 \n",
      "[37,     1] loss: 1.98127, adv_train_accuracy: 24.02, clean_train_accuracy : 42.38\n",
      "[37,    11] loss: 1.94633, adv_train_accuracy: 25.98, clean_train_accuracy : 45.31\n",
      "[37,    21] loss: 1.90142, adv_train_accuracy: 27.34, clean_train_accuracy : 47.07\n",
      "[37,    31] loss: 1.96308, adv_train_accuracy: 25.59, clean_train_accuracy : 41.80\n",
      "[37,    41] loss: 1.90679, adv_train_accuracy: 27.93, clean_train_accuracy : 43.55\n",
      "[37,    51] loss: 1.96837, adv_train_accuracy: 24.61, clean_train_accuracy : 40.23\n",
      "[37,    61] loss: 1.87433, adv_train_accuracy: 30.08, clean_train_accuracy : 46.29\n",
      "[37,    71] loss: 1.91965, adv_train_accuracy: 25.00, clean_train_accuracy : 45.31\n",
      "[37,    81] loss: 1.87268, adv_train_accuracy: 29.49, clean_train_accuracy : 46.88\n",
      "[37,    91] loss: 1.91240, adv_train_accuracy: 28.32, clean_train_accuracy : 48.05\n",
      "0.7396484375\n",
      "0.649609375\n",
      "duration: 185 s - train loss: 1.91747 - train accuracy: 28.06 - validation loss: 1.35927 - validation accuracy: 54.58 \n",
      "[38,     1] loss: 1.91420, adv_train_accuracy: 29.49, clean_train_accuracy : 46.48\n",
      "[38,    11] loss: 1.86244, adv_train_accuracy: 30.86, clean_train_accuracy : 48.24\n",
      "[38,    21] loss: 1.91893, adv_train_accuracy: 26.56, clean_train_accuracy : 47.07\n",
      "[38,    31] loss: 1.92667, adv_train_accuracy: 29.49, clean_train_accuracy : 46.68\n",
      "[38,    41] loss: 1.84069, adv_train_accuracy: 30.86, clean_train_accuracy : 50.00\n",
      "[38,    51] loss: 1.94503, adv_train_accuracy: 26.17, clean_train_accuracy : 41.60\n",
      "[38,    61] loss: 1.94513, adv_train_accuracy: 28.91, clean_train_accuracy : 43.95\n",
      "[38,    71] loss: 1.88207, adv_train_accuracy: 30.47, clean_train_accuracy : 46.88\n",
      "[38,    81] loss: 1.96441, adv_train_accuracy: 26.56, clean_train_accuracy : 45.51\n",
      "[38,    91] loss: 1.85990, adv_train_accuracy: 32.23, clean_train_accuracy : 48.44\n",
      "0.726953125\n",
      "0.651953125\n",
      "duration: 185 s - train loss: 1.90575 - train accuracy: 28.58 - validation loss: 1.35609 - validation accuracy: 54.78 \n",
      "[39,     1] loss: 1.89671, adv_train_accuracy: 24.02, clean_train_accuracy : 46.48\n",
      "[39,    11] loss: 1.90006, adv_train_accuracy: 30.86, clean_train_accuracy : 46.88\n",
      "[39,    21] loss: 1.88653, adv_train_accuracy: 27.93, clean_train_accuracy : 44.73\n",
      "[39,    31] loss: 1.92072, adv_train_accuracy: 28.12, clean_train_accuracy : 49.41\n",
      "[39,    41] loss: 1.90775, adv_train_accuracy: 30.86, clean_train_accuracy : 47.85\n",
      "[39,    51] loss: 1.99276, adv_train_accuracy: 25.00, clean_train_accuracy : 42.97\n",
      "[39,    61] loss: 1.92869, adv_train_accuracy: 28.52, clean_train_accuracy : 42.58\n",
      "[39,    71] loss: 1.91641, adv_train_accuracy: 29.49, clean_train_accuracy : 49.02\n",
      "[39,    81] loss: 1.93276, adv_train_accuracy: 26.37, clean_train_accuracy : 45.51\n",
      "[39,    91] loss: 1.87072, adv_train_accuracy: 31.25, clean_train_accuracy : 48.63\n",
      "0.7388671875\n",
      "0.6771484375\n",
      "duration: 184 s - train loss: 1.91117 - train accuracy: 28.22 - validation loss: 1.36228 - validation accuracy: 53.20 \n",
      "[40,     1] loss: 1.93218, adv_train_accuracy: 25.00, clean_train_accuracy : 44.92\n",
      "[40,    11] loss: 1.93157, adv_train_accuracy: 26.56, clean_train_accuracy : 43.95\n",
      "[40,    21] loss: 1.89725, adv_train_accuracy: 27.34, clean_train_accuracy : 48.63\n",
      "[40,    31] loss: 1.95768, adv_train_accuracy: 27.73, clean_train_accuracy : 47.66\n",
      "[40,    41] loss: 1.87133, adv_train_accuracy: 29.88, clean_train_accuracy : 48.83\n",
      "[40,    51] loss: 1.93324, adv_train_accuracy: 27.73, clean_train_accuracy : 44.34\n",
      "[40,    61] loss: 1.92495, adv_train_accuracy: 29.30, clean_train_accuracy : 49.02\n",
      "[40,    71] loss: 1.91148, adv_train_accuracy: 29.49, clean_train_accuracy : 45.51\n",
      "[40,    81] loss: 1.87526, adv_train_accuracy: 29.49, clean_train_accuracy : 50.39\n",
      "[40,    91] loss: 1.87463, adv_train_accuracy: 29.49, clean_train_accuracy : 49.80\n",
      "0.7216796875\n",
      "0.6470703125\n",
      "duration: 185 s - train loss: 1.89700 - train accuracy: 28.71 - validation loss: 1.33363 - validation accuracy: 55.23 \n",
      "[41,     1] loss: 1.92171, adv_train_accuracy: 28.12, clean_train_accuracy : 46.68\n",
      "[41,    11] loss: 1.89160, adv_train_accuracy: 27.73, clean_train_accuracy : 47.66\n",
      "[41,    21] loss: 1.90007, adv_train_accuracy: 26.17, clean_train_accuracy : 45.90\n",
      "[41,    31] loss: 1.88784, adv_train_accuracy: 29.10, clean_train_accuracy : 46.48\n",
      "[41,    41] loss: 1.90996, adv_train_accuracy: 25.98, clean_train_accuracy : 47.07\n",
      "[41,    51] loss: 1.97158, adv_train_accuracy: 25.78, clean_train_accuracy : 44.53\n",
      "[41,    61] loss: 1.84549, adv_train_accuracy: 29.88, clean_train_accuracy : 48.24\n",
      "[41,    71] loss: 1.86683, adv_train_accuracy: 29.10, clean_train_accuracy : 51.76\n",
      "[41,    81] loss: 1.85870, adv_train_accuracy: 28.12, clean_train_accuracy : 47.85\n",
      "[41,    91] loss: 1.94364, adv_train_accuracy: 26.95, clean_train_accuracy : 47.46\n",
      "0.7271484375\n",
      "0.638671875\n",
      "duration: 189 s - train loss: 1.89402 - train accuracy: 28.71 - validation loss: 1.31972 - validation accuracy: 57.58 \n",
      "[42,     1] loss: 1.87181, adv_train_accuracy: 26.76, clean_train_accuracy : 45.90\n",
      "[42,    11] loss: 1.90068, adv_train_accuracy: 28.12, clean_train_accuracy : 48.44\n",
      "[42,    21] loss: 1.87931, adv_train_accuracy: 29.69, clean_train_accuracy : 47.27\n",
      "[42,    31] loss: 1.87424, adv_train_accuracy: 30.47, clean_train_accuracy : 48.83\n",
      "[42,    41] loss: 1.84429, adv_train_accuracy: 28.52, clean_train_accuracy : 49.41\n",
      "[42,    51] loss: 1.90175, adv_train_accuracy: 29.30, clean_train_accuracy : 47.07\n",
      "[42,    61] loss: 1.86221, adv_train_accuracy: 26.76, clean_train_accuracy : 46.09\n",
      "[42,    71] loss: 1.91172, adv_train_accuracy: 30.08, clean_train_accuracy : 48.05\n",
      "[42,    81] loss: 1.86996, adv_train_accuracy: 30.66, clean_train_accuracy : 48.63\n",
      "[42,    91] loss: 1.87909, adv_train_accuracy: 29.88, clean_train_accuracy : 46.68\n",
      "0.735546875\n",
      "0.65859375\n",
      "duration: 192 s - train loss: 1.89533 - train accuracy: 29.04 - validation loss: 1.32293 - validation accuracy: 55.47 \n",
      "[43,     1] loss: 1.96540, adv_train_accuracy: 27.15, clean_train_accuracy : 45.51\n",
      "[43,    11] loss: 1.88573, adv_train_accuracy: 28.71, clean_train_accuracy : 43.75\n",
      "[43,    21] loss: 1.83771, adv_train_accuracy: 32.62, clean_train_accuracy : 51.76\n",
      "[43,    31] loss: 1.92145, adv_train_accuracy: 27.54, clean_train_accuracy : 48.44\n",
      "[43,    41] loss: 1.88323, adv_train_accuracy: 26.56, clean_train_accuracy : 46.48\n",
      "[43,    51] loss: 1.87254, adv_train_accuracy: 28.91, clean_train_accuracy : 50.78\n",
      "[43,    61] loss: 1.89440, adv_train_accuracy: 30.47, clean_train_accuracy : 46.88\n",
      "[43,    71] loss: 1.87195, adv_train_accuracy: 30.66, clean_train_accuracy : 50.98\n",
      "[43,    81] loss: 1.90738, adv_train_accuracy: 26.56, clean_train_accuracy : 44.53\n",
      "[43,    91] loss: 1.86149, adv_train_accuracy: 30.86, clean_train_accuracy : 48.44\n",
      "0.7349609375\n",
      "0.6341796875\n",
      "duration: 190 s - train loss: 1.88652 - train accuracy: 29.43 - validation loss: 1.29982 - validation accuracy: 56.87 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44,     1] loss: 1.82907, adv_train_accuracy: 29.10, clean_train_accuracy : 49.80\n",
      "[44,    11] loss: 1.97150, adv_train_accuracy: 23.44, clean_train_accuracy : 42.58\n",
      "[44,    21] loss: 1.89894, adv_train_accuracy: 26.76, clean_train_accuracy : 45.51\n",
      "[44,    31] loss: 1.88083, adv_train_accuracy: 27.73, clean_train_accuracy : 50.59\n",
      "[44,    41] loss: 1.94586, adv_train_accuracy: 27.15, clean_train_accuracy : 44.14\n",
      "[44,    51] loss: 1.89015, adv_train_accuracy: 31.05, clean_train_accuracy : 46.48\n",
      "[44,    61] loss: 1.89348, adv_train_accuracy: 30.66, clean_train_accuracy : 51.37\n",
      "[44,    71] loss: 1.91750, adv_train_accuracy: 25.20, clean_train_accuracy : 45.90\n",
      "[44,    81] loss: 1.86729, adv_train_accuracy: 29.49, clean_train_accuracy : 45.51\n",
      "[44,    91] loss: 1.86972, adv_train_accuracy: 29.30, clean_train_accuracy : 48.83\n",
      "0.7193359375\n",
      "0.6560546875\n",
      "duration: 192 s - train loss: 1.88369 - train accuracy: 29.11 - validation loss: 1.31582 - validation accuracy: 54.58 \n",
      "[45,     1] loss: 1.90236, adv_train_accuracy: 28.32, clean_train_accuracy : 46.48\n",
      "[45,    11] loss: 1.83664, adv_train_accuracy: 31.84, clean_train_accuracy : 50.00\n",
      "[45,    21] loss: 1.90796, adv_train_accuracy: 29.10, clean_train_accuracy : 46.68\n",
      "[45,    31] loss: 1.86094, adv_train_accuracy: 30.47, clean_train_accuracy : 49.80\n",
      "[45,    41] loss: 1.84294, adv_train_accuracy: 31.84, clean_train_accuracy : 50.59\n",
      "[45,    51] loss: 1.86875, adv_train_accuracy: 27.73, clean_train_accuracy : 48.24\n",
      "[45,    61] loss: 1.87465, adv_train_accuracy: 30.86, clean_train_accuracy : 50.00\n",
      "[45,    71] loss: 1.87712, adv_train_accuracy: 33.40, clean_train_accuracy : 51.17\n",
      "[45,    81] loss: 1.89166, adv_train_accuracy: 27.54, clean_train_accuracy : 46.68\n",
      "[45,    91] loss: 1.90079, adv_train_accuracy: 29.88, clean_train_accuracy : 48.44\n",
      "0.7271484375\n",
      "0.64296875\n",
      "duration: 186 s - train loss: 1.87852 - train accuracy: 29.50 - validation loss: 1.34099 - validation accuracy: 55.93 \n",
      "[46,     1] loss: 1.87589, adv_train_accuracy: 31.25, clean_train_accuracy : 47.07\n",
      "[46,    11] loss: 1.90343, adv_train_accuracy: 28.32, clean_train_accuracy : 46.09\n",
      "[46,    21] loss: 1.83687, adv_train_accuracy: 33.40, clean_train_accuracy : 47.85\n",
      "[46,    31] loss: 1.85282, adv_train_accuracy: 29.69, clean_train_accuracy : 50.39\n",
      "[46,    41] loss: 1.82587, adv_train_accuracy: 32.23, clean_train_accuracy : 50.78\n",
      "[46,    51] loss: 1.86038, adv_train_accuracy: 29.10, clean_train_accuracy : 46.29\n",
      "[46,    61] loss: 1.82546, adv_train_accuracy: 30.47, clean_train_accuracy : 54.10\n",
      "[46,    71] loss: 1.88662, adv_train_accuracy: 30.08, clean_train_accuracy : 48.63\n",
      "[46,    81] loss: 1.89796, adv_train_accuracy: 28.71, clean_train_accuracy : 47.27\n",
      "[46,    91] loss: 1.88161, adv_train_accuracy: 26.37, clean_train_accuracy : 45.70\n",
      "0.716015625\n",
      "0.6490234375\n",
      "duration: 185 s - train loss: 1.87377 - train accuracy: 29.55 - validation loss: 1.27250 - validation accuracy: 57.00 \n",
      "[47,     1] loss: 1.90135, adv_train_accuracy: 27.93, clean_train_accuracy : 46.68\n",
      "[47,    11] loss: 1.83856, adv_train_accuracy: 30.66, clean_train_accuracy : 50.59\n",
      "[47,    21] loss: 1.79744, adv_train_accuracy: 33.40, clean_train_accuracy : 51.17\n",
      "[47,    31] loss: 1.80455, adv_train_accuracy: 34.18, clean_train_accuracy : 51.95\n",
      "[47,    41] loss: 1.87686, adv_train_accuracy: 28.71, clean_train_accuracy : 49.02\n",
      "[47,    51] loss: 1.86126, adv_train_accuracy: 32.23, clean_train_accuracy : 49.80\n",
      "[47,    61] loss: 1.88182, adv_train_accuracy: 28.12, clean_train_accuracy : 47.85\n",
      "[47,    71] loss: 1.87686, adv_train_accuracy: 29.69, clean_train_accuracy : 47.66\n",
      "[47,    81] loss: 1.92762, adv_train_accuracy: 29.88, clean_train_accuracy : 46.48\n",
      "[47,    91] loss: 1.80362, adv_train_accuracy: 32.81, clean_train_accuracy : 50.59\n",
      "0.7265625\n",
      "0.640625\n",
      "duration: 185 s - train loss: 1.86876 - train accuracy: 29.91 - validation loss: 1.28659 - validation accuracy: 57.54 \n",
      "[48,     1] loss: 1.84396, adv_train_accuracy: 32.42, clean_train_accuracy : 52.93\n",
      "[48,    11] loss: 1.91061, adv_train_accuracy: 26.56, clean_train_accuracy : 46.88\n",
      "[48,    21] loss: 1.89797, adv_train_accuracy: 31.64, clean_train_accuracy : 49.80\n",
      "[48,    31] loss: 1.86148, adv_train_accuracy: 29.69, clean_train_accuracy : 48.44\n",
      "[48,    41] loss: 1.80516, adv_train_accuracy: 31.25, clean_train_accuracy : 51.56\n",
      "[48,    51] loss: 1.82677, adv_train_accuracy: 33.79, clean_train_accuracy : 49.61\n",
      "[48,    61] loss: 1.96100, adv_train_accuracy: 28.71, clean_train_accuracy : 43.75\n",
      "[48,    71] loss: 1.94855, adv_train_accuracy: 24.22, clean_train_accuracy : 47.46\n",
      "[48,    81] loss: 1.83072, adv_train_accuracy: 31.25, clean_train_accuracy : 53.71\n",
      "[48,    91] loss: 1.82888, adv_train_accuracy: 33.01, clean_train_accuracy : 50.98\n",
      "0.7212890625\n",
      "0.637890625\n",
      "duration: 191 s - train loss: 1.86860 - train accuracy: 29.68 - validation loss: 1.25611 - validation accuracy: 58.73 \n",
      "[49,     1] loss: 1.85793, adv_train_accuracy: 30.66, clean_train_accuracy : 47.66\n",
      "[49,    11] loss: 1.93492, adv_train_accuracy: 25.20, clean_train_accuracy : 45.51\n",
      "[49,    21] loss: 1.84069, adv_train_accuracy: 31.84, clean_train_accuracy : 50.59\n",
      "[49,    31] loss: 1.94370, adv_train_accuracy: 26.37, clean_train_accuracy : 47.46\n",
      "[49,    41] loss: 1.91145, adv_train_accuracy: 28.91, clean_train_accuracy : 45.70\n",
      "[49,    51] loss: 1.92067, adv_train_accuracy: 28.52, clean_train_accuracy : 49.61\n",
      "[49,    61] loss: 1.85167, adv_train_accuracy: 29.88, clean_train_accuracy : 52.54\n",
      "[49,    71] loss: 1.89292, adv_train_accuracy: 26.37, clean_train_accuracy : 46.48\n",
      "[49,    81] loss: 1.82868, adv_train_accuracy: 30.47, clean_train_accuracy : 48.05\n",
      "[49,    91] loss: 1.84191, adv_train_accuracy: 31.45, clean_train_accuracy : 50.59\n",
      "0.7212890625\n",
      "0.6333984375\n",
      "duration: 189 s - train loss: 1.86504 - train accuracy: 29.82 - validation loss: 1.22829 - validation accuracy: 59.26 \n",
      "[50,     1] loss: 1.85587, adv_train_accuracy: 31.05, clean_train_accuracy : 49.02\n",
      "[50,    11] loss: 1.80663, adv_train_accuracy: 31.84, clean_train_accuracy : 54.30\n",
      "[50,    21] loss: 1.87741, adv_train_accuracy: 29.30, clean_train_accuracy : 50.00\n",
      "[50,    31] loss: 1.83828, adv_train_accuracy: 28.91, clean_train_accuracy : 50.20\n",
      "[50,    41] loss: 1.88678, adv_train_accuracy: 29.10, clean_train_accuracy : 46.88\n",
      "[50,    51] loss: 1.88529, adv_train_accuracy: 26.95, clean_train_accuracy : 50.00\n",
      "[50,    61] loss: 1.82014, adv_train_accuracy: 31.45, clean_train_accuracy : 51.37\n",
      "[50,    71] loss: 1.81702, adv_train_accuracy: 30.08, clean_train_accuracy : 52.73\n",
      "[50,    81] loss: 1.83147, adv_train_accuracy: 32.81, clean_train_accuracy : 51.95\n",
      "[50,    91] loss: 1.87397, adv_train_accuracy: 29.69, clean_train_accuracy : 47.46\n",
      "0.724609375\n",
      "0.6486328125\n",
      "duration: 185 s - train loss: 1.86045 - train accuracy: 30.15 - validation loss: 1.26383 - validation accuracy: 57.99 \n",
      "[51,     1] loss: 1.81907, adv_train_accuracy: 31.25, clean_train_accuracy : 52.93\n",
      "[51,    11] loss: 1.81771, adv_train_accuracy: 29.88, clean_train_accuracy : 51.95\n",
      "[51,    21] loss: 1.89449, adv_train_accuracy: 29.69, clean_train_accuracy : 49.61\n",
      "[51,    31] loss: 1.87482, adv_train_accuracy: 30.27, clean_train_accuracy : 49.02\n",
      "[51,    41] loss: 1.85825, adv_train_accuracy: 30.66, clean_train_accuracy : 52.93\n",
      "[51,    51] loss: 1.79659, adv_train_accuracy: 33.01, clean_train_accuracy : 55.47\n",
      "[51,    61] loss: 1.89082, adv_train_accuracy: 28.91, clean_train_accuracy : 46.68\n",
      "[51,    71] loss: 1.81327, adv_train_accuracy: 32.03, clean_train_accuracy : 50.39\n",
      "[51,    81] loss: 1.86079, adv_train_accuracy: 29.30, clean_train_accuracy : 51.37\n",
      "[51,    91] loss: 1.87051, adv_train_accuracy: 27.93, clean_train_accuracy : 49.02\n",
      "0.6986328125\n",
      "0.636328125\n",
      "duration: 184 s - train loss: 1.85500 - train accuracy: 30.25 - validation loss: 1.22969 - validation accuracy: 57.90 \n",
      "[52,     1] loss: 1.83828, adv_train_accuracy: 30.47, clean_train_accuracy : 47.46\n",
      "[52,    11] loss: 1.87626, adv_train_accuracy: 32.62, clean_train_accuracy : 49.22\n",
      "[52,    21] loss: 1.85860, adv_train_accuracy: 28.12, clean_train_accuracy : 49.80\n",
      "[52,    31] loss: 1.86461, adv_train_accuracy: 31.84, clean_train_accuracy : 48.44\n",
      "[52,    41] loss: 1.84474, adv_train_accuracy: 30.08, clean_train_accuracy : 50.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52,    51] loss: 1.84816, adv_train_accuracy: 30.08, clean_train_accuracy : 50.98\n",
      "[52,    61] loss: 1.86620, adv_train_accuracy: 28.91, clean_train_accuracy : 50.39\n",
      "[52,    71] loss: 1.87298, adv_train_accuracy: 29.30, clean_train_accuracy : 46.88\n",
      "[52,    81] loss: 1.81303, adv_train_accuracy: 29.88, clean_train_accuracy : 52.34\n",
      "[52,    91] loss: 1.83687, adv_train_accuracy: 31.64, clean_train_accuracy : 50.98\n",
      "0.7154296875\n",
      "0.640625\n",
      "duration: 184 s - train loss: 1.84891 - train accuracy: 30.58 - validation loss: 1.21642 - validation accuracy: 58.12 \n",
      "[53,     1] loss: 1.81512, adv_train_accuracy: 28.71, clean_train_accuracy : 51.56\n",
      "[53,    11] loss: 1.84262, adv_train_accuracy: 32.81, clean_train_accuracy : 51.17\n",
      "[53,    21] loss: 1.78755, adv_train_accuracy: 27.93, clean_train_accuracy : 52.93\n",
      "[53,    31] loss: 1.90739, adv_train_accuracy: 27.54, clean_train_accuracy : 48.63\n",
      "[53,    41] loss: 1.93176, adv_train_accuracy: 23.44, clean_train_accuracy : 45.12\n",
      "[53,    51] loss: 1.89989, adv_train_accuracy: 29.10, clean_train_accuracy : 46.88\n",
      "[53,    61] loss: 1.86318, adv_train_accuracy: 30.66, clean_train_accuracy : 53.12\n",
      "[53,    71] loss: 1.81267, adv_train_accuracy: 30.86, clean_train_accuracy : 51.56\n",
      "[53,    81] loss: 1.85892, adv_train_accuracy: 29.69, clean_train_accuracy : 51.76\n",
      "[53,    91] loss: 1.83500, adv_train_accuracy: 30.86, clean_train_accuracy : 51.37\n",
      "0.7162109375\n",
      "0.651171875\n",
      "duration: 184 s - train loss: 1.85101 - train accuracy: 30.33 - validation loss: 1.25044 - validation accuracy: 57.15 \n",
      "[54,     1] loss: 1.82537, adv_train_accuracy: 30.66, clean_train_accuracy : 53.71\n",
      "[54,    11] loss: 1.77253, adv_train_accuracy: 32.81, clean_train_accuracy : 52.73\n",
      "[54,    21] loss: 1.85777, adv_train_accuracy: 32.23, clean_train_accuracy : 47.27\n",
      "[54,    31] loss: 1.82923, adv_train_accuracy: 31.25, clean_train_accuracy : 53.52\n",
      "[54,    41] loss: 1.85092, adv_train_accuracy: 27.73, clean_train_accuracy : 50.98\n",
      "[54,    51] loss: 1.89054, adv_train_accuracy: 25.98, clean_train_accuracy : 46.29\n",
      "[54,    61] loss: 1.82159, adv_train_accuracy: 33.79, clean_train_accuracy : 52.73\n",
      "[54,    71] loss: 1.85743, adv_train_accuracy: 25.39, clean_train_accuracy : 51.56\n",
      "[54,    81] loss: 1.82757, adv_train_accuracy: 31.64, clean_train_accuracy : 48.63\n",
      "[54,    91] loss: 1.91177, adv_train_accuracy: 27.93, clean_train_accuracy : 49.02\n",
      "0.7142578125\n",
      "0.6396484375\n",
      "duration: 184 s - train loss: 1.84657 - train accuracy: 30.77 - validation loss: 1.22299 - validation accuracy: 57.46 \n",
      "[55,     1] loss: 1.89428, adv_train_accuracy: 29.49, clean_train_accuracy : 46.88\n",
      "[55,    11] loss: 1.77808, adv_train_accuracy: 33.01, clean_train_accuracy : 51.95\n",
      "[55,    21] loss: 1.80853, adv_train_accuracy: 30.86, clean_train_accuracy : 53.32\n",
      "[55,    31] loss: 1.84999, adv_train_accuracy: 31.64, clean_train_accuracy : 52.93\n",
      "[55,    41] loss: 1.80876, adv_train_accuracy: 30.66, clean_train_accuracy : 52.15\n",
      "[55,    51] loss: 1.81339, adv_train_accuracy: 30.47, clean_train_accuracy : 51.56\n",
      "[55,    61] loss: 1.82321, adv_train_accuracy: 33.20, clean_train_accuracy : 51.95\n",
      "[55,    71] loss: 1.81974, adv_train_accuracy: 32.03, clean_train_accuracy : 52.93\n",
      "[55,    81] loss: 1.89015, adv_train_accuracy: 27.73, clean_train_accuracy : 50.00\n",
      "[55,    91] loss: 1.83102, adv_train_accuracy: 32.03, clean_train_accuracy : 51.76\n",
      "0.7244140625\n",
      "0.64609375\n",
      "duration: 192 s - train loss: 1.83753 - train accuracy: 30.98 - validation loss: 1.19444 - validation accuracy: 58.37 \n",
      "[56,     1] loss: 1.91400, adv_train_accuracy: 29.69, clean_train_accuracy : 51.37\n",
      "[56,    11] loss: 1.84652, adv_train_accuracy: 34.38, clean_train_accuracy : 51.17\n",
      "[56,    21] loss: 1.86615, adv_train_accuracy: 32.42, clean_train_accuracy : 48.63\n",
      "[56,    31] loss: 1.85289, adv_train_accuracy: 32.23, clean_train_accuracy : 52.73\n",
      "[56,    41] loss: 1.85099, adv_train_accuracy: 31.25, clean_train_accuracy : 51.76\n",
      "[56,    51] loss: 1.89093, adv_train_accuracy: 28.71, clean_train_accuracy : 48.83\n",
      "[56,    61] loss: 1.76859, adv_train_accuracy: 30.27, clean_train_accuracy : 53.52\n",
      "[56,    71] loss: 1.78281, adv_train_accuracy: 32.42, clean_train_accuracy : 53.12\n",
      "[56,    81] loss: 1.87250, adv_train_accuracy: 29.69, clean_train_accuracy : 49.41\n",
      "[56,    91] loss: 1.82500, adv_train_accuracy: 34.38, clean_train_accuracy : 54.30\n",
      "0.7037109375\n",
      "0.6412109375\n",
      "duration: 191 s - train loss: 1.83565 - train accuracy: 31.18 - validation loss: 1.20231 - validation accuracy: 58.50 \n",
      "[57,     1] loss: 1.70110, adv_train_accuracy: 36.33, clean_train_accuracy : 55.47\n",
      "[57,    11] loss: 1.75811, adv_train_accuracy: 35.35, clean_train_accuracy : 53.71\n",
      "[57,    21] loss: 1.77784, adv_train_accuracy: 33.79, clean_train_accuracy : 52.34\n",
      "[57,    31] loss: 1.79807, adv_train_accuracy: 31.84, clean_train_accuracy : 53.12\n",
      "[57,    41] loss: 1.81991, adv_train_accuracy: 31.84, clean_train_accuracy : 52.34\n",
      "[57,    51] loss: 1.84424, adv_train_accuracy: 30.08, clean_train_accuracy : 50.78\n",
      "[57,    61] loss: 1.86325, adv_train_accuracy: 30.47, clean_train_accuracy : 50.00\n",
      "[57,    71] loss: 1.90073, adv_train_accuracy: 29.10, clean_train_accuracy : 48.63\n",
      "[57,    81] loss: 1.82075, adv_train_accuracy: 29.49, clean_train_accuracy : 50.78\n",
      "[57,    91] loss: 1.82548, adv_train_accuracy: 32.62, clean_train_accuracy : 50.59\n",
      "0.6970703125\n",
      "0.6298828125\n",
      "duration: 185 s - train loss: 1.83216 - train accuracy: 31.18 - validation loss: 1.18026 - validation accuracy: 60.69 \n",
      "[58,     1] loss: 1.82728, adv_train_accuracy: 31.05, clean_train_accuracy : 52.93\n",
      "[58,    11] loss: 1.79719, adv_train_accuracy: 30.27, clean_train_accuracy : 49.80\n",
      "[58,    21] loss: 1.84867, adv_train_accuracy: 33.20, clean_train_accuracy : 53.32\n",
      "[58,    31] loss: 1.87687, adv_train_accuracy: 31.25, clean_train_accuracy : 51.76\n",
      "[58,    41] loss: 1.82872, adv_train_accuracy: 32.23, clean_train_accuracy : 50.00\n",
      "[58,    51] loss: 1.78469, adv_train_accuracy: 36.33, clean_train_accuracy : 52.54\n",
      "[58,    61] loss: 1.91470, adv_train_accuracy: 28.71, clean_train_accuracy : 52.54\n",
      "[58,    71] loss: 1.79656, adv_train_accuracy: 31.25, clean_train_accuracy : 52.34\n",
      "[58,    81] loss: 1.81653, adv_train_accuracy: 31.64, clean_train_accuracy : 52.93\n",
      "[58,    91] loss: 1.78232, adv_train_accuracy: 32.42, clean_train_accuracy : 52.54\n",
      "0.7255859375\n",
      "0.648046875\n",
      "duration: 185 s - train loss: 1.83190 - train accuracy: 31.33 - validation loss: 1.20026 - validation accuracy: 59.95 \n",
      "[59,     1] loss: 1.82859, adv_train_accuracy: 32.23, clean_train_accuracy : 54.88\n",
      "[59,    11] loss: 1.84100, adv_train_accuracy: 32.03, clean_train_accuracy : 51.76\n",
      "[59,    21] loss: 1.81684, adv_train_accuracy: 30.86, clean_train_accuracy : 51.76\n",
      "[59,    31] loss: 1.79115, adv_train_accuracy: 33.40, clean_train_accuracy : 53.12\n",
      "[59,    41] loss: 1.88071, adv_train_accuracy: 30.27, clean_train_accuracy : 53.32\n",
      "[59,    51] loss: 1.81459, adv_train_accuracy: 33.40, clean_train_accuracy : 53.12\n",
      "[59,    61] loss: 1.77613, adv_train_accuracy: 35.74, clean_train_accuracy : 54.49\n",
      "[59,    71] loss: 1.83281, adv_train_accuracy: 27.54, clean_train_accuracy : 51.95\n",
      "[59,    81] loss: 1.76835, adv_train_accuracy: 33.98, clean_train_accuracy : 55.47\n",
      "[59,    91] loss: 1.80627, adv_train_accuracy: 30.66, clean_train_accuracy : 47.85\n",
      "0.7068359375\n",
      "0.6275390625\n",
      "duration: 185 s - train loss: 1.81970 - train accuracy: 31.57 - validation loss: 1.16089 - validation accuracy: 61.13 \n",
      "[60,     1] loss: 1.81086, adv_train_accuracy: 32.81, clean_train_accuracy : 54.69\n",
      "[60,    11] loss: 1.85631, adv_train_accuracy: 29.10, clean_train_accuracy : 51.17\n",
      "[60,    21] loss: 1.85061, adv_train_accuracy: 28.91, clean_train_accuracy : 48.05\n",
      "[60,    31] loss: 1.76465, adv_train_accuracy: 35.16, clean_train_accuracy : 52.73\n",
      "[60,    41] loss: 1.90380, adv_train_accuracy: 28.12, clean_train_accuracy : 49.80\n",
      "[60,    51] loss: 1.79795, adv_train_accuracy: 34.18, clean_train_accuracy : 55.08\n",
      "[60,    61] loss: 1.76727, adv_train_accuracy: 34.96, clean_train_accuracy : 55.47\n",
      "[60,    71] loss: 1.78509, adv_train_accuracy: 31.84, clean_train_accuracy : 52.15\n",
      "[60,    81] loss: 1.79157, adv_train_accuracy: 33.01, clean_train_accuracy : 52.34\n",
      "[60,    91] loss: 1.81787, adv_train_accuracy: 33.79, clean_train_accuracy : 52.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931640625\n",
      "0.6283203125\n",
      "duration: 185 s - train loss: 1.82080 - train accuracy: 31.54 - validation loss: 1.17518 - validation accuracy: 60.01 \n",
      "[61,     1] loss: 1.82978, adv_train_accuracy: 31.25, clean_train_accuracy : 54.88\n",
      "[61,    11] loss: 1.80540, adv_train_accuracy: 30.66, clean_train_accuracy : 56.25\n",
      "[61,    21] loss: 1.87973, adv_train_accuracy: 28.71, clean_train_accuracy : 47.66\n",
      "[61,    31] loss: 1.82334, adv_train_accuracy: 30.47, clean_train_accuracy : 49.41\n",
      "[61,    41] loss: 1.76350, adv_train_accuracy: 32.42, clean_train_accuracy : 55.27\n",
      "[61,    51] loss: 1.78447, adv_train_accuracy: 35.35, clean_train_accuracy : 53.12\n",
      "[61,    61] loss: 1.85297, adv_train_accuracy: 32.62, clean_train_accuracy : 50.00\n",
      "[61,    71] loss: 1.83581, adv_train_accuracy: 28.71, clean_train_accuracy : 50.78\n",
      "[61,    81] loss: 1.83810, adv_train_accuracy: 30.86, clean_train_accuracy : 51.76\n",
      "[61,    91] loss: 1.82796, adv_train_accuracy: 31.64, clean_train_accuracy : 52.93\n",
      "0.7060546875\n",
      "0.6279296875\n",
      "duration: 185 s - train loss: 1.81676 - train accuracy: 32.02 - validation loss: 1.17150 - validation accuracy: 61.35 \n",
      "[62,     1] loss: 1.77587, adv_train_accuracy: 31.05, clean_train_accuracy : 53.71\n",
      "[62,    11] loss: 1.78515, adv_train_accuracy: 33.20, clean_train_accuracy : 56.45\n",
      "[62,    21] loss: 1.80308, adv_train_accuracy: 32.62, clean_train_accuracy : 56.25\n",
      "[62,    31] loss: 1.83750, adv_train_accuracy: 33.59, clean_train_accuracy : 53.91\n",
      "[62,    41] loss: 1.82546, adv_train_accuracy: 32.62, clean_train_accuracy : 52.15\n",
      "[62,    51] loss: 1.75772, adv_train_accuracy: 31.25, clean_train_accuracy : 56.84\n",
      "[62,    61] loss: 1.84657, adv_train_accuracy: 29.69, clean_train_accuracy : 51.76\n",
      "[62,    71] loss: 1.84330, adv_train_accuracy: 31.45, clean_train_accuracy : 50.00\n",
      "[62,    81] loss: 1.78665, adv_train_accuracy: 31.45, clean_train_accuracy : 55.47\n",
      "[62,    91] loss: 1.84023, adv_train_accuracy: 30.08, clean_train_accuracy : 51.37\n",
      "0.6998046875\n",
      "0.61015625\n",
      "duration: 185 s - train loss: 1.81285 - train accuracy: 31.91 - validation loss: 1.13977 - validation accuracy: 61.61 \n",
      "[63,     1] loss: 1.83299, adv_train_accuracy: 30.66, clean_train_accuracy : 51.37\n",
      "[63,    11] loss: 1.76087, adv_train_accuracy: 33.20, clean_train_accuracy : 56.45\n",
      "[63,    21] loss: 1.81561, adv_train_accuracy: 32.03, clean_train_accuracy : 54.88\n",
      "[63,    31] loss: 1.76221, adv_train_accuracy: 32.23, clean_train_accuracy : 53.71\n",
      "[63,    41] loss: 1.87057, adv_train_accuracy: 28.52, clean_train_accuracy : 50.78\n",
      "[63,    51] loss: 1.82846, adv_train_accuracy: 30.86, clean_train_accuracy : 51.76\n",
      "[63,    61] loss: 1.85879, adv_train_accuracy: 29.49, clean_train_accuracy : 50.20\n",
      "[63,    71] loss: 1.78549, adv_train_accuracy: 31.25, clean_train_accuracy : 53.32\n",
      "[63,    81] loss: 1.81077, adv_train_accuracy: 33.98, clean_train_accuracy : 54.30\n",
      "[63,    91] loss: 1.78927, adv_train_accuracy: 30.47, clean_train_accuracy : 51.76\n",
      "0.7205078125\n",
      "0.62109375\n",
      "duration: 185 s - train loss: 1.80884 - train accuracy: 32.01 - validation loss: 1.15485 - validation accuracy: 61.89 \n",
      "[64,     1] loss: 1.82101, adv_train_accuracy: 35.35, clean_train_accuracy : 54.49\n",
      "[64,    11] loss: 1.80155, adv_train_accuracy: 31.25, clean_train_accuracy : 53.12\n",
      "[64,    21] loss: 1.77325, adv_train_accuracy: 32.03, clean_train_accuracy : 54.10\n",
      "[64,    31] loss: 1.78338, adv_train_accuracy: 32.62, clean_train_accuracy : 53.52\n",
      "[64,    41] loss: 1.81286, adv_train_accuracy: 28.52, clean_train_accuracy : 51.95\n",
      "[64,    51] loss: 1.77678, adv_train_accuracy: 34.57, clean_train_accuracy : 53.71\n",
      "[64,    61] loss: 1.78480, adv_train_accuracy: 33.79, clean_train_accuracy : 54.30\n",
      "[64,    71] loss: 1.78811, adv_train_accuracy: 32.03, clean_train_accuracy : 51.37\n",
      "[64,    81] loss: 1.87019, adv_train_accuracy: 30.08, clean_train_accuracy : 49.02\n",
      "[64,    91] loss: 1.74412, adv_train_accuracy: 34.77, clean_train_accuracy : 55.27\n",
      "0.70078125\n",
      "0.628515625\n",
      "duration: 192 s - train loss: 1.80199 - train accuracy: 32.34 - validation loss: 1.13468 - validation accuracy: 61.70 \n",
      "[65,     1] loss: 1.73069, adv_train_accuracy: 34.18, clean_train_accuracy : 55.66\n",
      "[65,    11] loss: 1.83188, adv_train_accuracy: 32.23, clean_train_accuracy : 53.12\n",
      "[65,    21] loss: 1.78648, adv_train_accuracy: 33.40, clean_train_accuracy : 55.27\n",
      "[65,    31] loss: 1.79118, adv_train_accuracy: 34.18, clean_train_accuracy : 55.47\n",
      "[65,    41] loss: 1.82547, adv_train_accuracy: 31.05, clean_train_accuracy : 53.32\n",
      "[65,    51] loss: 1.82618, adv_train_accuracy: 31.05, clean_train_accuracy : 52.54\n",
      "[65,    61] loss: 1.82102, adv_train_accuracy: 32.23, clean_train_accuracy : 51.37\n",
      "[65,    71] loss: 1.87370, adv_train_accuracy: 28.32, clean_train_accuracy : 50.59\n",
      "[65,    81] loss: 1.75216, adv_train_accuracy: 32.23, clean_train_accuracy : 55.08\n",
      "[65,    91] loss: 1.83068, adv_train_accuracy: 31.05, clean_train_accuracy : 52.15\n",
      "0.688671875\n",
      "0.609765625\n",
      "duration: 186 s - train loss: 1.81007 - train accuracy: 32.30 - validation loss: 1.15110 - validation accuracy: 63.18 \n",
      "[66,     1] loss: 1.76736, adv_train_accuracy: 33.98, clean_train_accuracy : 55.47\n",
      "[66,    11] loss: 1.78783, adv_train_accuracy: 33.98, clean_train_accuracy : 51.95\n",
      "[66,    21] loss: 1.74917, adv_train_accuracy: 33.40, clean_train_accuracy : 55.27\n",
      "[66,    31] loss: 1.82618, adv_train_accuracy: 32.81, clean_train_accuracy : 56.45\n",
      "[66,    41] loss: 1.85962, adv_train_accuracy: 31.25, clean_train_accuracy : 50.98\n",
      "[66,    51] loss: 1.78083, adv_train_accuracy: 33.59, clean_train_accuracy : 56.45\n",
      "[66,    61] loss: 1.85257, adv_train_accuracy: 30.86, clean_train_accuracy : 51.37\n",
      "[66,    71] loss: 1.80662, adv_train_accuracy: 30.66, clean_train_accuracy : 54.10\n",
      "[66,    81] loss: 1.80947, adv_train_accuracy: 32.03, clean_train_accuracy : 52.34\n",
      "[66,    91] loss: 1.85336, adv_train_accuracy: 31.64, clean_train_accuracy : 50.78\n",
      "0.6939453125\n",
      "0.5974609375\n",
      "duration: 185 s - train loss: 1.79993 - train accuracy: 32.31 - validation loss: 1.14112 - validation accuracy: 61.30 \n",
      "[67,     1] loss: 1.77071, adv_train_accuracy: 35.55, clean_train_accuracy : 55.27\n",
      "[67,    11] loss: 1.77688, adv_train_accuracy: 35.35, clean_train_accuracy : 54.49\n",
      "[67,    21] loss: 1.78692, adv_train_accuracy: 32.03, clean_train_accuracy : 54.88\n",
      "[67,    31] loss: 1.82031, adv_train_accuracy: 34.18, clean_train_accuracy : 55.27\n",
      "[67,    41] loss: 1.78239, adv_train_accuracy: 33.01, clean_train_accuracy : 52.93\n",
      "[67,    51] loss: 1.76181, adv_train_accuracy: 32.23, clean_train_accuracy : 55.86\n",
      "[67,    61] loss: 1.84344, adv_train_accuracy: 30.27, clean_train_accuracy : 50.00\n",
      "[67,    71] loss: 1.73179, adv_train_accuracy: 34.38, clean_train_accuracy : 55.47\n",
      "[67,    81] loss: 1.81167, adv_train_accuracy: 31.84, clean_train_accuracy : 49.80\n",
      "[67,    91] loss: 1.82052, adv_train_accuracy: 31.84, clean_train_accuracy : 52.54\n",
      "0.70546875\n",
      "0.6314453125\n",
      "duration: 185 s - train loss: 1.79398 - train accuracy: 32.80 - validation loss: 1.12018 - validation accuracy: 62.73 \n",
      "[68,     1] loss: 1.77100, adv_train_accuracy: 34.77, clean_train_accuracy : 54.49\n",
      "[68,    11] loss: 1.80042, adv_train_accuracy: 30.08, clean_train_accuracy : 54.10\n",
      "[68,    21] loss: 1.78728, adv_train_accuracy: 33.20, clean_train_accuracy : 53.32\n",
      "[68,    31] loss: 1.80501, adv_train_accuracy: 32.42, clean_train_accuracy : 55.86\n",
      "[68,    41] loss: 1.75060, adv_train_accuracy: 34.77, clean_train_accuracy : 56.25\n",
      "[68,    51] loss: 1.79422, adv_train_accuracy: 32.62, clean_train_accuracy : 53.71\n",
      "[68,    61] loss: 1.75719, adv_train_accuracy: 35.16, clean_train_accuracy : 57.03\n",
      "[68,    71] loss: 1.79508, adv_train_accuracy: 33.01, clean_train_accuracy : 53.91\n",
      "[68,    81] loss: 1.75579, adv_train_accuracy: 35.74, clean_train_accuracy : 58.01\n",
      "[68,    91] loss: 1.73673, adv_train_accuracy: 35.55, clean_train_accuracy : 57.03\n",
      "0.7046875\n",
      "0.61953125\n",
      "duration: 184 s - train loss: 1.79673 - train accuracy: 32.67 - validation loss: 1.13217 - validation accuracy: 61.23 \n",
      "[69,     1] loss: 1.77483, adv_train_accuracy: 32.81, clean_train_accuracy : 54.49\n",
      "[69,    11] loss: 1.79386, adv_train_accuracy: 31.84, clean_train_accuracy : 53.52\n",
      "[69,    21] loss: 1.83781, adv_train_accuracy: 32.23, clean_train_accuracy : 56.64\n",
      "[69,    31] loss: 1.86456, adv_train_accuracy: 30.08, clean_train_accuracy : 51.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69,    41] loss: 1.78701, adv_train_accuracy: 33.98, clean_train_accuracy : 54.10\n",
      "[69,    51] loss: 1.79058, adv_train_accuracy: 33.40, clean_train_accuracy : 55.66\n",
      "[69,    61] loss: 1.83678, adv_train_accuracy: 31.84, clean_train_accuracy : 51.95\n",
      "[69,    71] loss: 1.79002, adv_train_accuracy: 34.77, clean_train_accuracy : 55.47\n",
      "[69,    81] loss: 1.79453, adv_train_accuracy: 34.77, clean_train_accuracy : 51.17\n",
      "[69,    91] loss: 1.76140, adv_train_accuracy: 32.62, clean_train_accuracy : 51.17\n",
      "0.6982421875\n",
      "0.6232421875\n",
      "duration: 184 s - train loss: 1.79347 - train accuracy: 32.76 - validation loss: 1.10884 - validation accuracy: 61.98 \n",
      "[70,     1] loss: 1.80111, adv_train_accuracy: 31.84, clean_train_accuracy : 50.98\n",
      "[70,    11] loss: 1.81565, adv_train_accuracy: 33.98, clean_train_accuracy : 54.30\n",
      "[70,    21] loss: 1.78711, adv_train_accuracy: 34.18, clean_train_accuracy : 56.84\n",
      "[70,    31] loss: 1.66340, adv_train_accuracy: 41.02, clean_train_accuracy : 57.42\n",
      "[70,    41] loss: 1.75433, adv_train_accuracy: 33.40, clean_train_accuracy : 56.84\n",
      "[70,    51] loss: 1.85014, adv_train_accuracy: 28.32, clean_train_accuracy : 51.76\n",
      "[70,    61] loss: 1.79018, adv_train_accuracy: 34.38, clean_train_accuracy : 53.71\n",
      "[70,    71] loss: 1.74197, adv_train_accuracy: 34.96, clean_train_accuracy : 57.23\n",
      "[70,    81] loss: 1.79374, adv_train_accuracy: 34.18, clean_train_accuracy : 55.08\n",
      "[70,    91] loss: 1.76523, adv_train_accuracy: 36.13, clean_train_accuracy : 55.27\n",
      "0.708984375\n",
      "0.603515625\n",
      "duration: 184 s - train loss: 1.78990 - train accuracy: 32.92 - validation loss: 1.09237 - validation accuracy: 63.93 \n",
      "[71,     1] loss: 1.78337, adv_train_accuracy: 34.18, clean_train_accuracy : 57.23\n",
      "[71,    11] loss: 1.75876, adv_train_accuracy: 36.33, clean_train_accuracy : 57.81\n",
      "[71,    21] loss: 1.78244, adv_train_accuracy: 31.64, clean_train_accuracy : 56.05\n",
      "[71,    31] loss: 1.76110, adv_train_accuracy: 33.98, clean_train_accuracy : 54.88\n",
      "[71,    41] loss: 1.72781, adv_train_accuracy: 34.96, clean_train_accuracy : 55.27\n",
      "[71,    51] loss: 1.77588, adv_train_accuracy: 37.30, clean_train_accuracy : 56.45\n",
      "[71,    61] loss: 1.81208, adv_train_accuracy: 31.64, clean_train_accuracy : 54.49\n",
      "[71,    71] loss: 1.75478, adv_train_accuracy: 35.16, clean_train_accuracy : 57.42\n",
      "[71,    81] loss: 1.78037, adv_train_accuracy: 33.01, clean_train_accuracy : 54.88\n",
      "[71,    91] loss: 1.80801, adv_train_accuracy: 31.84, clean_train_accuracy : 54.30\n",
      "0.69453125\n",
      "0.618359375\n",
      "duration: 184 s - train loss: 1.78353 - train accuracy: 33.10 - validation loss: 1.11819 - validation accuracy: 63.37 \n",
      "[72,     1] loss: 1.76526, adv_train_accuracy: 32.03, clean_train_accuracy : 57.62\n",
      "[72,    11] loss: 1.76015, adv_train_accuracy: 33.79, clean_train_accuracy : 56.84\n",
      "[72,    21] loss: 1.75177, adv_train_accuracy: 31.84, clean_train_accuracy : 57.62\n",
      "[72,    31] loss: 1.84359, adv_train_accuracy: 31.84, clean_train_accuracy : 52.73\n",
      "[72,    41] loss: 1.78470, adv_train_accuracy: 33.20, clean_train_accuracy : 55.86\n",
      "[72,    51] loss: 1.82494, adv_train_accuracy: 28.71, clean_train_accuracy : 51.95\n",
      "[72,    61] loss: 1.76538, adv_train_accuracy: 33.79, clean_train_accuracy : 55.27\n",
      "[72,    71] loss: 1.79947, adv_train_accuracy: 31.05, clean_train_accuracy : 55.08\n",
      "[72,    81] loss: 1.76540, adv_train_accuracy: 32.62, clean_train_accuracy : 56.84\n",
      "[72,    91] loss: 1.81416, adv_train_accuracy: 34.18, clean_train_accuracy : 55.27\n",
      "0.6818359375\n",
      "0.6048828125\n",
      "duration: 184 s - train loss: 1.77943 - train accuracy: 32.92 - validation loss: 1.09205 - validation accuracy: 63.63 \n",
      "[73,     1] loss: 1.69358, adv_train_accuracy: 33.20, clean_train_accuracy : 54.69\n",
      "[73,    11] loss: 1.80394, adv_train_accuracy: 31.05, clean_train_accuracy : 53.71\n",
      "[73,    21] loss: 1.75478, adv_train_accuracy: 38.09, clean_train_accuracy : 56.45\n",
      "[73,    31] loss: 1.83790, adv_train_accuracy: 29.69, clean_train_accuracy : 54.30\n",
      "[73,    41] loss: 1.79810, adv_train_accuracy: 35.35, clean_train_accuracy : 53.71\n",
      "[73,    51] loss: 1.75294, adv_train_accuracy: 32.42, clean_train_accuracy : 54.30\n",
      "[73,    61] loss: 1.74287, adv_train_accuracy: 33.79, clean_train_accuracy : 55.66\n",
      "[73,    71] loss: 1.87044, adv_train_accuracy: 27.15, clean_train_accuracy : 51.95\n",
      "[73,    81] loss: 1.79398, adv_train_accuracy: 33.40, clean_train_accuracy : 55.08\n",
      "[73,    91] loss: 1.75953, adv_train_accuracy: 33.01, clean_train_accuracy : 57.23\n",
      "0.698828125\n",
      "0.6021484375\n",
      "duration: 184 s - train loss: 1.78061 - train accuracy: 33.14 - validation loss: 1.11286 - validation accuracy: 63.37 \n",
      "[74,     1] loss: 1.84155, adv_train_accuracy: 32.23, clean_train_accuracy : 53.32\n",
      "[74,    11] loss: 1.78938, adv_train_accuracy: 33.40, clean_train_accuracy : 55.86\n",
      "[74,    21] loss: 1.82446, adv_train_accuracy: 32.03, clean_train_accuracy : 53.91\n",
      "[74,    31] loss: 1.75109, adv_train_accuracy: 35.16, clean_train_accuracy : 57.23\n",
      "[74,    41] loss: 1.76400, adv_train_accuracy: 34.38, clean_train_accuracy : 54.69\n",
      "[74,    51] loss: 1.78955, adv_train_accuracy: 30.66, clean_train_accuracy : 53.71\n",
      "[74,    61] loss: 1.80061, adv_train_accuracy: 32.23, clean_train_accuracy : 55.66\n",
      "[74,    71] loss: 1.73371, adv_train_accuracy: 35.94, clean_train_accuracy : 58.01\n",
      "[74,    81] loss: 1.75594, adv_train_accuracy: 32.62, clean_train_accuracy : 55.86\n",
      "[74,    91] loss: 1.79109, adv_train_accuracy: 33.40, clean_train_accuracy : 55.66\n",
      "0.6869140625\n",
      "0.5943359375\n",
      "duration: 184 s - train loss: 1.77409 - train accuracy: 33.44 - validation loss: 1.09154 - validation accuracy: 64.25 \n",
      "[75,     1] loss: 1.73320, adv_train_accuracy: 33.40, clean_train_accuracy : 53.52\n",
      "[75,    11] loss: 1.81634, adv_train_accuracy: 29.10, clean_train_accuracy : 52.93\n",
      "[75,    21] loss: 1.75856, adv_train_accuracy: 32.23, clean_train_accuracy : 56.05\n",
      "[75,    31] loss: 1.70662, adv_train_accuracy: 35.16, clean_train_accuracy : 55.27\n",
      "[75,    41] loss: 1.83523, adv_train_accuracy: 29.49, clean_train_accuracy : 51.56\n",
      "[75,    51] loss: 1.79957, adv_train_accuracy: 33.01, clean_train_accuracy : 55.08\n",
      "[75,    61] loss: 1.78225, adv_train_accuracy: 32.03, clean_train_accuracy : 56.84\n",
      "[75,    71] loss: 1.75628, adv_train_accuracy: 35.94, clean_train_accuracy : 56.64\n",
      "[75,    81] loss: 1.84623, adv_train_accuracy: 29.88, clean_train_accuracy : 51.56\n",
      "[75,    91] loss: 1.79489, adv_train_accuracy: 29.88, clean_train_accuracy : 53.71\n",
      "0.68984375\n",
      "0.619140625\n",
      "duration: 184 s - train loss: 1.76878 - train accuracy: 33.35 - validation loss: 1.07952 - validation accuracy: 62.32 \n",
      "[76,     1] loss: 1.72967, adv_train_accuracy: 35.94, clean_train_accuracy : 53.91\n",
      "[76,    11] loss: 1.78038, adv_train_accuracy: 34.18, clean_train_accuracy : 57.23\n",
      "[76,    21] loss: 1.74390, adv_train_accuracy: 34.96, clean_train_accuracy : 55.47\n",
      "[76,    31] loss: 1.70894, adv_train_accuracy: 37.89, clean_train_accuracy : 58.98\n",
      "[76,    41] loss: 1.74588, adv_train_accuracy: 31.84, clean_train_accuracy : 56.05\n",
      "[76,    51] loss: 1.72402, adv_train_accuracy: 34.96, clean_train_accuracy : 58.79\n",
      "[76,    61] loss: 1.79867, adv_train_accuracy: 33.79, clean_train_accuracy : 52.93\n",
      "[76,    71] loss: 1.68519, adv_train_accuracy: 37.89, clean_train_accuracy : 58.20\n",
      "[76,    81] loss: 1.74714, adv_train_accuracy: 33.98, clean_train_accuracy : 54.69\n",
      "[76,    91] loss: 1.79009, adv_train_accuracy: 31.64, clean_train_accuracy : 54.69\n",
      "0.676953125\n",
      "0.6017578125\n",
      "duration: 184 s - train loss: 1.76121 - train accuracy: 33.72 - validation loss: 1.08268 - validation accuracy: 62.68 \n",
      "[77,     1] loss: 1.70716, adv_train_accuracy: 35.94, clean_train_accuracy : 57.23\n",
      "[77,    11] loss: 1.75692, adv_train_accuracy: 35.55, clean_train_accuracy : 56.64\n",
      "[77,    21] loss: 1.77748, adv_train_accuracy: 32.42, clean_train_accuracy : 55.27\n",
      "[77,    31] loss: 1.81557, adv_train_accuracy: 29.49, clean_train_accuracy : 52.54\n",
      "[77,    41] loss: 1.78690, adv_train_accuracy: 31.84, clean_train_accuracy : 54.49\n",
      "[77,    51] loss: 1.74130, adv_train_accuracy: 35.16, clean_train_accuracy : 54.10\n",
      "[77,    61] loss: 1.76675, adv_train_accuracy: 32.81, clean_train_accuracy : 57.23\n",
      "[77,    71] loss: 1.74476, adv_train_accuracy: 37.50, clean_train_accuracy : 58.59\n",
      "[77,    81] loss: 1.76477, adv_train_accuracy: 33.79, clean_train_accuracy : 56.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77,    91] loss: 1.77787, adv_train_accuracy: 31.64, clean_train_accuracy : 54.10\n",
      "0.6841796875\n",
      "0.6279296875\n",
      "duration: 184 s - train loss: 1.76894 - train accuracy: 33.65 - validation loss: 1.10633 - validation accuracy: 61.68 \n",
      "[78,     1] loss: 1.69862, adv_train_accuracy: 37.50, clean_train_accuracy : 59.57\n",
      "[78,    11] loss: 1.78136, adv_train_accuracy: 37.50, clean_train_accuracy : 58.20\n",
      "[78,    21] loss: 1.79831, adv_train_accuracy: 32.81, clean_train_accuracy : 53.71\n",
      "[78,    31] loss: 1.79534, adv_train_accuracy: 33.79, clean_train_accuracy : 55.66\n",
      "[78,    41] loss: 1.79677, adv_train_accuracy: 31.84, clean_train_accuracy : 56.45\n",
      "[78,    51] loss: 1.76870, adv_train_accuracy: 32.62, clean_train_accuracy : 56.45\n",
      "[78,    61] loss: 1.74416, adv_train_accuracy: 34.96, clean_train_accuracy : 54.10\n",
      "[78,    71] loss: 1.77910, adv_train_accuracy: 30.86, clean_train_accuracy : 56.64\n",
      "[78,    81] loss: 1.74571, adv_train_accuracy: 33.98, clean_train_accuracy : 56.45\n",
      "[78,    91] loss: 1.71968, adv_train_accuracy: 37.89, clean_train_accuracy : 59.77\n",
      "0.68359375\n",
      "0.6130859375\n",
      "duration: 185 s - train loss: 1.76440 - train accuracy: 33.74 - validation loss: 1.06701 - validation accuracy: 65.71 \n",
      "[79,     1] loss: 1.73028, adv_train_accuracy: 33.20, clean_train_accuracy : 57.03\n",
      "[79,    11] loss: 1.77209, adv_train_accuracy: 33.98, clean_train_accuracy : 54.30\n",
      "[79,    21] loss: 1.80228, adv_train_accuracy: 30.66, clean_train_accuracy : 55.08\n",
      "[79,    31] loss: 1.74307, adv_train_accuracy: 35.35, clean_train_accuracy : 60.74\n",
      "[79,    41] loss: 1.78173, adv_train_accuracy: 32.23, clean_train_accuracy : 53.71\n",
      "[79,    51] loss: 1.79881, adv_train_accuracy: 34.18, clean_train_accuracy : 57.62\n",
      "[79,    61] loss: 1.80148, adv_train_accuracy: 31.64, clean_train_accuracy : 54.69\n",
      "[79,    71] loss: 1.75622, adv_train_accuracy: 34.38, clean_train_accuracy : 55.27\n",
      "[79,    81] loss: 1.79384, adv_train_accuracy: 32.81, clean_train_accuracy : 55.66\n",
      "[79,    91] loss: 1.73560, adv_train_accuracy: 36.13, clean_train_accuracy : 57.23\n",
      "0.6736328125\n",
      "0.603125\n",
      "duration: 193 s - train loss: 1.75636 - train accuracy: 33.95 - validation loss: 1.06017 - validation accuracy: 64.37 \n",
      "[80,     1] loss: 1.72906, adv_train_accuracy: 34.96, clean_train_accuracy : 56.45\n",
      "[80,    11] loss: 1.73525, adv_train_accuracy: 31.84, clean_train_accuracy : 58.01\n",
      "[80,    21] loss: 1.68842, adv_train_accuracy: 39.65, clean_train_accuracy : 59.57\n",
      "[80,    31] loss: 1.69527, adv_train_accuracy: 36.91, clean_train_accuracy : 59.38\n",
      "[80,    41] loss: 1.73296, adv_train_accuracy: 33.79, clean_train_accuracy : 59.38\n",
      "[80,    51] loss: 1.70085, adv_train_accuracy: 36.72, clean_train_accuracy : 61.72\n",
      "[80,    61] loss: 1.71358, adv_train_accuracy: 33.01, clean_train_accuracy : 50.98\n",
      "[80,    71] loss: 1.74118, adv_train_accuracy: 34.57, clean_train_accuracy : 56.64\n",
      "[80,    81] loss: 1.69671, adv_train_accuracy: 33.98, clean_train_accuracy : 56.25\n",
      "[80,    91] loss: 1.77339, adv_train_accuracy: 31.45, clean_train_accuracy : 56.25\n",
      "0.6798828125\n",
      "0.5951171875\n",
      "duration: 188 s - train loss: 1.74998 - train accuracy: 34.07 - validation loss: 1.03209 - validation accuracy: 65.66 \n",
      "[81,     1] loss: 1.76010, adv_train_accuracy: 35.55, clean_train_accuracy : 57.81\n",
      "[81,    11] loss: 1.78039, adv_train_accuracy: 33.01, clean_train_accuracy : 56.25\n",
      "[81,    21] loss: 1.75259, adv_train_accuracy: 34.57, clean_train_accuracy : 55.86\n",
      "[81,    31] loss: 1.75674, adv_train_accuracy: 34.57, clean_train_accuracy : 58.79\n",
      "[81,    41] loss: 1.75928, adv_train_accuracy: 34.96, clean_train_accuracy : 52.73\n",
      "[81,    51] loss: 1.79210, adv_train_accuracy: 31.45, clean_train_accuracy : 54.10\n",
      "[81,    61] loss: 1.75900, adv_train_accuracy: 33.40, clean_train_accuracy : 54.88\n",
      "[81,    71] loss: 1.86792, adv_train_accuracy: 26.95, clean_train_accuracy : 52.34\n",
      "[81,    81] loss: 1.69065, adv_train_accuracy: 39.06, clean_train_accuracy : 61.91\n",
      "[81,    91] loss: 1.69951, adv_train_accuracy: 37.30, clean_train_accuracy : 59.77\n",
      "0.68359375\n",
      "0.6177734375\n",
      "duration: 185 s - train loss: 1.75416 - train accuracy: 33.98 - validation loss: 1.03797 - validation accuracy: 65.22 \n",
      "[82,     1] loss: 1.73257, adv_train_accuracy: 35.94, clean_train_accuracy : 59.57\n",
      "[82,    11] loss: 1.76359, adv_train_accuracy: 31.84, clean_train_accuracy : 56.05\n",
      "[82,    21] loss: 1.73558, adv_train_accuracy: 32.62, clean_train_accuracy : 59.57\n",
      "[82,    31] loss: 1.74657, adv_train_accuracy: 33.59, clean_train_accuracy : 53.91\n",
      "[82,    41] loss: 1.76060, adv_train_accuracy: 34.77, clean_train_accuracy : 51.17\n",
      "[82,    51] loss: 1.76911, adv_train_accuracy: 32.62, clean_train_accuracy : 53.91\n",
      "[82,    61] loss: 1.81414, adv_train_accuracy: 30.66, clean_train_accuracy : 55.47\n",
      "[82,    71] loss: 1.71321, adv_train_accuracy: 33.20, clean_train_accuracy : 56.84\n",
      "[82,    81] loss: 1.77318, adv_train_accuracy: 34.38, clean_train_accuracy : 55.86\n",
      "[82,    91] loss: 1.76591, adv_train_accuracy: 31.05, clean_train_accuracy : 54.30\n",
      "0.68203125\n",
      "[83,    81] loss: 1.85225, adv_train_accuracy: 31.64, clean_train_accuracy : 52.93\n",
      "[83,    91] loss: 1.65222, adv_train_accuracy: 37.11, clean_train_accuracy : 61.13\n",
      "0.6791015625\n",
      "0.5958984375\n",
      "duration: 184 s - train loss: 1.74270 - train accuracy: 34.15 - validation loss: 1.02885 - validation accuracy: 65.80 \n",
      "[84,     1] loss: 1.71443, adv_train_accuracy: 35.55, clean_train_accuracy : 58.01\n",
      "[84,    11] loss: 1.76215, adv_train_accuracy: 32.23, clean_train_accuracy : 55.66\n",
      "[84,    21] loss: 1.74033, adv_train_accuracy: 33.59, clean_train_accuracy : 55.47\n",
      "[84,    31] loss: 1.69401, adv_train_accuracy: 33.01, clean_train_accuracy : 54.49\n",
      "[84,    41] loss: 1.71606, adv_train_accuracy: 37.30, clean_train_accuracy : 60.35\n",
      "[84,    51] loss: 1.77206, adv_train_accuracy: 35.35, clean_train_accuracy : 53.52\n",
      "[84,    61] loss: 1.75761, adv_train_accuracy: 34.18, clean_train_accuracy : 59.18\n",
      "[84,    71] loss: 1.76122, adv_train_accuracy: 35.16, clean_train_accuracy : 56.64\n",
      "[84,    81] loss: 1.74881, adv_train_accuracy: 36.91, clean_train_accuracy : 59.38\n",
      "[84,    91] loss: 1.82513, adv_train_accuracy: 30.47, clean_train_accuracy : 51.95\n",
      "0.6845703125\n",
      "0.6201171875\n",
      "duration: 184 s - train loss: 1.73843 - train accuracy: 34.45 - validation loss: 1.07972 - validation accuracy: 64.00 \n",
      "[85,     1] loss: 1.74442, adv_train_accuracy: 31.84, clean_train_accuracy : 56.64\n",
      "[85,    11] loss: 1.71986, adv_train_accuracy: 37.50, clean_train_accuracy : 61.91\n",
      "[85,    21] loss: 1.67605, adv_train_accuracy: 36.91, clean_train_accuracy : 57.81\n",
      "[85,    31] loss: 1.65555, adv_train_accuracy: 35.74, clean_train_accuracy : 58.40\n",
      "[85,    41] loss: 1.73732, adv_train_accuracy: 34.18, clean_train_accuracy : 55.86\n",
      "[85,    51] loss: 1.83099, adv_train_accuracy: 34.96, clean_train_accuracy : 56.25\n",
      "[85,    61] loss: 1.73063, adv_train_accuracy: 32.81, clean_train_accuracy : 58.98\n",
      "[85,    71] loss: 1.86634, adv_train_accuracy: 29.49, clean_train_accuracy : 48.05\n",
      "[85,    81] loss: 1.73869, adv_train_accuracy: 32.62, clean_train_accuracy : 57.62\n",
      "[85,    91] loss: 1.79826, adv_train_accuracy: 28.91, clean_train_accuracy : 55.27\n",
      "0.6837890625\n",
      "0.6083984375\n",
      "duration: 184 s - train loss: 1.74012 - train accuracy: 34.34 - validation loss: 1.05554 - validation accuracy: 64.20 \n",
      "[86,     1] loss: 1.67055, adv_train_accuracy: 35.74, clean_train_accuracy : 60.16\n",
      "[86,    11] loss: 1.78071, adv_train_accuracy: 32.23, clean_train_accuracy : 58.79\n",
      "[86,    21] loss: 1.73669, adv_train_accuracy: 33.40, clean_train_accuracy : 57.23\n",
      "[86,    31] loss: 1.71194, adv_train_accuracy: 34.38, clean_train_accuracy : 57.81\n",
      "[86,    41] loss: 1.69375, adv_train_accuracy: 38.48, clean_train_accuracy : 60.35\n",
      "[86,    51] loss: 1.70736, adv_train_accuracy: 34.38, clean_train_accuracy : 61.13\n",
      "[86,    61] loss: 1.71492, adv_train_accuracy: 34.18, clean_train_accuracy : 58.98\n",
      "[86,    71] loss: 1.76797, adv_train_accuracy: 34.96, clean_train_accuracy : 58.01\n",
      "[86,    81] loss: 1.80684, adv_train_accuracy: 31.45, clean_train_accuracy : 54.10\n",
      "[86,    91] loss: 1.76739, adv_train_accuracy: 34.77, clean_train_accuracy : 58.98\n",
      "0.686328125\n",
      "0.5982421875\n",
      "duration: 184 s - train loss: 1.73428 - train accuracy: 34.66 - validation loss: 1.01546 - validation accuracy: 65.03 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87,     1] loss: 1.74436, adv_train_accuracy: 33.40, clean_train_accuracy : 56.45\n",
      "[87,    11] loss: 1.67182, adv_train_accuracy: 34.57, clean_train_accuracy : 57.42\n",
      "[87,    21] loss: 1.75348, adv_train_accuracy: 33.59, clean_train_accuracy : 53.12\n",
      "[87,    31] loss: 1.71774, adv_train_accuracy: 33.98, clean_train_accuracy : 56.84\n",
      "[87,    41] loss: 1.76977, adv_train_accuracy: 33.20, clean_train_accuracy : 53.71\n",
      "[87,    51] loss: 1.74625, adv_train_accuracy: 33.59, clean_train_accuracy : 57.81\n",
      "[87,    61] loss: 1.75290, adv_train_accuracy: 33.20, clean_train_accuracy : 58.01\n",
      "[87,    71] loss: 1.69286, adv_train_accuracy: 33.59, clean_train_accuracy : 59.38\n",
      "[87,    81] loss: 1.72620, adv_train_accuracy: 36.52, clean_train_accuracy : 55.86\n",
      "[87,    91] loss: 1.76641, adv_train_accuracy: 35.55, clean_train_accuracy : 60.74\n",
      "0.6794921875\n",
      "0.5998046875\n",
      "duration: 184 s - train loss: 1.73128 - train accuracy: 34.65 - validation loss: 1.03395 - validation accuracy: 64.66 \n",
      "[88,     1] loss: 1.78416, adv_train_accuracy: 35.74, clean_train_accuracy : 57.81\n",
      "[88,    11] loss: 1.73598, adv_train_accuracy: 35.74, clean_train_accuracy : 60.16\n",
      "[88,    21] loss: 1.66247, adv_train_accuracy: 38.28, clean_train_accuracy : 58.59\n",
      "[88,    31] loss: 1.71749, adv_train_accuracy: 34.57, clean_train_accuracy : 60.16\n",
      "[88,    41] loss: 1.67401, adv_train_accuracy: 39.65, clean_train_accuracy : 58.20\n",
      "[88,    51] loss: 1.79540, adv_train_accuracy: 33.01, clean_train_accuracy : 58.01\n",
      "[88,    61] loss: 1.74440, adv_train_accuracy: 31.25, clean_train_accuracy : 61.13\n",
      "[88,    71] loss: 1.75529, adv_train_accuracy: 35.55, clean_train_accuracy : 57.23\n",
      "[88,    81] loss: 1.72651, adv_train_accuracy: 34.38, clean_train_accuracy : 60.74\n",
      "[88,    91] loss: 1.72425, adv_train_accuracy: 35.74, clean_train_accuracy : 60.16\n",
      "0.6845703125\n",
      "0.602734375\n",
      "duration: 185 s - train loss: 1.73304 - train accuracy: 34.58 - validation loss: 1.03530 - validation accuracy: 66.06 \n",
      "[89,     1] loss: 1.72479, adv_train_accuracy: 33.98, clean_train_accuracy : 54.49\n",
      "[89,    11] loss: 1.73820, adv_train_accuracy: 32.62, clean_train_accuracy : 58.20\n",
      "[89,    21] loss: 1.71527, adv_train_accuracy: 34.18, clean_train_accuracy : 54.88\n",
      "[89,    31] loss: 1.80477, adv_train_accuracy: 30.08, clean_train_accuracy : 54.49\n",
      "[89,    41] loss: 1.75056, adv_train_accuracy: 36.33, clean_train_accuracy : 57.23\n",
      "[89,    51] loss: 1.81844, adv_train_accuracy: 31.45, clean_train_accuracy : 53.71\n",
      "[89,    61] loss: 1.68563, adv_train_accuracy: 35.35, clean_train_accuracy : 60.74\n",
      "[89,    71] loss: 1.80168, adv_train_accuracy: 32.62, clean_train_accuracy : 53.71\n",
      "[89,    81] loss: 1.69391, adv_train_accuracy: 34.38, clean_train_accuracy : 56.64\n",
      "[89,    91] loss: 1.69674, adv_train_accuracy: 36.13, clean_train_accuracy : 58.59\n",
      "0.6810546875\n",
      "0.603515625\n",
      "duration: 185 s - train loss: 1.73120 - train accuracy: 34.74 - validation loss: 1.01523 - validation accuracy: 66.35 \n",
      "[90,     1] loss: 1.70163, adv_train_accuracy: 36.72, clean_train_accuracy : 61.13\n",
      "[90,    11] loss: 1.75490, adv_train_accuracy: 32.03, clean_train_accuracy : 57.03\n",
      "[90,    21] loss: 1.75142, adv_train_accuracy: 32.62, clean_train_accuracy : 54.88\n",
      "[90,    31] loss: 1.67410, adv_train_accuracy: 37.30, clean_train_accuracy : 59.57\n",
      "[90,    41] loss: 1.64743, adv_train_accuracy: 36.33, clean_train_accuracy : 59.96\n",
      "[90,    51] loss: 1.69025, adv_train_accuracy: 35.74, clean_train_accuracy : 57.81\n",
      "[90,    61] loss: 1.73994, adv_train_accuracy: 33.98, clean_train_accuracy : 57.23\n",
      "[90,    71] loss: 1.70333, adv_train_accuracy: 36.91, clean_train_accuracy : 57.81\n",
      "[90,    81] loss: 1.68580, adv_train_accuracy: 37.11, clean_train_accuracy : 61.72\n",
      "[90,    91] loss: 1.69097, adv_train_accuracy: 38.09, clean_train_accuracy : 57.03\n",
      "0.6751953125\n",
      "0.59609375\n",
      "duration: 185 s - train loss: 1.72151 - train accuracy: 35.19 - validation loss: 1.04366 - validation accuracy: 65.58 \n",
      "[91,     1] loss: 1.72235, adv_train_accuracy: 36.13, clean_train_accuracy : 58.40\n",
      "[91,    11] loss: 1.72827, adv_train_accuracy: 36.52, clean_train_accuracy : 58.98\n",
      "[91,    21] loss: 1.76091, adv_train_accuracy: 33.79, clean_train_accuracy : 55.86\n",
      "[91,    31] loss: 1.69944, adv_train_accuracy: 33.59, clean_train_accuracy : 57.62\n",
      "[91,    41] loss: 1.68711, adv_train_accuracy: 33.98, clean_train_accuracy : 58.40\n",
      "[91,    51] loss: 1.74876, adv_train_accuracy: 38.09, clean_train_accuracy : 57.62\n",
      "[91,    61] loss: 1.70511, adv_train_accuracy: 36.52, clean_train_accuracy : 61.72\n",
      "[91,    71] loss: 1.70400, adv_train_accuracy: 35.35, clean_train_accuracy : 58.79\n",
      "[91,    81] loss: 1.67932, adv_train_accuracy: 36.72, clean_train_accuracy : 60.35\n",
      "[91,    91] loss: 1.79329, adv_train_accuracy: 34.18, clean_train_accuracy : 56.05\n",
      "0.67421875\n",
      "0.6072265625\n",
      "duration: 192 s - train loss: 1.71877 - train accuracy: 35.13 - validation loss: 1.05345 - validation accuracy: 65.42 \n",
      "[92,     1] loss: 1.67525, adv_train_accuracy: 38.67, clean_train_accuracy : 57.81\n",
      "[92,    11] loss: 1.72505, adv_train_accuracy: 34.96, clean_train_accuracy : 60.74\n",
      "[92,    21] loss: 1.67729, adv_train_accuracy: 37.50, clean_train_accuracy : 56.45\n",
      "[92,    31] loss: 1.76027, adv_train_accuracy: 32.03, clean_train_accuracy : 57.81\n",
      "[92,    41] loss: 1.73507, adv_train_accuracy: 34.77, clean_train_accuracy : 58.40\n",
      "[92,    51] loss: 1.64411, adv_train_accuracy: 38.28, clean_train_accuracy : 61.52\n",
      "[92,    61] loss: 1.69945, adv_train_accuracy: 34.38, clean_train_accuracy : 59.57\n",
      "[92,    71] loss: 1.73118, adv_train_accuracy: 33.79, clean_train_accuracy : 60.55\n",
      "[92,    81] loss: 1.72620, adv_train_accuracy: 32.42, clean_train_accuracy : 54.30\n",
      "[92,    91] loss: 1.72860, adv_train_accuracy: 36.13, clean_train_accuracy : 58.79\n",
      "0.6720703125\n",
      "0.6068359375\n",
      "duration: 186 s - train loss: 1.71481 - train accuracy: 35.43 - validation loss: 1.01039 - validation accuracy: 66.16 \n",
      "[93,     1] loss: 1.72550, adv_train_accuracy: 35.74, clean_train_accuracy : 60.35\n",
      "[93,    11] loss: 1.70180, adv_train_accuracy: 35.74, clean_train_accuracy : 59.57\n",
      "[93,    21] loss: 1.67233, adv_train_accuracy: 35.55, clean_train_accuracy : 62.70\n",
      "[93,    31] loss: 1.63312, adv_train_accuracy: 39.45, clean_train_accuracy : 63.48\n",
      "[93,    41] loss: 1.70054, adv_train_accuracy: 35.35, clean_train_accuracy : 58.40\n",
      "[93,    51] loss: 1.77634, adv_train_accuracy: 32.42, clean_train_accuracy : 58.98\n",
      "[93,    61] loss: 1.74236, adv_train_accuracy: 36.13, clean_train_accuracy : 59.57\n",
      "[93,    71] loss: 1.64991, adv_train_accuracy: 38.87, clean_train_accuracy : 65.04\n",
      "[93,    81] loss: 1.75752, adv_train_accuracy: 33.01, clean_train_accuracy : 55.66\n",
      "[93,    91] loss: 1.70576, adv_train_accuracy: 34.57, clean_train_accuracy : 57.42\n",
      "0.6623046875\n",
      "0.6033203125\n",
      "duration: 191 s - train loss: 1.71495 - train accuracy: 35.19 - validation loss: 1.04504 - validation accuracy: 64.63 \n",
      "[94,     1] loss: 1.66933, adv_train_accuracy: 36.33, clean_train_accuracy : 59.57\n",
      "[94,    11] loss: 1.70576, adv_train_accuracy: 35.94, clean_train_accuracy : 57.03\n",
      "[94,    21] loss: 1.62774, adv_train_accuracy: 37.89, clean_train_accuracy : 62.11\n",
      "[94,    31] loss: 1.74213, adv_train_accuracy: 32.81, clean_train_accuracy : 56.05\n",
      "[94,    41] loss: 1.74229, adv_train_accuracy: 34.57, clean_train_accuracy : 60.94\n",
      "[94,    51] loss: 1.74972, adv_train_accuracy: 34.18, clean_train_accuracy : 56.05\n",
      "[94,    61] loss: 1.71702, adv_train_accuracy: 36.91, clean_train_accuracy : 58.01\n",
      "[94,    71] loss: 1.70559, adv_train_accuracy: 35.55, clean_train_accuracy : 61.13\n",
      "[94,    81] loss: 1.68557, adv_train_accuracy: 37.70, clean_train_accuracy : 61.13\n",
      "[94,    91] loss: 1.68735, adv_train_accuracy: 34.57, clean_train_accuracy : 57.42\n",
      "0.6765625\n",
      "0.609375\n",
      "duration: 188 s - train loss: 1.71506 - train accuracy: 35.20 - validation loss: 1.02617 - validation accuracy: 65.04 \n",
      "[95,     1] loss: 1.79029, adv_train_accuracy: 33.59, clean_train_accuracy : 56.64\n",
      "[95,    11] loss: 1.71897, adv_train_accuracy: 37.89, clean_train_accuracy : 61.13\n",
      "[95,    21] loss: 1.63837, adv_train_accuracy: 39.26, clean_train_accuracy : 60.94\n",
      "[95,    31] loss: 1.68495, adv_train_accuracy: 37.11, clean_train_accuracy : 59.18\n",
      "[95,    41] loss: 1.71973, adv_train_accuracy: 35.55, clean_train_accuracy : 59.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95,    51] loss: 1.81883, adv_train_accuracy: 31.05, clean_train_accuracy : 56.05\n",
      "[95,    61] loss: 1.71291, adv_train_accuracy: 33.79, clean_train_accuracy : 59.96\n",
      "[95,    71] loss: 1.70654, adv_train_accuracy: 36.33, clean_train_accuracy : 61.13\n",
      "[95,    81] loss: 1.70474, adv_train_accuracy: 36.72, clean_train_accuracy : 62.30\n",
      "[95,    91] loss: 1.75650, adv_train_accuracy: 33.01, clean_train_accuracy : 56.45\n",
      "0.669140625\n",
      "0.5927734375\n",
      "duration: 192 s - train loss: 1.71313 - train accuracy: 35.50 - validation loss: 0.99052 - validation accuracy: 66.91 \n",
      "[96,     1] loss: 1.66758, adv_train_accuracy: 36.33, clean_train_accuracy : 60.55\n",
      "[96,    11] loss: 1.71879, adv_train_accuracy: 39.26, clean_train_accuracy : 59.96\n",
      "[96,    21] loss: 1.68289, adv_train_accuracy: 37.89, clean_train_accuracy : 61.33\n",
      "[96,    31] loss: 1.71984, adv_train_accuracy: 36.13, clean_train_accuracy : 58.59\n",
      "[96,    41] loss: 1.67395, adv_train_accuracy: 38.09, clean_train_accuracy : 59.77\n",
      "[96,    51] loss: 1.72533, adv_train_accuracy: 36.13, clean_train_accuracy : 59.57\n",
      "[96,    61] loss: 1.70684, adv_train_accuracy: 36.13, clean_train_accuracy : 58.40\n",
      "[96,    71] loss: 1.67361, adv_train_accuracy: 34.96, clean_train_accuracy : 59.18\n",
      "[96,    81] loss: 1.75209, adv_train_accuracy: 34.18, clean_train_accuracy : 59.38\n",
      "[96,    91] loss: 1.65783, adv_train_accuracy: 35.94, clean_train_accuracy : 58.40\n",
      "0.6689453125\n",
      "0.592578125\n",
      "duration: 188 s - train loss: 1.70282 - train accuracy: 35.99 - validation loss: 0.97978 - validation accuracy: 68.08 \n",
      "[97,     1] loss: 1.69664, adv_train_accuracy: 37.70, clean_train_accuracy : 62.50\n",
      "[97,    11] loss: 1.72224, adv_train_accuracy: 32.62, clean_train_accuracy : 58.98\n",
      "[97,    21] loss: 1.70687, adv_train_accuracy: 37.11, clean_train_accuracy : 61.91\n",
      "[97,    31] loss: 1.64830, adv_train_accuracy: 37.89, clean_train_accuracy : 59.77\n",
      "[97,    41] loss: 1.66056, adv_train_accuracy: 37.89, clean_train_accuracy : 60.16\n",
      "[97,    51] loss: 1.72004, adv_train_accuracy: 32.81, clean_train_accuracy : 55.66\n",
      "[97,    61] loss: 1.68943, adv_train_accuracy: 36.13, clean_train_accuracy : 60.16\n",
      "[97,    71] loss: 1.71496, adv_train_accuracy: 33.59, clean_train_accuracy : 59.38\n",
      "[97,    81] loss: 1.71186, adv_train_accuracy: 33.59, clean_train_accuracy : 58.40\n",
      "[97,    91] loss: 1.69100, adv_train_accuracy: 35.94, clean_train_accuracy : 61.13\n",
      "0.652734375\n",
      "0.59140625\n",
      "duration: 184 s - train loss: 1.70263 - train accuracy: 35.84 - validation loss: 1.00171 - validation accuracy: 66.66 \n",
      "[98,     1] loss: 1.70736, adv_train_accuracy: 36.91, clean_train_accuracy : 58.59\n",
      "[98,    11] loss: 1.74837, adv_train_accuracy: 33.59, clean_train_accuracy : 58.98\n",
      "[98,    21] loss: 1.67002, adv_train_accuracy: 36.33, clean_train_accuracy : 63.09\n",
      "[98,    31] loss: 1.64995, adv_train_accuracy: 37.30, clean_train_accuracy : 57.81\n",
      "[98,    41] loss: 1.72279, adv_train_accuracy: 35.55, clean_train_accuracy : 61.72\n",
      "[98,    51] loss: 1.67494, adv_train_accuracy: 37.70, clean_train_accuracy : 62.11\n",
      "[98,    61] loss: 1.68564, adv_train_accuracy: 36.33, clean_train_accuracy : 59.77\n",
      "[98,    71] loss: 1.77435, adv_train_accuracy: 31.05, clean_train_accuracy : 55.27\n",
      "[98,    81] loss: 1.72206, adv_train_accuracy: 34.38, clean_train_accuracy : 56.84\n",
      "[98,    91] loss: 1.73007, adv_train_accuracy: 33.98, clean_train_accuracy : 59.77\n",
      "0.6744140625\n",
      "0.591015625\n",
      "duration: 184 s - train loss: 1.69744 - train accuracy: 36.04 - validation loss: 0.96629 - validation accuracy: 67.91 \n",
      "[99,     1] loss: 1.76119, adv_train_accuracy: 33.20, clean_train_accuracy : 52.93\n",
      "[99,    11] loss: 1.73114, adv_train_accuracy: 32.62, clean_train_accuracy : 61.52\n",
      "[99,    21] loss: 1.67750, adv_train_accuracy: 36.33, clean_train_accuracy : 58.59\n",
      "[99,    31] loss: 1.67002, adv_train_accuracy: 37.30, clean_train_accuracy : 59.77\n",
      "[99,    41] loss: 1.72896, adv_train_accuracy: 35.16, clean_train_accuracy : 58.59\n",
      "[99,    51] loss: 1.69451, adv_train_accuracy: 38.09, clean_train_accuracy : 60.74\n",
      "[99,    61] loss: 1.73060, adv_train_accuracy: 34.38, clean_train_accuracy : 55.47\n",
      "[99,    71] loss: 1.69775, adv_train_accuracy: 38.09, clean_train_accuracy : 59.77\n",
      "[99,    81] loss: 1.65157, adv_train_accuracy: 38.09, clean_train_accuracy : 61.13\n",
      "[99,    91] loss: 1.73969, adv_train_accuracy: 35.35, clean_train_accuracy : 55.86\n",
      "0.6625\n",
      "0.583203125\n",
      "duration: 184 s - train loss: 1.69778 - train accuracy: 35.89 - validation loss: 0.99501 - validation accuracy: 66.10 \n",
      "[100,     1] loss: 1.66730, adv_train_accuracy: 36.91, clean_train_accuracy : 60.74\n",
      "[100,    11] loss: 1.72872, adv_train_accuracy: 36.33, clean_train_accuracy : 57.81\n",
      "[100,    21] loss: 1.70820, adv_train_accuracy: 37.70, clean_train_accuracy : 59.38\n",
      "[100,    31] loss: 1.70780, adv_train_accuracy: 36.33, clean_train_accuracy : 62.30\n",
      "[100,    41] loss: 1.70671, adv_train_accuracy: 38.28, clean_train_accuracy : 59.57\n",
      "[100,    51] loss: 1.65930, adv_train_accuracy: 36.72, clean_train_accuracy : 60.74\n",
      "[100,    61] loss: 1.64902, adv_train_accuracy: 38.87, clean_train_accuracy : 63.09\n",
      "[100,    71] loss: 1.73192, adv_train_accuracy: 35.16, clean_train_accuracy : 57.03\n",
      "[100,    81] loss: 1.65825, adv_train_accuracy: 35.55, clean_train_accuracy : 59.18\n",
      "[100,    91] loss: 1.74200, adv_train_accuracy: 33.79, clean_train_accuracy : 56.84\n",
      "0.64453125\n",
      "0.5716796875\n",
      "duration: 192 s - train loss: 1.69213 - train accuracy: 36.16 - validation loss: 0.95824 - validation accuracy: 67.32 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_stats = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-100-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 100,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast adversarial training\n",
      "fast adv. train.\n",
      "[1,     1] loss: 1.66856, adv_train_accuracy: 37.70, clean_train_accuracy : 64.26\n",
      "[1,    11] loss: 1.75248, adv_train_accuracy: 33.01, clean_train_accuracy : 56.64\n",
      "[1,    21] loss: 1.73944, adv_train_accuracy: 33.59, clean_train_accuracy : 53.52\n",
      "[1,    31] loss: 1.72856, adv_train_accuracy: 35.35, clean_train_accuracy : 60.16\n",
      "[1,    41] loss: 1.64640, adv_train_accuracy: 36.91, clean_train_accuracy : 59.57\n",
      "[1,    51] loss: 1.70116, adv_train_accuracy: 36.72, clean_train_accuracy : 58.20\n",
      "[1,    61] loss: 1.73351, adv_train_accuracy: 32.42, clean_train_accuracy : 58.59\n",
      "[1,    71] loss: 1.76894, adv_train_accuracy: 33.59, clean_train_accuracy : 55.27\n",
      "[1,    81] loss: 1.74671, adv_train_accuracy: 32.42, clean_train_accuracy : 59.38\n",
      "[1,    91] loss: 1.74011, adv_train_accuracy: 33.01, clean_train_accuracy : 54.88\n",
      "0.64765625\n",
      "0.583984375\n",
      "duration: 188 s - train loss: 1.70493 - train accuracy: 35.64 - validation loss: 0.99675 - validation accuracy: 65.38 \n",
      "[2,     1] loss: 1.71960, adv_train_accuracy: 34.96, clean_train_accuracy : 57.03\n",
      "[2,    11] loss: 1.69912, adv_train_accuracy: 33.59, clean_train_accuracy : 60.16\n",
      "[2,    21] loss: 1.68470, adv_train_accuracy: 36.91, clean_train_accuracy : 57.81\n",
      "[2,    31] loss: 1.71854, adv_train_accuracy: 37.11, clean_train_accuracy : 59.18\n",
      "[2,    41] loss: 1.67008, adv_train_accuracy: 36.13, clean_train_accuracy : 60.16\n",
      "[2,    51] loss: 1.64604, adv_train_accuracy: 38.28, clean_train_accuracy : 59.57\n",
      "[2,    61] loss: 1.67981, adv_train_accuracy: 34.38, clean_train_accuracy : 59.18\n",
      "[2,    71] loss: 1.71731, adv_train_accuracy: 35.74, clean_train_accuracy : 58.40\n",
      "[2,    81] loss: 1.66216, adv_train_accuracy: 37.30, clean_train_accuracy : 61.72\n",
      "[2,    91] loss: 1.70512, adv_train_accuracy: 36.33, clean_train_accuracy : 56.25\n",
      "0.6587890625\n",
      "0.584765625\n",
      "duration: 190 s - train loss: 1.68930 - train accuracy: 36.13 - validation loss: 0.97632 - validation accuracy: 67.19 \n",
      "[3,     1] loss: 1.72181, adv_train_accuracy: 34.57, clean_train_accuracy : 58.98\n",
      "[3,    11] loss: 1.64588, adv_train_accuracy: 36.72, clean_train_accuracy : 61.91\n",
      "[3,    21] loss: 1.68306, adv_train_accuracy: 33.20, clean_train_accuracy : 55.86\n",
      "[3,    31] loss: 1.72330, adv_train_accuracy: 35.16, clean_train_accuracy : 57.81\n",
      "[3,    41] loss: 1.71340, adv_train_accuracy: 37.50, clean_train_accuracy : 56.45\n",
      "[3,    51] loss: 1.70754, adv_train_accuracy: 33.01, clean_train_accuracy : 59.18\n",
      "[3,    61] loss: 1.75054, adv_train_accuracy: 34.77, clean_train_accuracy : 56.25\n",
      "[3,    71] loss: 1.71652, adv_train_accuracy: 34.96, clean_train_accuracy : 58.20\n",
      "[3,    81] loss: 1.65154, adv_train_accuracy: 37.30, clean_train_accuracy : 61.33\n",
      "[3,    91] loss: 1.67282, adv_train_accuracy: 36.33, clean_train_accuracy : 57.81\n",
      "0.66171875\n",
      "0.5822265625\n",
      "duration: 192 s - train loss: 1.69516 - train accuracy: 35.97 - validation loss: 0.95182 - validation accuracy: 68.09 \n",
      "[4,     1] loss: 1.59866, adv_train_accuracy: 41.99, clean_train_accuracy : 62.50\n",
      "[4,    11] loss: 1.71295, adv_train_accuracy: 35.16, clean_train_accuracy : 58.01\n",
      "[4,    21] loss: 1.65519, adv_train_accuracy: 36.13, clean_train_accuracy : 62.30\n",
      "[4,    31] loss: 1.70381, adv_train_accuracy: 37.70, clean_train_accuracy : 60.55\n",
      "[4,    41] loss: 1.74310, adv_train_accuracy: 35.16, clean_train_accuracy : 58.20\n",
      "[4,    51] loss: 1.79136, adv_train_accuracy: 33.20, clean_train_accuracy : 58.40\n",
      "[4,    61] loss: 1.65617, adv_train_accuracy: 38.87, clean_train_accuracy : 61.33\n",
      "[4,    71] loss: 1.63217, adv_train_accuracy: 40.04, clean_train_accuracy : 58.98\n",
      "[4,    81] loss: 1.61591, adv_train_accuracy: 38.09, clean_train_accuracy : 60.74\n",
      "[4,    91] loss: 1.69285, adv_train_accuracy: 35.74, clean_train_accuracy : 58.01\n",
      "0.661328125\n",
      "0.57109375\n",
      "duration: 192 s - train loss: 1.69071 - train accuracy: 36.29 - validation loss: 0.94108 - validation accuracy: 68.07 \n",
      "[5,     1] loss: 1.68879, adv_train_accuracy: 36.72, clean_train_accuracy : 58.40\n",
      "[5,    11] loss: 1.64077, adv_train_accuracy: 39.06, clean_train_accuracy : 63.48\n",
      "[5,    21] loss: 1.72392, adv_train_accuracy: 37.70, clean_train_accuracy : 55.66\n",
      "[5,    31] loss: 1.71758, adv_train_accuracy: 34.96, clean_train_accuracy : 60.55\n",
      "[5,    41] loss: 1.67841, adv_train_accuracy: 35.16, clean_train_accuracy : 62.50\n",
      "[5,    51] loss: 1.65198, adv_train_accuracy: 35.55, clean_train_accuracy : 60.55\n",
      "[5,    61] loss: 1.64438, adv_train_accuracy: 38.87, clean_train_accuracy : 62.11\n",
      "[5,    71] loss: 1.65619, adv_train_accuracy: 39.65, clean_train_accuracy : 63.48\n",
      "[5,    81] loss: 1.76130, adv_train_accuracy: 37.11, clean_train_accuracy : 55.66\n",
      "[5,    91] loss: 1.65397, adv_train_accuracy: 39.26, clean_train_accuracy : 63.48\n",
      "0.656640625\n",
      "0.59140625\n",
      "duration: 192 s - train loss: 1.68018 - train accuracy: 36.57 - validation loss: 0.99174 - validation accuracy: 66.86 \n",
      "[6,     1] loss: 1.71513, adv_train_accuracy: 36.13, clean_train_accuracy : 57.62\n",
      "[6,    11] loss: 1.68211, adv_train_accuracy: 33.98, clean_train_accuracy : 58.20\n",
      "[6,    21] loss: 1.70394, adv_train_accuracy: 35.35, clean_train_accuracy : 58.79\n",
      "[6,    31] loss: 1.73960, adv_train_accuracy: 35.16, clean_train_accuracy : 56.25\n",
      "[6,    41] loss: 1.63870, adv_train_accuracy: 37.70, clean_train_accuracy : 60.16\n",
      "[6,    51] loss: 1.68508, adv_train_accuracy: 36.91, clean_train_accuracy : 59.18\n",
      "[6,    61] loss: 1.65072, adv_train_accuracy: 37.11, clean_train_accuracy : 62.11\n",
      "[6,    71] loss: 1.67359, adv_train_accuracy: 37.30, clean_train_accuracy : 58.01\n",
      "[6,    81] loss: 1.70361, adv_train_accuracy: 35.16, clean_train_accuracy : 58.59\n",
      "[6,    91] loss: 1.69831, adv_train_accuracy: 34.57, clean_train_accuracy : 63.28\n",
      "0.65546875\n",
      "0.576171875\n",
      "duration: 192 s - train loss: 1.67772 - train accuracy: 36.72 - validation loss: 0.94242 - validation accuracy: 67.17 \n",
      "[7,     1] loss: 1.63965, adv_train_accuracy: 36.52, clean_train_accuracy : 61.13\n",
      "[7,    11] loss: 1.65713, adv_train_accuracy: 41.21, clean_train_accuracy : 62.50\n",
      "[7,    21] loss: 1.64802, adv_train_accuracy: 39.26, clean_train_accuracy : 59.18\n",
      "[7,    31] loss: 1.74000, adv_train_accuracy: 32.62, clean_train_accuracy : 57.81\n",
      "[7,    41] loss: 1.68290, adv_train_accuracy: 35.94, clean_train_accuracy : 58.01\n",
      "[7,    51] loss: 1.63993, adv_train_accuracy: 36.52, clean_train_accuracy : 58.01\n",
      "[7,    61] loss: 1.63224, adv_train_accuracy: 38.67, clean_train_accuracy : 62.89\n",
      "[7,    71] loss: 1.67962, adv_train_accuracy: 35.35, clean_train_accuracy : 58.59\n",
      "[7,    81] loss: 1.66976, adv_train_accuracy: 39.06, clean_train_accuracy : 61.13\n",
      "[7,    91] loss: 1.70655, adv_train_accuracy: 35.74, clean_train_accuracy : 60.35\n",
      "0.654296875\n",
      "0.5806640625\n",
      "duration: 188 s - train loss: 1.67620 - train accuracy: 36.89 - validation loss: 0.95899 - validation accuracy: 67.94 \n",
      "[8,     1] loss: 1.73592, adv_train_accuracy: 33.40, clean_train_accuracy : 59.38\n",
      "[8,    11] loss: 1.72677, adv_train_accuracy: 34.77, clean_train_accuracy : 59.57\n",
      "[8,    21] loss: 1.75560, adv_train_accuracy: 34.96, clean_train_accuracy : 58.98\n",
      "[8,    31] loss: 1.62848, adv_train_accuracy: 38.28, clean_train_accuracy : 62.50\n",
      "[8,    41] loss: 1.62744, adv_train_accuracy: 37.11, clean_train_accuracy : 64.45\n",
      "[8,    51] loss: 1.60323, adv_train_accuracy: 40.04, clean_train_accuracy : 67.38\n",
      "[8,    61] loss: 1.65610, adv_train_accuracy: 37.50, clean_train_accuracy : 63.67\n",
      "[8,    71] loss: 1.72830, adv_train_accuracy: 34.18, clean_train_accuracy : 60.35\n",
      "[8,    81] loss: 1.69443, adv_train_accuracy: 37.50, clean_train_accuracy : 61.52\n",
      "[8,    91] loss: 1.67014, adv_train_accuracy: 39.06, clean_train_accuracy : 61.91\n",
      "0.65703125\n",
      "0.577734375\n",
      "duration: 185 s - train loss: 1.67499 - train accuracy: 36.70 - validation loss: 0.96451 - validation accuracy: 67.10 \n",
      "[9,     1] loss: 1.69013, adv_train_accuracy: 33.98, clean_train_accuracy : 60.55\n",
      "[9,    11] loss: 1.70687, adv_train_accuracy: 36.13, clean_train_accuracy : 62.30\n",
      "[9,    21] loss: 1.73512, adv_train_accuracy: 34.57, clean_train_accuracy : 59.38\n",
      "[9,    31] loss: 1.62150, adv_train_accuracy: 38.28, clean_train_accuracy : 63.48\n",
      "[9,    41] loss: 1.69422, adv_train_accuracy: 35.74, clean_train_accuracy : 57.81\n",
      "[9,    51] loss: 1.73441, adv_train_accuracy: 36.33, clean_train_accuracy : 58.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,    61] loss: 1.63051, adv_train_accuracy: 40.23, clean_train_accuracy : 58.98\n",
      "[9,    71] loss: 1.70527, adv_train_accuracy: 37.11, clean_train_accuracy : 59.77\n",
      "[9,    81] loss: 1.69267, adv_train_accuracy: 37.70, clean_train_accuracy : 62.89\n",
      "[9,    91] loss: 1.69663, adv_train_accuracy: 36.72, clean_train_accuracy : 59.18\n",
      "0.658203125\n",
      "0.58046875\n",
      "duration: 184 s - train loss: 1.66953 - train accuracy: 37.26 - validation loss: 0.95854 - validation accuracy: 66.44 \n",
      "[10,     1] loss: 1.65323, adv_train_accuracy: 38.09, clean_train_accuracy : 55.27\n",
      "[10,    11] loss: 1.62083, adv_train_accuracy: 37.50, clean_train_accuracy : 59.57\n",
      "[10,    21] loss: 1.70966, adv_train_accuracy: 34.77, clean_train_accuracy : 57.42\n",
      "[10,    31] loss: 1.63102, adv_train_accuracy: 40.62, clean_train_accuracy : 64.26\n",
      "[10,    41] loss: 1.71669, adv_train_accuracy: 35.16, clean_train_accuracy : 57.03\n",
      "[10,    51] loss: 1.73153, adv_train_accuracy: 34.57, clean_train_accuracy : 60.55\n",
      "[10,    61] loss: 1.62225, adv_train_accuracy: 38.48, clean_train_accuracy : 63.67\n",
      "[10,    71] loss: 1.63745, adv_train_accuracy: 40.23, clean_train_accuracy : 62.11\n",
      "[10,    81] loss: 1.62615, adv_train_accuracy: 39.65, clean_train_accuracy : 61.72\n",
      "[10,    91] loss: 1.64262, adv_train_accuracy: 37.30, clean_train_accuracy : 58.59\n",
      "0.66015625\n",
      "0.581640625\n",
      "duration: 185 s - train loss: 1.65928 - train accuracy: 37.46 - validation loss: 0.91228 - validation accuracy: 69.32 \n",
      "[11,     1] loss: 1.60444, adv_train_accuracy: 42.19, clean_train_accuracy : 66.60\n",
      "[11,    11] loss: 1.72937, adv_train_accuracy: 35.74, clean_train_accuracy : 58.79\n",
      "[11,    21] loss: 1.64241, adv_train_accuracy: 37.50, clean_train_accuracy : 61.72\n",
      "[11,    31] loss: 1.73985, adv_train_accuracy: 34.57, clean_train_accuracy : 58.59\n",
      "[11,    41] loss: 1.63548, adv_train_accuracy: 36.91, clean_train_accuracy : 64.84\n",
      "[11,    51] loss: 1.65475, adv_train_accuracy: 37.70, clean_train_accuracy : 61.52\n",
      "[11,    61] loss: 1.61906, adv_train_accuracy: 38.87, clean_train_accuracy : 60.94\n",
      "[11,    71] loss: 1.69457, adv_train_accuracy: 35.94, clean_train_accuracy : 59.57\n",
      "[11,    81] loss: 1.67575, adv_train_accuracy: 35.74, clean_train_accuracy : 58.40\n",
      "[11,    91] loss: 1.58064, adv_train_accuracy: 40.23, clean_train_accuracy : 64.84\n",
      "0.6412109375\n",
      "0.568359375\n",
      "duration: 185 s - train loss: 1.66443 - train accuracy: 37.09 - validation loss: 0.94714 - validation accuracy: 67.47 \n",
      "[12,     1] loss: 1.71031, adv_train_accuracy: 33.79, clean_train_accuracy : 58.59\n",
      "[12,    11] loss: 1.67559, adv_train_accuracy: 38.09, clean_train_accuracy : 60.16\n",
      "[12,    21] loss: 1.62766, adv_train_accuracy: 40.04, clean_train_accuracy : 64.45\n",
      "[12,    31] loss: 1.66137, adv_train_accuracy: 35.16, clean_train_accuracy : 62.70\n",
      "[12,    41] loss: 1.67370, adv_train_accuracy: 36.13, clean_train_accuracy : 58.98\n",
      "[12,    51] loss: 1.67272, adv_train_accuracy: 36.13, clean_train_accuracy : 60.74\n",
      "[12,    61] loss: 1.59579, adv_train_accuracy: 41.99, clean_train_accuracy : 63.67\n",
      "[12,    71] loss: 1.61493, adv_train_accuracy: 35.74, clean_train_accuracy : 62.30\n",
      "[12,    81] loss: 1.59223, adv_train_accuracy: 41.99, clean_train_accuracy : 64.84\n",
      "[12,    91] loss: 1.59102, adv_train_accuracy: 38.87, clean_train_accuracy : 61.72\n",
      "0.6607421875\n",
      "0.56875\n",
      "duration: 184 s - train loss: 1.65379 - train accuracy: 37.63 - validation loss: 0.91355 - validation accuracy: 68.10 \n",
      "[13,     1] loss: 1.64389, adv_train_accuracy: 39.26, clean_train_accuracy : 59.96\n",
      "[13,    11] loss: 1.70838, adv_train_accuracy: 35.35, clean_train_accuracy : 58.01\n",
      "[13,    21] loss: 1.70822, adv_train_accuracy: 34.96, clean_train_accuracy : 60.35\n",
      "[13,    31] loss: 1.69709, adv_train_accuracy: 37.30, clean_train_accuracy : 61.13\n",
      "[13,    41] loss: 1.66565, adv_train_accuracy: 36.72, clean_train_accuracy : 64.45\n",
      "[13,    51] loss: 1.65925, adv_train_accuracy: 35.16, clean_train_accuracy : 62.89\n",
      "[13,    61] loss: 1.72085, adv_train_accuracy: 34.38, clean_train_accuracy : 57.42\n",
      "[13,    71] loss: 1.73263, adv_train_accuracy: 31.05, clean_train_accuracy : 55.66\n",
      "[13,    81] loss: 1.64196, adv_train_accuracy: 36.72, clean_train_accuracy : 62.89\n",
      "[13,    91] loss: 1.69120, adv_train_accuracy: 35.16, clean_train_accuracy : 55.86\n",
      "0.65\n",
      "0.58515625\n",
      "duration: 185 s - train loss: 1.66338 - train accuracy: 36.97 - validation loss: 0.90415 - validation accuracy: 68.85 \n",
      "[14,     1] loss: 1.59058, adv_train_accuracy: 38.28, clean_train_accuracy : 63.09\n",
      "[14,    11] loss: 1.66345, adv_train_accuracy: 40.43, clean_train_accuracy : 63.28\n",
      "[14,    21] loss: 1.71167, adv_train_accuracy: 35.94, clean_train_accuracy : 59.57\n",
      "[14,    31] loss: 1.63848, adv_train_accuracy: 36.72, clean_train_accuracy : 58.79\n",
      "[14,    41] loss: 1.67155, adv_train_accuracy: 34.18, clean_train_accuracy : 62.11\n",
      "[14,    51] loss: 1.60550, adv_train_accuracy: 37.11, clean_train_accuracy : 61.91\n",
      "[14,    61] loss: 1.64027, adv_train_accuracy: 38.67, clean_train_accuracy : 63.87\n",
      "[14,    71] loss: 1.69202, adv_train_accuracy: 34.96, clean_train_accuracy : 57.23\n",
      "[14,    81] loss: 1.66483, adv_train_accuracy: 37.50, clean_train_accuracy : 60.16\n",
      "[14,    91] loss: 1.71786, adv_train_accuracy: 37.50, clean_train_accuracy : 59.57\n",
      "0.6392578125\n",
      "0.576171875\n",
      "duration: 185 s - train loss: 1.65060 - train accuracy: 37.65 - validation loss: 0.91993 - validation accuracy: 69.14 \n",
      "[15,     1] loss: 1.60468, adv_train_accuracy: 39.45, clean_train_accuracy : 61.72\n",
      "[15,    11] loss: 1.62711, adv_train_accuracy: 38.48, clean_train_accuracy : 63.67\n",
      "[15,    21] loss: 1.66224, adv_train_accuracy: 34.96, clean_train_accuracy : 58.01\n",
      "[15,    31] loss: 1.61015, adv_train_accuracy: 39.45, clean_train_accuracy : 58.79\n",
      "[15,    41] loss: 1.48974, adv_train_accuracy: 44.73, clean_train_accuracy : 64.06\n",
      "[15,    51] loss: 1.34327, adv_train_accuracy: 49.41, clean_train_accuracy : 64.45\n",
      "[15,    61] loss: 1.20995, adv_train_accuracy: 55.47, clean_train_accuracy : 69.34\n",
      "[15,    71] loss: 1.14566, adv_train_accuracy: 58.59, clean_train_accuracy : 66.02\n",
      "[15,    81] loss: 1.17186, adv_train_accuracy: 58.79, clean_train_accuracy : 61.33\n",
      "[15,    91] loss: 1.03979, adv_train_accuracy: 62.89, clean_train_accuracy : 68.95\n",
      "0.5927734375\n",
      "0.497265625\n",
      "duration: 185 s - train loss: 1.36141 - train accuracy: 49.76 - validation loss: 0.80692 - validation accuracy: 71.52 \n",
      "[16,     1] loss: 1.02183, adv_train_accuracy: 64.65, clean_train_accuracy : 62.30\n",
      "[16,    11] loss: 0.88435, adv_train_accuracy: 70.90, clean_train_accuracy : 63.09\n",
      "[16,    21] loss: 0.91960, adv_train_accuracy: 67.58, clean_train_accuracy : 58.01\n",
      "[16,    31] loss: 1.19955, adv_train_accuracy: 57.42, clean_train_accuracy : 64.06\n",
      "[16,    41] loss: 0.90745, adv_train_accuracy: 68.55, clean_train_accuracy : 63.87\n",
      "[16,    51] loss: 0.81641, adv_train_accuracy: 71.09, clean_train_accuracy : 58.59\n",
      "[16,    61] loss: 0.76537, adv_train_accuracy: 72.85, clean_train_accuracy : 63.67\n",
      "[16,    71] loss: 0.65123, adv_train_accuracy: 78.91, clean_train_accuracy : 63.09\n",
      "[16,    81] loss: 0.58602, adv_train_accuracy: 78.12, clean_train_accuracy : 57.62\n",
      "[16,    91] loss: 0.91167, adv_train_accuracy: 68.75, clean_train_accuracy : 54.10\n",
      "0.6568359375\n",
      "0.4763671875\n",
      "duration: 184 s - train loss: 0.87985 - train accuracy: 69.64 - validation loss: 0.96094 - validation accuracy: 71.11 \n",
      "[17,     1] loss: 0.76609, adv_train_accuracy: 74.22, clean_train_accuracy : 60.35\n",
      "[17,    11] loss: 0.89331, adv_train_accuracy: 68.55, clean_train_accuracy : 60.74\n",
      "[17,    21] loss: 0.71769, adv_train_accuracy: 76.37, clean_train_accuracy : 63.09\n",
      "[17,    31] loss: 0.66237, adv_train_accuracy: 74.80, clean_train_accuracy : 56.25\n",
      "[17,    41] loss: 0.57140, adv_train_accuracy: 79.10, clean_train_accuracy : 59.96\n",
      "[17,    51] loss: 0.83349, adv_train_accuracy: 71.09, clean_train_accuracy : 61.72\n",
      "[17,    61] loss: 0.87753, adv_train_accuracy: 70.12, clean_train_accuracy : 63.67\n",
      "[17,    71] loss: 0.94493, adv_train_accuracy: 69.73, clean_train_accuracy : 60.74\n",
      "[17,    81] loss: 0.86340, adv_train_accuracy: 69.53, clean_train_accuracy : 62.50\n",
      "[17,    91] loss: 0.71553, adv_train_accuracy: 75.00, clean_train_accuracy : 64.65\n",
      "0.6802734375\n",
      "0.568359375\n",
      "duration: 192 s - train loss: 0.77732 - train accuracy: 73.38 - validation loss: 0.94772 - validation accuracy: 70.66 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,     1] loss: 0.70508, adv_train_accuracy: 75.59, clean_train_accuracy : 62.50\n",
      "[18,    11] loss: 0.59213, adv_train_accuracy: 81.05, clean_train_accuracy : 58.59\n",
      "[18,    21] loss: 0.50417, adv_train_accuracy: 83.59, clean_train_accuracy : 56.25\n",
      "[18,    31] loss: 0.44692, adv_train_accuracy: 84.77, clean_train_accuracy : 59.57\n",
      "[18,    41] loss: 0.48281, adv_train_accuracy: 83.01, clean_train_accuracy : 52.93\n",
      "[18,    51] loss: 0.62277, adv_train_accuracy: 79.88, clean_train_accuracy : 60.74\n",
      "[18,    61] loss: 0.64278, adv_train_accuracy: 77.15, clean_train_accuracy : 53.12\n",
      "[18,    71] loss: 0.57398, adv_train_accuracy: 81.45, clean_train_accuracy : 62.70\n",
      "[18,    81] loss: 0.69824, adv_train_accuracy: 75.98, clean_train_accuracy : 57.62\n",
      "[18,    91] loss: 0.55563, adv_train_accuracy: 83.20, clean_train_accuracy : 64.06\n",
      "0.8431640625\n",
      "0.6947265625\n",
      "duration: 193 s - train loss: 0.59679 - train accuracy: 80.29 - validation loss: 0.85993 - validation accuracy: 72.81 \n",
      "[19,     1] loss: 0.66234, adv_train_accuracy: 76.76, clean_train_accuracy : 59.77\n",
      "[19,    11] loss: 0.43241, adv_train_accuracy: 84.38, clean_train_accuracy : 56.84\n",
      "[19,    21] loss: 0.48448, adv_train_accuracy: 84.18, clean_train_accuracy : 61.33\n",
      "[19,    31] loss: 0.39159, adv_train_accuracy: 86.13, clean_train_accuracy : 61.33\n",
      "[19,    41] loss: 0.37095, adv_train_accuracy: 86.33, clean_train_accuracy : 61.13\n",
      "[19,    51] loss: 0.31408, adv_train_accuracy: 87.70, clean_train_accuracy : 58.20\n",
      "[19,    61] loss: 0.30240, adv_train_accuracy: 90.43, clean_train_accuracy : 55.86\n",
      "[19,    71] loss: 0.32412, adv_train_accuracy: 87.89, clean_train_accuracy : 55.08\n",
      "[19,    81] loss: 0.43908, adv_train_accuracy: 86.72, clean_train_accuracy : 59.18\n",
      "[19,    91] loss: 0.45400, adv_train_accuracy: 85.74, clean_train_accuracy : 52.54\n",
      "0.9001953125\n",
      "0.7298828125\n",
      "duration: 192 s - train loss: 0.41698 - train accuracy: 86.21 - validation loss: 1.05686 - validation accuracy: 69.53 \n",
      "[20,     1] loss: 0.38687, adv_train_accuracy: 86.52, clean_train_accuracy : 57.62\n",
      "[20,    11] loss: 0.36872, adv_train_accuracy: 87.89, clean_train_accuracy : 54.30\n",
      "[20,    21] loss: 0.37797, adv_train_accuracy: 88.87, clean_train_accuracy : 58.98\n",
      "[20,    31] loss: 0.33205, adv_train_accuracy: 86.33, clean_train_accuracy : 57.23\n",
      "[20,    41] loss: 0.38223, adv_train_accuracy: 88.28, clean_train_accuracy : 60.16\n",
      "[20,    51] loss: 0.41785, adv_train_accuracy: 85.35, clean_train_accuracy : 58.01\n",
      "[20,    61] loss: 0.47394, adv_train_accuracy: 84.18, clean_train_accuracy : 58.79\n",
      "[20,    71] loss: 0.55616, adv_train_accuracy: 80.27, clean_train_accuracy : 67.38\n",
      "[20,    81] loss: 0.51161, adv_train_accuracy: 83.59, clean_train_accuracy : 59.18\n",
      "[20,    91] loss: 0.54276, adv_train_accuracy: 81.84, clean_train_accuracy : 62.89\n",
      "0.928125\n",
      "0.7953125\n",
      "duration: 192 s - train loss: 0.49562 - train accuracy: 83.68 - validation loss: 1.34735 - validation accuracy: 62.06 \n",
      "[21,     1] loss: 0.47948, adv_train_accuracy: 82.81, clean_train_accuracy : 58.59\n",
      "[21,    11] loss: 0.40367, adv_train_accuracy: 85.94, clean_train_accuracy : 61.13\n",
      "[21,    21] loss: 0.35118, adv_train_accuracy: 88.48, clean_train_accuracy : 60.16\n",
      "[21,    31] loss: 0.36705, adv_train_accuracy: 88.48, clean_train_accuracy : 56.64\n",
      "[21,    41] loss: 0.42914, adv_train_accuracy: 86.52, clean_train_accuracy : 60.94\n",
      "[21,    51] loss: 0.42750, adv_train_accuracy: 87.11, clean_train_accuracy : 51.37\n",
      "[21,    61] loss: 0.60653, adv_train_accuracy: 82.03, clean_train_accuracy : 59.77\n",
      "[21,    71] loss: 0.61383, adv_train_accuracy: 79.69, clean_train_accuracy : 57.23\n",
      "[21,    81] loss: 0.47140, adv_train_accuracy: 85.94, clean_train_accuracy : 60.94\n",
      "[21,    91] loss: 0.47781, adv_train_accuracy: 84.96, clean_train_accuracy : 54.10\n",
      "0.8189453125\n",
      "0.6224609375\n",
      "duration: 188 s - train loss: 0.43643 - train accuracy: 85.72 - validation loss: 1.02823 - validation accuracy: 71.57 \n",
      "[22,     1] loss: 0.40675, adv_train_accuracy: 86.91, clean_train_accuracy : 58.79\n",
      "[22,    11] loss: 0.37650, adv_train_accuracy: 88.09, clean_train_accuracy : 60.74\n",
      "[22,    21] loss: 0.45024, adv_train_accuracy: 84.77, clean_train_accuracy : 61.72\n",
      "[22,    31] loss: 0.38859, adv_train_accuracy: 89.06, clean_train_accuracy : 61.33\n",
      "[22,    41] loss: 0.59403, adv_train_accuracy: 80.66, clean_train_accuracy : 54.10\n",
      "[22,    51] loss: 0.44194, adv_train_accuracy: 84.77, clean_train_accuracy : 55.08\n",
      "[22,    61] loss: 0.42613, adv_train_accuracy: 84.96, clean_train_accuracy : 59.18\n",
      "[22,    71] loss: 0.35768, adv_train_accuracy: 86.91, clean_train_accuracy : 61.33\n",
      "[22,    81] loss: 0.33417, adv_train_accuracy: 90.23, clean_train_accuracy : 59.18\n",
      "[22,    91] loss: 0.35533, adv_train_accuracy: 89.65, clean_train_accuracy : 61.13\n",
      "0.8044921875\n",
      "0.6296875\n",
      "duration: 185 s - train loss: 0.41780 - train accuracy: 86.31 - validation loss: 0.94760 - validation accuracy: 73.03 \n",
      "[23,     1] loss: 0.33138, adv_train_accuracy: 89.26, clean_train_accuracy : 63.48\n",
      "[23,    11] loss: 0.40450, adv_train_accuracy: 87.11, clean_train_accuracy : 56.45\n",
      "[23,    21] loss: 0.36628, adv_train_accuracy: 88.67, clean_train_accuracy : 64.06\n",
      "[23,    31] loss: 0.33557, adv_train_accuracy: 88.28, clean_train_accuracy : 62.30\n",
      "[23,    41] loss: 0.33503, adv_train_accuracy: 88.67, clean_train_accuracy : 54.49\n",
      "[23,    51] loss: 0.48929, adv_train_accuracy: 84.18, clean_train_accuracy : 60.55\n",
      "[23,    61] loss: 0.36935, adv_train_accuracy: 87.30, clean_train_accuracy : 60.35\n",
      "[23,    71] loss: 0.41019, adv_train_accuracy: 86.91, clean_train_accuracy : 62.89\n",
      "[23,    81] loss: 0.50142, adv_train_accuracy: 83.98, clean_train_accuracy : 64.65\n",
      "[23,    91] loss: 0.48120, adv_train_accuracy: 84.18, clean_train_accuracy : 60.74\n",
      "0.6935546875\n",
      "0.44296875\n",
      "duration: 185 s - train loss: 0.44164 - train accuracy: 85.46 - validation loss: 0.91153 - validation accuracy: 73.83 \n",
      "[24,     1] loss: 0.43575, adv_train_accuracy: 83.20, clean_train_accuracy : 61.52\n",
      "[24,    11] loss: 0.34347, adv_train_accuracy: 88.09, clean_train_accuracy : 62.11\n",
      "[24,    21] loss: 0.47673, adv_train_accuracy: 83.59, clean_train_accuracy : 59.77\n",
      "[24,    31] loss: 0.48196, adv_train_accuracy: 82.42, clean_train_accuracy : 54.30\n",
      "[24,    41] loss: 0.39350, adv_train_accuracy: 88.09, clean_train_accuracy : 58.98\n",
      "[24,    51] loss: 0.40244, adv_train_accuracy: 85.16, clean_train_accuracy : 64.26\n",
      "[24,    61] loss: 0.54528, adv_train_accuracy: 81.84, clean_train_accuracy : 64.45\n",
      "[24,    71] loss: 0.46413, adv_train_accuracy: 85.16, clean_train_accuracy : 58.40\n",
      "[24,    81] loss: 0.34899, adv_train_accuracy: 87.30, clean_train_accuracy : 63.67\n",
      "[24,    91] loss: 0.33849, adv_train_accuracy: 88.87, clean_train_accuracy : 58.40\n",
      "0.687890625\n",
      "0.494921875\n",
      "duration: 185 s - train loss: 0.42782 - train accuracy: 85.81 - validation loss: 1.14051 - validation accuracy: 72.59 \n",
      "[25,     1] loss: 0.27642, adv_train_accuracy: 91.21, clean_train_accuracy : 60.16\n",
      "[25,    11] loss: 0.34116, adv_train_accuracy: 88.48, clean_train_accuracy : 56.84\n",
      "[25,    21] loss: 0.36765, adv_train_accuracy: 88.09, clean_train_accuracy : 59.77\n",
      "[25,    31] loss: 0.35672, adv_train_accuracy: 89.26, clean_train_accuracy : 60.55\n",
      "[25,    41] loss: 0.33002, adv_train_accuracy: 90.62, clean_train_accuracy : 59.18\n",
      "[25,    51] loss: 0.35479, adv_train_accuracy: 88.87, clean_train_accuracy : 59.57\n",
      "[25,    61] loss: 0.32146, adv_train_accuracy: 87.50, clean_train_accuracy : 62.11\n",
      "[25,    71] loss: 0.36942, adv_train_accuracy: 88.67, clean_train_accuracy : 63.48\n",
      "[25,    81] loss: 0.32248, adv_train_accuracy: 89.65, clean_train_accuracy : 61.33\n",
      "[25,    91] loss: 0.23394, adv_train_accuracy: 91.60, clean_train_accuracy : 62.89\n",
      "0.6029296875\n",
      "0.45234375\n",
      "duration: 185 s - train loss: 0.32020 - train accuracy: 89.51 - validation loss: 1.09236 - validation accuracy: 72.18 \n",
      "[26,     1] loss: 0.33303, adv_train_accuracy: 88.48, clean_train_accuracy : 59.18\n",
      "[26,    11] loss: 0.31585, adv_train_accuracy: 89.06, clean_train_accuracy : 57.62\n",
      "[26,    21] loss: 0.26159, adv_train_accuracy: 91.80, clean_train_accuracy : 61.33\n",
      "[26,    31] loss: 0.31352, adv_train_accuracy: 89.26, clean_train_accuracy : 58.40\n",
      "[26,    41] loss: 0.31840, adv_train_accuracy: 88.87, clean_train_accuracy : 61.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26,    51] loss: 0.27810, adv_train_accuracy: 90.23, clean_train_accuracy : 61.91\n",
      "[26,    61] loss: 0.41897, adv_train_accuracy: 86.33, clean_train_accuracy : 56.45\n",
      "[26,    71] loss: 0.39834, adv_train_accuracy: 88.09, clean_train_accuracy : 65.82\n",
      "[26,    81] loss: 0.36925, adv_train_accuracy: 87.30, clean_train_accuracy : 63.28\n",
      "[26,    91] loss: 0.28689, adv_train_accuracy: 90.04, clean_train_accuracy : 59.57\n",
      "0.4453125\n",
      "0.324609375\n",
      "duration: 185 s - train loss: 0.29665 - train accuracy: 90.15 - validation loss: 1.03212 - validation accuracy: 73.76 \n",
      "[27,     1] loss: 0.28371, adv_train_accuracy: 90.62, clean_train_accuracy : 63.67\n",
      "[27,    11] loss: 0.30424, adv_train_accuracy: 89.45, clean_train_accuracy : 54.69\n",
      "[27,    21] loss: 0.29896, adv_train_accuracy: 90.04, clean_train_accuracy : 61.33\n",
      "[27,    31] loss: 0.34701, adv_train_accuracy: 89.84, clean_train_accuracy : 60.55\n",
      "[27,    41] loss: 0.22363, adv_train_accuracy: 92.77, clean_train_accuracy : 60.74\n",
      "[27,    51] loss: 0.20607, adv_train_accuracy: 92.97, clean_train_accuracy : 62.50\n",
      "[27,    61] loss: 0.21194, adv_train_accuracy: 93.36, clean_train_accuracy : 61.33\n",
      "[27,    71] loss: 0.19782, adv_train_accuracy: 93.36, clean_train_accuracy : 60.16\n",
      "[27,    81] loss: 0.23948, adv_train_accuracy: 91.60, clean_train_accuracy : 62.50\n",
      "[27,    91] loss: 0.33884, adv_train_accuracy: 89.26, clean_train_accuracy : 59.18\n",
      "0.4224609375\n",
      "0.348828125\n",
      "duration: 191 s - train loss: 0.27484 - train accuracy: 90.98 - validation loss: 1.04836 - validation accuracy: 73.18 \n",
      "[28,     1] loss: 0.28939, adv_train_accuracy: 90.62, clean_train_accuracy : 58.79\n",
      "[28,    11] loss: 0.21774, adv_train_accuracy: 92.19, clean_train_accuracy : 64.06\n",
      "[28,    21] loss: 0.25289, adv_train_accuracy: 90.62, clean_train_accuracy : 56.64\n",
      "[28,    31] loss: 0.23538, adv_train_accuracy: 91.02, clean_train_accuracy : 63.87\n",
      "[28,    41] loss: 0.27225, adv_train_accuracy: 91.21, clean_train_accuracy : 59.77\n",
      "[28,    51] loss: 0.27100, adv_train_accuracy: 91.21, clean_train_accuracy : 63.09\n",
      "[28,    61] loss: 0.23041, adv_train_accuracy: 91.80, clean_train_accuracy : 57.62\n",
      "[28,    71] loss: 0.30823, adv_train_accuracy: 89.45, clean_train_accuracy : 64.45\n",
      "[28,    81] loss: 0.28413, adv_train_accuracy: 91.21, clean_train_accuracy : 61.72\n",
      "[28,    91] loss: 0.29380, adv_train_accuracy: 89.65, clean_train_accuracy : 62.11\n",
      "0.3798828125\n",
      "0.3294921875\n",
      "duration: 191 s - train loss: 0.25502 - train accuracy: 91.51 - validation loss: 1.04963 - validation accuracy: 72.48 \n",
      "[29,     1] loss: 0.27900, adv_train_accuracy: 90.43, clean_train_accuracy : 60.55\n",
      "[29,    11] loss: 0.24622, adv_train_accuracy: 92.58, clean_train_accuracy : 62.89\n",
      "[29,    21] loss: 0.20672, adv_train_accuracy: 93.75, clean_train_accuracy : 61.33\n",
      "[29,    31] loss: 0.25917, adv_train_accuracy: 91.80, clean_train_accuracy : 63.48\n",
      "[29,    41] loss: 0.30421, adv_train_accuracy: 90.04, clean_train_accuracy : 61.13\n",
      "[29,    51] loss: 0.23804, adv_train_accuracy: 91.02, clean_train_accuracy : 55.66\n",
      "[29,    61] loss: 0.34456, adv_train_accuracy: 90.04, clean_train_accuracy : 62.50\n",
      "[29,    71] loss: 0.23784, adv_train_accuracy: 92.58, clean_train_accuracy : 62.50\n",
      "[29,    81] loss: 0.27187, adv_train_accuracy: 91.60, clean_train_accuracy : 60.74\n",
      "[29,    91] loss: 0.32060, adv_train_accuracy: 90.62, clean_train_accuracy : 56.25\n",
      "0.64921875\n",
      "0.366015625\n",
      "duration: 186 s - train loss: 0.25170 - train accuracy: 91.80 - validation loss: 0.83805 - validation accuracy: 76.12 \n",
      "[30,     1] loss: 0.39041, adv_train_accuracy: 87.89, clean_train_accuracy : 62.11\n",
      "[30,    11] loss: 0.25249, adv_train_accuracy: 91.21, clean_train_accuracy : 65.04\n",
      "[30,    21] loss: 0.16502, adv_train_accuracy: 94.92, clean_train_accuracy : 66.60\n",
      "[30,    31] loss: 0.14231, adv_train_accuracy: 95.90, clean_train_accuracy : 63.28\n",
      "[30,    41] loss: 0.20477, adv_train_accuracy: 92.97, clean_train_accuracy : 62.70\n",
      "[30,    51] loss: 0.18559, adv_train_accuracy: 94.34, clean_train_accuracy : 62.30\n",
      "[30,    61] loss: 0.23483, adv_train_accuracy: 90.82, clean_train_accuracy : 65.43\n",
      "[30,    71] loss: 0.23998, adv_train_accuracy: 91.41, clean_train_accuracy : 57.81\n",
      "[30,    81] loss: 0.20471, adv_train_accuracy: 93.55, clean_train_accuracy : 62.50\n",
      "[30,    91] loss: 0.17965, adv_train_accuracy: 94.14, clean_train_accuracy : 63.87\n",
      "0.25\n",
      "0.274609375\n",
      "duration: 184 s - train loss: 0.23031 - train accuracy: 92.51 - validation loss: 1.16202 - validation accuracy: 73.29 \n",
      "[31,     1] loss: 0.30341, adv_train_accuracy: 90.43, clean_train_accuracy : 56.84\n",
      "[31,    11] loss: 0.25877, adv_train_accuracy: 90.23, clean_train_accuracy : 58.98\n",
      "[31,    21] loss: 0.24039, adv_train_accuracy: 91.99, clean_train_accuracy : 63.48\n",
      "[31,    31] loss: 0.22103, adv_train_accuracy: 92.97, clean_train_accuracy : 58.01\n",
      "[31,    41] loss: 0.21934, adv_train_accuracy: 93.16, clean_train_accuracy : 61.33\n",
      "[31,    51] loss: 0.15685, adv_train_accuracy: 95.51, clean_train_accuracy : 64.65\n",
      "[31,    61] loss: 0.23992, adv_train_accuracy: 91.21, clean_train_accuracy : 63.48\n",
      "[31,    71] loss: 0.19307, adv_train_accuracy: 94.53, clean_train_accuracy : 64.26\n",
      "[31,    81] loss: 0.19426, adv_train_accuracy: 93.75, clean_train_accuracy : 59.38\n",
      "[31,    91] loss: 0.20078, adv_train_accuracy: 93.16, clean_train_accuracy : 58.98\n",
      "0.243359375\n",
      "0.1814453125\n",
      "duration: 184 s - train loss: 0.21604 - train accuracy: 92.95 - validation loss: 0.92563 - validation accuracy: 75.93 \n",
      "[32,     1] loss: 0.26559, adv_train_accuracy: 91.60, clean_train_accuracy : 62.30\n",
      "[32,    11] loss: 0.19611, adv_train_accuracy: 93.16, clean_train_accuracy : 60.35\n",
      "[32,    21] loss: 0.16489, adv_train_accuracy: 94.73, clean_train_accuracy : 57.23\n",
      "[32,    31] loss: 0.11907, adv_train_accuracy: 95.31, clean_train_accuracy : 65.43\n",
      "[32,    41] loss: 0.19500, adv_train_accuracy: 93.36, clean_train_accuracy : 66.41\n",
      "[32,    51] loss: 0.15085, adv_train_accuracy: 95.12, clean_train_accuracy : 64.26\n",
      "[32,    61] loss: 0.27790, adv_train_accuracy: 91.21, clean_train_accuracy : 62.70\n",
      "[32,    71] loss: 0.24822, adv_train_accuracy: 93.95, clean_train_accuracy : 64.45\n",
      "[32,    81] loss: 0.22175, adv_train_accuracy: 93.36, clean_train_accuracy : 68.16\n",
      "[32,    91] loss: 0.19170, adv_train_accuracy: 92.77, clean_train_accuracy : 63.67\n",
      "0.1345703125\n",
      "0.1826171875\n",
      "duration: 184 s - train loss: 0.21657 - train accuracy: 92.92 - validation loss: 0.98461 - validation accuracy: 75.40 \n",
      "[33,     1] loss: 0.24032, adv_train_accuracy: 92.97, clean_train_accuracy : 62.30\n",
      "[33,    11] loss: 0.26053, adv_train_accuracy: 90.23, clean_train_accuracy : 63.67\n",
      "[33,    21] loss: 0.31353, adv_train_accuracy: 89.65, clean_train_accuracy : 59.18\n",
      "[33,    31] loss: 0.19596, adv_train_accuracy: 92.77, clean_train_accuracy : 64.26\n",
      "[33,    41] loss: 0.24779, adv_train_accuracy: 91.60, clean_train_accuracy : 60.55\n",
      "[33,    51] loss: 0.13411, adv_train_accuracy: 95.12, clean_train_accuracy : 62.50\n",
      "[33,    61] loss: 0.16674, adv_train_accuracy: 95.12, clean_train_accuracy : 62.11\n",
      "[33,    71] loss: 0.20288, adv_train_accuracy: 92.77, clean_train_accuracy : 58.79\n",
      "[33,    81] loss: 0.19144, adv_train_accuracy: 93.75, clean_train_accuracy : 60.16\n",
      "[33,    91] loss: 0.15627, adv_train_accuracy: 95.12, clean_train_accuracy : 64.26\n",
      "0.1943359375\n",
      "0.2099609375\n",
      "duration: 185 s - train loss: 0.20111 - train accuracy: 93.32 - validation loss: 0.98604 - validation accuracy: 76.58 \n",
      "[34,     1] loss: 0.14193, adv_train_accuracy: 96.68, clean_train_accuracy : 64.45\n",
      "[34,    11] loss: 0.19243, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[34,    21] loss: 0.12953, adv_train_accuracy: 95.51, clean_train_accuracy : 68.55\n",
      "[34,    31] loss: 0.20705, adv_train_accuracy: 92.58, clean_train_accuracy : 61.91\n",
      "[34,    41] loss: 0.17811, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[34,    51] loss: 0.13120, adv_train_accuracy: 96.09, clean_train_accuracy : 66.80\n",
      "[34,    61] loss: 0.28770, adv_train_accuracy: 89.84, clean_train_accuracy : 64.84\n",
      "[34,    71] loss: 0.22561, adv_train_accuracy: 93.16, clean_train_accuracy : 61.33\n",
      "[34,    81] loss: 0.23034, adv_train_accuracy: 92.77, clean_train_accuracy : 64.26\n",
      "[34,    91] loss: 0.18963, adv_train_accuracy: 93.36, clean_train_accuracy : 61.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22421875\n",
      "0.2140625\n",
      "duration: 185 s - train loss: 0.22045 - train accuracy: 92.77 - validation loss: 0.94493 - validation accuracy: 77.29 \n",
      "[35,     1] loss: 0.16332, adv_train_accuracy: 95.90, clean_train_accuracy : 61.52\n",
      "[35,    11] loss: 0.21065, adv_train_accuracy: 93.16, clean_train_accuracy : 65.23\n",
      "[35,    21] loss: 0.21400, adv_train_accuracy: 93.16, clean_train_accuracy : 60.74\n",
      "[35,    31] loss: 0.22202, adv_train_accuracy: 92.58, clean_train_accuracy : 62.50\n",
      "[35,    41] loss: 0.21658, adv_train_accuracy: 92.58, clean_train_accuracy : 64.26\n",
      "[35,    51] loss: 0.24467, adv_train_accuracy: 91.60, clean_train_accuracy : 58.01\n",
      "[35,    61] loss: 0.20539, adv_train_accuracy: 94.34, clean_train_accuracy : 63.09\n",
      "[35,    71] loss: 0.24316, adv_train_accuracy: 91.60, clean_train_accuracy : 60.74\n",
      "[35,    81] loss: 0.18523, adv_train_accuracy: 93.36, clean_train_accuracy : 66.99\n",
      "[35,    91] loss: 0.16102, adv_train_accuracy: 94.53, clean_train_accuracy : 61.72\n",
      "0.1236328125\n",
      "0.1564453125\n",
      "duration: 185 s - train loss: 0.21428 - train accuracy: 92.93 - validation loss: 0.95413 - validation accuracy: 77.64 \n",
      "[36,     1] loss: 0.18811, adv_train_accuracy: 94.34, clean_train_accuracy : 65.62\n",
      "[36,    11] loss: 0.18536, adv_train_accuracy: 92.97, clean_train_accuracy : 66.02\n",
      "[36,    21] loss: 0.27557, adv_train_accuracy: 91.60, clean_train_accuracy : 65.23\n",
      "[36,    31] loss: 0.15205, adv_train_accuracy: 94.53, clean_train_accuracy : 59.96\n",
      "[36,    41] loss: 0.22572, adv_train_accuracy: 92.38, clean_train_accuracy : 65.82\n",
      "[36,    51] loss: 0.17707, adv_train_accuracy: 93.16, clean_train_accuracy : 64.84\n",
      "[36,    61] loss: 0.24425, adv_train_accuracy: 92.38, clean_train_accuracy : 63.28\n",
      "[36,    71] loss: 0.23167, adv_train_accuracy: 93.16, clean_train_accuracy : 62.30\n",
      "[36,    81] loss: 0.22013, adv_train_accuracy: 93.55, clean_train_accuracy : 66.60\n",
      "[36,    91] loss: 0.13255, adv_train_accuracy: 95.70, clean_train_accuracy : 68.36\n",
      "0.1267578125\n",
      "0.1853515625\n",
      "duration: 185 s - train loss: 0.21184 - train accuracy: 93.14 - validation loss: 0.96178 - validation accuracy: 76.33 \n",
      "[37,     1] loss: 0.17832, adv_train_accuracy: 93.75, clean_train_accuracy : 67.77\n",
      "[37,    11] loss: 0.23970, adv_train_accuracy: 91.60, clean_train_accuracy : 67.19\n",
      "[37,    21] loss: 0.16397, adv_train_accuracy: 94.73, clean_train_accuracy : 66.41\n",
      "[37,    31] loss: 0.25541, adv_train_accuracy: 92.19, clean_train_accuracy : 63.67\n",
      "[37,    41] loss: 0.22534, adv_train_accuracy: 92.97, clean_train_accuracy : 59.57\n",
      "[37,    51] loss: 0.32777, adv_train_accuracy: 89.26, clean_train_accuracy : 62.70\n",
      "[37,    61] loss: 0.25679, adv_train_accuracy: 91.02, clean_train_accuracy : 63.09\n",
      "[37,    71] loss: 0.17027, adv_train_accuracy: 94.14, clean_train_accuracy : 64.65\n",
      "[37,    81] loss: 0.19303, adv_train_accuracy: 93.36, clean_train_accuracy : 66.02\n",
      "[37,    91] loss: 0.21489, adv_train_accuracy: 93.55, clean_train_accuracy : 65.43\n",
      "0.1650390625\n",
      "0.1703125\n",
      "duration: 185 s - train loss: 0.21288 - train accuracy: 93.06 - validation loss: 0.94011 - validation accuracy: 76.04 \n",
      "[38,     1] loss: 0.19660, adv_train_accuracy: 92.58, clean_train_accuracy : 63.28\n",
      "[38,    11] loss: 0.20552, adv_train_accuracy: 92.77, clean_train_accuracy : 64.26\n",
      "[38,    21] loss: 0.18759, adv_train_accuracy: 92.19, clean_train_accuracy : 63.67\n",
      "[38,    31] loss: 0.18061, adv_train_accuracy: 92.77, clean_train_accuracy : 69.34\n",
      "[38,    41] loss: 0.19180, adv_train_accuracy: 93.55, clean_train_accuracy : 68.36\n",
      "[38,    51] loss: 0.24408, adv_train_accuracy: 91.60, clean_train_accuracy : 64.06\n",
      "[38,    61] loss: 0.27539, adv_train_accuracy: 90.62, clean_train_accuracy : 60.35\n",
      "[38,    71] loss: 0.18099, adv_train_accuracy: 94.14, clean_train_accuracy : 64.65\n",
      "[38,    81] loss: 0.18604, adv_train_accuracy: 93.95, clean_train_accuracy : 65.62\n",
      "[38,    91] loss: 0.21960, adv_train_accuracy: 93.75, clean_train_accuracy : 64.45\n",
      "0.121484375\n",
      "0.162890625\n",
      "duration: 184 s - train loss: 0.21428 - train accuracy: 92.97 - validation loss: 1.06466 - validation accuracy: 75.12 \n",
      "[39,     1] loss: 0.19226, adv_train_accuracy: 93.55, clean_train_accuracy : 62.30\n",
      "[39,    11] loss: 0.16908, adv_train_accuracy: 93.95, clean_train_accuracy : 67.19\n",
      "[39,    21] loss: 0.17060, adv_train_accuracy: 94.34, clean_train_accuracy : 64.84\n",
      "[39,    31] loss: 0.17974, adv_train_accuracy: 94.92, clean_train_accuracy : 65.23\n",
      "[39,    41] loss: 0.24401, adv_train_accuracy: 92.19, clean_train_accuracy : 68.36\n",
      "[39,    51] loss: 0.18758, adv_train_accuracy: 93.36, clean_train_accuracy : 63.28\n",
      "[39,    61] loss: 0.16126, adv_train_accuracy: 95.51, clean_train_accuracy : 68.55\n",
      "[39,    71] loss: 0.20626, adv_train_accuracy: 92.97, clean_train_accuracy : 63.87\n",
      "[39,    81] loss: 0.18757, adv_train_accuracy: 93.75, clean_train_accuracy : 66.80\n",
      "[39,    91] loss: 0.25806, adv_train_accuracy: 92.38, clean_train_accuracy : 67.19\n",
      "0.0673828125\n",
      "0.140625\n",
      "duration: 189 s - train loss: 0.21127 - train accuracy: 93.08 - validation loss: 1.08312 - validation accuracy: 75.83 \n",
      "[40,     1] loss: 0.17883, adv_train_accuracy: 94.34, clean_train_accuracy : 69.53\n",
      "[40,    11] loss: 0.25515, adv_train_accuracy: 91.02, clean_train_accuracy : 62.89\n",
      "[40,    21] loss: 0.14343, adv_train_accuracy: 95.31, clean_train_accuracy : 71.68\n",
      "[40,    31] loss: 0.26935, adv_train_accuracy: 90.82, clean_train_accuracy : 62.30\n",
      "[40,    41] loss: 0.22408, adv_train_accuracy: 93.75, clean_train_accuracy : 68.16\n",
      "[40,    51] loss: 0.21364, adv_train_accuracy: 93.75, clean_train_accuracy : 66.60\n",
      "[40,    61] loss: 0.22363, adv_train_accuracy: 91.21, clean_train_accuracy : 62.30\n",
      "[40,    71] loss: 0.16064, adv_train_accuracy: 94.14, clean_train_accuracy : 66.99\n",
      "[40,    81] loss: 0.24718, adv_train_accuracy: 91.21, clean_train_accuracy : 62.50\n",
      "[40,    91] loss: 0.20120, adv_train_accuracy: 93.36, clean_train_accuracy : 62.30\n",
      "0.0505859375\n",
      "0.12578125\n",
      "duration: 190 s - train loss: 0.21526 - train accuracy: 92.86 - validation loss: 1.12553 - validation accuracy: 76.11 \n",
      "[41,     1] loss: 0.14935, adv_train_accuracy: 95.51, clean_train_accuracy : 67.97\n",
      "[41,    11] loss: 0.15680, adv_train_accuracy: 95.70, clean_train_accuracy : 61.72\n",
      "[41,    21] loss: 0.15975, adv_train_accuracy: 94.34, clean_train_accuracy : 65.62\n",
      "[41,    31] loss: 0.19890, adv_train_accuracy: 93.55, clean_train_accuracy : 63.28\n",
      "[41,    41] loss: 0.13741, adv_train_accuracy: 95.51, clean_train_accuracy : 67.58\n",
      "[41,    51] loss: 0.15532, adv_train_accuracy: 94.73, clean_train_accuracy : 66.21\n",
      "[41,    61] loss: 0.25296, adv_train_accuracy: 92.38, clean_train_accuracy : 63.48\n",
      "[41,    71] loss: 0.15978, adv_train_accuracy: 93.16, clean_train_accuracy : 65.04\n",
      "[41,    81] loss: 0.19701, adv_train_accuracy: 93.95, clean_train_accuracy : 68.16\n",
      "[41,    91] loss: 0.15725, adv_train_accuracy: 94.73, clean_train_accuracy : 62.50\n",
      "0.09375\n",
      "0.161328125\n",
      "duration: 185 s - train loss: 0.18916 - train accuracy: 93.75 - validation loss: 1.11303 - validation accuracy: 76.26 \n",
      "[42,     1] loss: 0.20001, adv_train_accuracy: 93.55, clean_train_accuracy : 68.75\n",
      "[42,    11] loss: 0.20077, adv_train_accuracy: 94.14, clean_train_accuracy : 65.43\n",
      "[42,    21] loss: 0.17630, adv_train_accuracy: 94.53, clean_train_accuracy : 64.45\n",
      "[42,    31] loss: 0.18033, adv_train_accuracy: 93.75, clean_train_accuracy : 67.77\n",
      "[42,    41] loss: 0.20020, adv_train_accuracy: 92.38, clean_train_accuracy : 63.09\n",
      "[42,    51] loss: 0.24324, adv_train_accuracy: 93.16, clean_train_accuracy : 64.45\n",
      "[42,    61] loss: 0.21040, adv_train_accuracy: 93.36, clean_train_accuracy : 66.80\n",
      "[42,    71] loss: 0.18594, adv_train_accuracy: 94.14, clean_train_accuracy : 60.35\n",
      "[42,    81] loss: 0.19018, adv_train_accuracy: 93.55, clean_train_accuracy : 68.36\n",
      "[42,    91] loss: 0.23237, adv_train_accuracy: 91.60, clean_train_accuracy : 64.65\n",
      "0.0939453125\n",
      "0.133984375\n",
      "duration: 184 s - train loss: 0.18168 - train accuracy: 94.02 - validation loss: 1.02517 - validation accuracy: 75.69 \n",
      "[43,     1] loss: 0.22379, adv_train_accuracy: 92.38, clean_train_accuracy : 66.41\n",
      "[43,    11] loss: 0.16389, adv_train_accuracy: 94.34, clean_train_accuracy : 67.19\n",
      "[43,    21] loss: 0.13417, adv_train_accuracy: 95.51, clean_train_accuracy : 63.48\n",
      "[43,    31] loss: 0.26267, adv_train_accuracy: 91.21, clean_train_accuracy : 65.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43,    41] loss: 0.18786, adv_train_accuracy: 93.95, clean_train_accuracy : 65.23\n",
      "[43,    51] loss: 0.16205, adv_train_accuracy: 94.34, clean_train_accuracy : 61.91\n",
      "[43,    61] loss: 0.18791, adv_train_accuracy: 95.31, clean_train_accuracy : 66.02\n",
      "[43,    71] loss: 0.16490, adv_train_accuracy: 93.75, clean_train_accuracy : 68.55\n",
      "[43,    81] loss: 0.16782, adv_train_accuracy: 93.75, clean_train_accuracy : 68.55\n",
      "[43,    91] loss: 0.21781, adv_train_accuracy: 92.58, clean_train_accuracy : 63.09\n",
      "0.09296875\n",
      "0.1083984375\n",
      "duration: 184 s - train loss: 0.19575 - train accuracy: 93.59 - validation loss: 0.97134 - validation accuracy: 75.88 \n",
      "[44,     1] loss: 0.16657, adv_train_accuracy: 95.12, clean_train_accuracy : 65.43\n",
      "[44,    11] loss: 0.24143, adv_train_accuracy: 93.16, clean_train_accuracy : 64.45\n",
      "[44,    21] loss: 0.19009, adv_train_accuracy: 93.16, clean_train_accuracy : 63.67\n",
      "[44,    31] loss: 0.19168, adv_train_accuracy: 93.55, clean_train_accuracy : 62.89\n",
      "[44,    41] loss: 0.12497, adv_train_accuracy: 95.90, clean_train_accuracy : 65.04\n",
      "[44,    51] loss: 0.20933, adv_train_accuracy: 93.75, clean_train_accuracy : 60.94\n",
      "[44,    61] loss: 0.19183, adv_train_accuracy: 93.75, clean_train_accuracy : 62.30\n",
      "[44,    71] loss: 0.32537, adv_train_accuracy: 91.21, clean_train_accuracy : 61.52\n",
      "[44,    81] loss: 0.15193, adv_train_accuracy: 94.92, clean_train_accuracy : 66.02\n",
      "[44,    91] loss: 0.19436, adv_train_accuracy: 91.60, clean_train_accuracy : 64.65\n",
      "0.0560546875\n",
      "0.1044921875\n",
      "duration: 185 s - train loss: 0.20558 - train accuracy: 93.38 - validation loss: 1.04464 - validation accuracy: 77.53 \n",
      "[45,     1] loss: 0.20770, adv_train_accuracy: 93.16, clean_train_accuracy : 65.82\n",
      "[45,    11] loss: 0.20335, adv_train_accuracy: 92.38, clean_train_accuracy : 63.48\n",
      "[45,    21] loss: 0.21636, adv_train_accuracy: 92.58, clean_train_accuracy : 66.80\n",
      "[45,    31] loss: 0.26313, adv_train_accuracy: 91.99, clean_train_accuracy : 66.41\n",
      "[45,    41] loss: 0.26196, adv_train_accuracy: 91.80, clean_train_accuracy : 62.50\n",
      "[45,    51] loss: 0.15575, adv_train_accuracy: 94.53, clean_train_accuracy : 63.87\n",
      "[45,    61] loss: 0.22870, adv_train_accuracy: 92.38, clean_train_accuracy : 62.89\n",
      "[45,    71] loss: 0.22019, adv_train_accuracy: 93.55, clean_train_accuracy : 65.43\n",
      "[45,    81] loss: 0.21807, adv_train_accuracy: 92.19, clean_train_accuracy : 59.96\n",
      "[45,    91] loss: 0.17943, adv_train_accuracy: 93.55, clean_train_accuracy : 62.89\n",
      "0.0318359375\n",
      "0.09296875\n",
      "duration: 185 s - train loss: 0.21034 - train accuracy: 93.11 - validation loss: 1.03387 - validation accuracy: 77.24 \n",
      "[46,     1] loss: 0.25857, adv_train_accuracy: 91.80, clean_train_accuracy : 64.26\n",
      "[46,    11] loss: 0.21323, adv_train_accuracy: 92.38, clean_train_accuracy : 62.50\n",
      "[46,    21] loss: 0.24819, adv_train_accuracy: 92.19, clean_train_accuracy : 61.91\n",
      "[46,    31] loss: 0.20393, adv_train_accuracy: 93.16, clean_train_accuracy : 64.06\n",
      "[46,    41] loss: 0.22536, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[46,    51] loss: 0.16258, adv_train_accuracy: 94.34, clean_train_accuracy : 64.26\n",
      "[46,    61] loss: 0.16064, adv_train_accuracy: 94.73, clean_train_accuracy : 64.45\n",
      "[46,    71] loss: 0.14811, adv_train_accuracy: 95.31, clean_train_accuracy : 65.82\n",
      "[46,    81] loss: 0.20987, adv_train_accuracy: 92.77, clean_train_accuracy : 61.52\n",
      "[46,    91] loss: 0.21285, adv_train_accuracy: 92.77, clean_train_accuracy : 65.82\n",
      "0.1267578125\n",
      "0.1560546875\n",
      "duration: 185 s - train loss: 0.19663 - train accuracy: 93.39 - validation loss: 1.24395 - validation accuracy: 75.23 \n",
      "[47,     1] loss: 0.19691, adv_train_accuracy: 93.16, clean_train_accuracy : 64.06\n",
      "[47,    11] loss: 0.13314, adv_train_accuracy: 97.07, clean_train_accuracy : 66.41\n",
      "[47,    21] loss: 0.23974, adv_train_accuracy: 92.77, clean_train_accuracy : 58.59\n",
      "[47,    31] loss: 0.20968, adv_train_accuracy: 92.19, clean_train_accuracy : 62.30\n",
      "[47,    41] loss: 0.23755, adv_train_accuracy: 91.41, clean_train_accuracy : 58.01\n",
      "[47,    51] loss: 0.11919, adv_train_accuracy: 95.31, clean_train_accuracy : 65.62\n",
      "[47,    61] loss: 0.18240, adv_train_accuracy: 94.34, clean_train_accuracy : 68.55\n",
      "[47,    71] loss: 0.23047, adv_train_accuracy: 93.55, clean_train_accuracy : 63.28\n",
      "[47,    81] loss: 0.19402, adv_train_accuracy: 93.95, clean_train_accuracy : 61.91\n",
      "[47,    91] loss: 0.21551, adv_train_accuracy: 92.38, clean_train_accuracy : 65.04\n",
      "0.030859375\n",
      "0.094921875\n",
      "duration: 185 s - train loss: 0.17428 - train accuracy: 94.25 - validation loss: 1.01358 - validation accuracy: 77.77 \n",
      "[48,     1] loss: 0.14218, adv_train_accuracy: 95.12, clean_train_accuracy : 67.38\n",
      "[48,    11] loss: 0.15032, adv_train_accuracy: 95.51, clean_train_accuracy : 69.73\n",
      "[48,    21] loss: 0.14294, adv_train_accuracy: 94.73, clean_train_accuracy : 70.70\n",
      "[48,    31] loss: 0.14402, adv_train_accuracy: 95.70, clean_train_accuracy : 64.45\n",
      "[48,    41] loss: 0.15340, adv_train_accuracy: 95.31, clean_train_accuracy : 68.16\n",
      "[48,    51] loss: 0.14294, adv_train_accuracy: 95.31, clean_train_accuracy : 65.82\n",
      "[48,    61] loss: 0.21313, adv_train_accuracy: 93.16, clean_train_accuracy : 66.41\n",
      "[48,    71] loss: 0.12353, adv_train_accuracy: 95.51, clean_train_accuracy : 66.80\n",
      "[48,    81] loss: 0.16108, adv_train_accuracy: 94.34, clean_train_accuracy : 66.99\n",
      "[48,    91] loss: 0.15036, adv_train_accuracy: 94.73, clean_train_accuracy : 64.84\n",
      "0.020703125\n",
      "0.0845703125\n",
      "duration: 185 s - train loss: 0.16401 - train accuracy: 94.58 - validation loss: 0.96627 - validation accuracy: 78.64 \n",
      "[49,     1] loss: 0.18194, adv_train_accuracy: 93.36, clean_train_accuracy : 65.62\n",
      "[49,    11] loss: 0.16015, adv_train_accuracy: 95.70, clean_train_accuracy : 64.26\n",
      "[49,    21] loss: 0.21657, adv_train_accuracy: 93.55, clean_train_accuracy : 62.89\n",
      "[49,    31] loss: 0.15346, adv_train_accuracy: 95.70, clean_train_accuracy : 66.60\n",
      "[49,    41] loss: 0.15530, adv_train_accuracy: 95.31, clean_train_accuracy : 62.50\n",
      "[49,    51] loss: 0.21754, adv_train_accuracy: 92.58, clean_train_accuracy : 66.02\n",
      "[49,    61] loss: 0.18706, adv_train_accuracy: 93.16, clean_train_accuracy : 66.60\n",
      "[49,    71] loss: 0.16542, adv_train_accuracy: 93.95, clean_train_accuracy : 65.23\n",
      "[49,    81] loss: 0.16211, adv_train_accuracy: 94.92, clean_train_accuracy : 63.87\n",
      "[49,    91] loss: 0.22972, adv_train_accuracy: 92.38, clean_train_accuracy : 65.23\n",
      "0.029296875\n",
      "0.0857421875\n",
      "duration: 184 s - train loss: 0.17588 - train accuracy: 94.21 - validation loss: 1.01677 - validation accuracy: 77.31 \n",
      "[50,     1] loss: 0.15102, adv_train_accuracy: 95.90, clean_train_accuracy : 63.87\n",
      "[50,    11] loss: 0.17406, adv_train_accuracy: 93.75, clean_train_accuracy : 65.04\n",
      "[50,    21] loss: 0.17147, adv_train_accuracy: 94.73, clean_train_accuracy : 68.95\n",
      "[50,    31] loss: 0.17368, adv_train_accuracy: 93.75, clean_train_accuracy : 64.84\n",
      "[50,    41] loss: 0.24002, adv_train_accuracy: 90.23, clean_train_accuracy : 63.28\n",
      "[50,    51] loss: 0.17888, adv_train_accuracy: 94.53, clean_train_accuracy : 62.70\n",
      "[50,    61] loss: 0.11655, adv_train_accuracy: 96.29, clean_train_accuracy : 71.09\n",
      "[50,    71] loss: 0.14920, adv_train_accuracy: 94.73, clean_train_accuracy : 63.28\n",
      "[50,    81] loss: 0.24732, adv_train_accuracy: 91.41, clean_train_accuracy : 64.26\n",
      "[50,    91] loss: 0.14769, adv_train_accuracy: 95.31, clean_train_accuracy : 66.41\n",
      "0.044921875\n",
      "0.1111328125\n",
      "duration: 184 s - train loss: 0.16645 - train accuracy: 94.59 - validation loss: 1.05839 - validation accuracy: 75.11 \n",
      "[51,     1] loss: 0.16352, adv_train_accuracy: 94.73, clean_train_accuracy : 64.26\n",
      "[51,    11] loss: 0.19972, adv_train_accuracy: 93.16, clean_train_accuracy : 66.60\n",
      "[51,    21] loss: 0.18710, adv_train_accuracy: 94.73, clean_train_accuracy : 66.80\n",
      "[51,    31] loss: 0.12794, adv_train_accuracy: 95.31, clean_train_accuracy : 71.29\n",
      "[51,    41] loss: 0.21128, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[51,    51] loss: 0.12315, adv_train_accuracy: 96.29, clean_train_accuracy : 69.92\n",
      "[51,    61] loss: 0.13846, adv_train_accuracy: 96.29, clean_train_accuracy : 65.82\n",
      "[51,    71] loss: 0.17017, adv_train_accuracy: 95.51, clean_train_accuracy : 65.82\n",
      "[51,    81] loss: 0.12318, adv_train_accuracy: 96.29, clean_train_accuracy : 62.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51,    91] loss: 0.12257, adv_train_accuracy: 95.31, clean_train_accuracy : 67.38\n",
      "0.037890625\n",
      "0.083984375\n",
      "duration: 185 s - train loss: 0.15676 - train accuracy: 94.89 - validation loss: 1.22410 - validation accuracy: 75.44 \n",
      "[52,     1] loss: 0.15283, adv_train_accuracy: 94.92, clean_train_accuracy : 64.45\n",
      "[52,    11] loss: 0.18559, adv_train_accuracy: 94.53, clean_train_accuracy : 64.84\n",
      "[52,    21] loss: 0.17312, adv_train_accuracy: 94.34, clean_train_accuracy : 65.62\n",
      "[52,    31] loss: 0.13718, adv_train_accuracy: 94.92, clean_train_accuracy : 67.77\n",
      "[52,    41] loss: 0.20693, adv_train_accuracy: 93.16, clean_train_accuracy : 62.89\n",
      "[52,    51] loss: 0.14939, adv_train_accuracy: 95.51, clean_train_accuracy : 64.65\n",
      "[52,    61] loss: 0.24600, adv_train_accuracy: 92.58, clean_train_accuracy : 69.73\n",
      "[52,    71] loss: 0.23514, adv_train_accuracy: 94.34, clean_train_accuracy : 64.84\n",
      "[52,    81] loss: 0.15772, adv_train_accuracy: 94.34, clean_train_accuracy : 64.06\n",
      "[52,    91] loss: 0.17291, adv_train_accuracy: 93.16, clean_train_accuracy : 69.14\n",
      "0.0443359375\n",
      "0.0875\n",
      "duration: 185 s - train loss: 0.16530 - train accuracy: 94.50 - validation loss: 0.94030 - validation accuracy: 77.71 \n",
      "[53,     1] loss: 0.14611, adv_train_accuracy: 95.70, clean_train_accuracy : 67.77\n",
      "[53,    11] loss: 0.15679, adv_train_accuracy: 94.53, clean_train_accuracy : 64.26\n",
      "[53,    21] loss: 0.14239, adv_train_accuracy: 96.48, clean_train_accuracy : 64.26\n",
      "[53,    31] loss: 0.15024, adv_train_accuracy: 94.92, clean_train_accuracy : 66.02\n",
      "[53,    41] loss: 0.11918, adv_train_accuracy: 96.48, clean_train_accuracy : 66.60\n",
      "[53,    51] loss: 0.17521, adv_train_accuracy: 93.95, clean_train_accuracy : 69.73\n",
      "[53,    61] loss: 0.20010, adv_train_accuracy: 94.34, clean_train_accuracy : 65.43\n",
      "[53,    71] loss: 0.17985, adv_train_accuracy: 93.55, clean_train_accuracy : 64.45\n",
      "[53,    81] loss: 0.15309, adv_train_accuracy: 95.51, clean_train_accuracy : 60.94\n",
      "[53,    91] loss: 0.13733, adv_train_accuracy: 96.09, clean_train_accuracy : 66.80\n",
      "0.0466796875\n",
      "0.1025390625\n",
      "duration: 192 s - train loss: 0.16713 - train accuracy: 94.50 - validation loss: 1.20755 - validation accuracy: 75.19 \n",
      "[54,     1] loss: 0.18252, adv_train_accuracy: 93.95, clean_train_accuracy : 63.87\n",
      "[54,    11] loss: 0.18854, adv_train_accuracy: 93.16, clean_train_accuracy : 69.34\n",
      "[54,    21] loss: 0.15903, adv_train_accuracy: 94.73, clean_train_accuracy : 62.11\n",
      "[54,    31] loss: 0.20481, adv_train_accuracy: 93.55, clean_train_accuracy : 68.16\n",
      "[54,    41] loss: 0.18734, adv_train_accuracy: 94.73, clean_train_accuracy : 67.19\n",
      "[54,    51] loss: 0.17700, adv_train_accuracy: 93.95, clean_train_accuracy : 65.62\n",
      "[54,    61] loss: 0.16066, adv_train_accuracy: 94.92, clean_train_accuracy : 64.45\n",
      "[54,    71] loss: 0.18547, adv_train_accuracy: 94.53, clean_train_accuracy : 62.89\n",
      "[54,    81] loss: 0.13984, adv_train_accuracy: 96.29, clean_train_accuracy : 68.16\n",
      "[54,    91] loss: 0.15474, adv_train_accuracy: 95.12, clean_train_accuracy : 67.97\n",
      "0.0193359375\n",
      "0.0736328125\n",
      "duration: 187 s - train loss: 0.17068 - train accuracy: 94.46 - validation loss: 1.06483 - validation accuracy: 77.77 \n",
      "[55,     1] loss: 0.12216, adv_train_accuracy: 95.31, clean_train_accuracy : 66.21\n",
      "[55,    11] loss: 0.20493, adv_train_accuracy: 93.75, clean_train_accuracy : 66.99\n",
      "[55,    21] loss: 0.16887, adv_train_accuracy: 94.53, clean_train_accuracy : 64.06\n",
      "[55,    31] loss: 0.09143, adv_train_accuracy: 97.27, clean_train_accuracy : 65.04\n",
      "[55,    41] loss: 0.14068, adv_train_accuracy: 95.51, clean_train_accuracy : 67.58\n",
      "[55,    51] loss: 0.13540, adv_train_accuracy: 94.92, clean_train_accuracy : 66.80\n",
      "[55,    61] loss: 0.10594, adv_train_accuracy: 96.68, clean_train_accuracy : 63.48\n",
      "[55,    71] loss: 0.14952, adv_train_accuracy: 93.95, clean_train_accuracy : 64.45\n",
      "[55,    81] loss: 0.16099, adv_train_accuracy: 95.12, clean_train_accuracy : 64.84\n",
      "[55,    91] loss: 0.14863, adv_train_accuracy: 95.31, clean_train_accuracy : 66.02\n",
      "0.041015625\n",
      "0.076953125\n",
      "duration: 185 s - train loss: 0.14551 - train accuracy: 95.29 - validation loss: 1.06735 - validation accuracy: 78.21 \n",
      "[56,     1] loss: 0.13473, adv_train_accuracy: 94.92, clean_train_accuracy : 66.02\n",
      "[56,    11] loss: 0.12520, adv_train_accuracy: 96.09, clean_train_accuracy : 63.67\n",
      "[56,    21] loss: 0.16219, adv_train_accuracy: 95.31, clean_train_accuracy : 69.14\n",
      "[56,    31] loss: 0.17663, adv_train_accuracy: 93.95, clean_train_accuracy : 64.26\n",
      "[56,    41] loss: 0.21521, adv_train_accuracy: 92.58, clean_train_accuracy : 62.70\n",
      "[56,    51] loss: 0.06912, adv_train_accuracy: 97.85, clean_train_accuracy : 70.12\n",
      "[56,    61] loss: 0.24951, adv_train_accuracy: 91.60, clean_train_accuracy : 65.23\n",
      "[56,    71] loss: 0.12339, adv_train_accuracy: 97.07, clean_train_accuracy : 61.91\n",
      "[56,    81] loss: 0.11833, adv_train_accuracy: 96.29, clean_train_accuracy : 66.02\n",
      "[56,    91] loss: 0.15505, adv_train_accuracy: 95.12, clean_train_accuracy : 67.38\n",
      "0.0509765625\n",
      "0.1076171875\n",
      "duration: 186 s - train loss: 0.16157 - train accuracy: 94.87 - validation loss: 1.19370 - validation accuracy: 76.77 \n",
      "[57,     1] loss: 0.15632, adv_train_accuracy: 94.53, clean_train_accuracy : 69.34\n",
      "[57,    11] loss: 0.16166, adv_train_accuracy: 95.12, clean_train_accuracy : 68.95\n",
      "[57,    21] loss: 0.17578, adv_train_accuracy: 94.14, clean_train_accuracy : 69.34\n",
      "[57,    31] loss: 0.15639, adv_train_accuracy: 95.31, clean_train_accuracy : 66.21\n",
      "[57,    41] loss: 0.14446, adv_train_accuracy: 94.34, clean_train_accuracy : 66.60\n",
      "[57,    51] loss: 0.18151, adv_train_accuracy: 94.34, clean_train_accuracy : 68.55\n",
      "[57,    61] loss: 0.12317, adv_train_accuracy: 96.68, clean_train_accuracy : 63.28\n",
      "[57,    71] loss: 0.23160, adv_train_accuracy: 92.58, clean_train_accuracy : 63.28\n",
      "[57,    81] loss: 0.20923, adv_train_accuracy: 93.16, clean_train_accuracy : 64.06\n",
      "[57,    91] loss: 0.20985, adv_train_accuracy: 93.75, clean_train_accuracy : 69.73\n",
      "0.043359375\n",
      "0.0888671875\n",
      "duration: 192 s - train loss: 0.17343 - train accuracy: 94.35 - validation loss: 0.96080 - validation accuracy: 77.87 \n",
      "[58,     1] loss: 0.15563, adv_train_accuracy: 94.92, clean_train_accuracy : 66.41\n",
      "[58,    11] loss: 0.19246, adv_train_accuracy: 94.14, clean_train_accuracy : 65.04\n",
      "[58,    21] loss: 0.22409, adv_train_accuracy: 92.77, clean_train_accuracy : 64.26\n",
      "[58,    31] loss: 0.11220, adv_train_accuracy: 95.90, clean_train_accuracy : 69.73\n",
      "[58,    41] loss: 0.13896, adv_train_accuracy: 95.12, clean_train_accuracy : 64.06\n",
      "[58,    51] loss: 0.16360, adv_train_accuracy: 95.31, clean_train_accuracy : 63.67\n",
      "[58,    61] loss: 0.17126, adv_train_accuracy: 95.90, clean_train_accuracy : 64.26\n",
      "[58,    71] loss: 0.14040, adv_train_accuracy: 95.70, clean_train_accuracy : 66.99\n",
      "[58,    81] loss: 0.12315, adv_train_accuracy: 96.68, clean_train_accuracy : 65.23\n",
      "[58,    91] loss: 0.13054, adv_train_accuracy: 95.12, clean_train_accuracy : 64.84\n",
      "0.0203125\n",
      "0.072265625\n",
      "duration: 192 s - train loss: 0.15461 - train accuracy: 95.03 - validation loss: 1.08148 - validation accuracy: 77.88 \n",
      "[59,     1] loss: 0.10082, adv_train_accuracy: 96.88, clean_train_accuracy : 67.19\n",
      "[59,    11] loss: 0.15648, adv_train_accuracy: 93.95, clean_train_accuracy : 67.19\n",
      "[59,    21] loss: 0.16954, adv_train_accuracy: 94.73, clean_train_accuracy : 63.67\n",
      "[59,    31] loss: 0.17988, adv_train_accuracy: 94.14, clean_train_accuracy : 66.02\n",
      "[59,    41] loss: 0.16576, adv_train_accuracy: 94.92, clean_train_accuracy : 66.60\n",
      "[59,    51] loss: 0.15180, adv_train_accuracy: 94.53, clean_train_accuracy : 65.82\n",
      "[59,    61] loss: 0.14428, adv_train_accuracy: 95.12, clean_train_accuracy : 68.75\n",
      "[59,    71] loss: 0.09801, adv_train_accuracy: 96.48, clean_train_accuracy : 65.62\n",
      "[59,    81] loss: 0.16531, adv_train_accuracy: 93.16, clean_train_accuracy : 67.19\n",
      "[59,    91] loss: 0.12960, adv_train_accuracy: 95.12, clean_train_accuracy : 68.95\n",
      "0.0271484375\n",
      "0.0892578125\n",
      "duration: 192 s - train loss: 0.14946 - train accuracy: 95.05 - validation loss: 1.09152 - validation accuracy: 77.36 \n",
      "[60,     1] loss: 0.12095, adv_train_accuracy: 95.51, clean_train_accuracy : 68.16\n",
      "[60,    11] loss: 0.12933, adv_train_accuracy: 95.31, clean_train_accuracy : 66.60\n",
      "[60,    21] loss: 0.17260, adv_train_accuracy: 93.36, clean_train_accuracy : 66.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60,    31] loss: 0.15598, adv_train_accuracy: 93.95, clean_train_accuracy : 63.28\n",
      "[60,    41] loss: 0.16389, adv_train_accuracy: 94.34, clean_train_accuracy : 65.82\n",
      "[60,    51] loss: 0.12185, adv_train_accuracy: 96.88, clean_train_accuracy : 66.99\n",
      "[60,    61] loss: 0.13018, adv_train_accuracy: 95.51, clean_train_accuracy : 65.43\n",
      "[60,    71] loss: 0.17068, adv_train_accuracy: 93.75, clean_train_accuracy : 66.99\n",
      "[60,    81] loss: 0.15011, adv_train_accuracy: 93.36, clean_train_accuracy : 67.97\n",
      "[60,    91] loss: 0.17547, adv_train_accuracy: 95.51, clean_train_accuracy : 67.38\n",
      "0.0314453125\n",
      "0.1017578125\n",
      "duration: 192 s - train loss: 0.14656 - train accuracy: 95.18 - validation loss: 1.01090 - validation accuracy: 77.72 \n",
      "[61,     1] loss: 0.12340, adv_train_accuracy: 95.70, clean_train_accuracy : 69.53\n",
      "[61,    11] loss: 0.17675, adv_train_accuracy: 93.16, clean_train_accuracy : 66.02\n",
      "[61,    21] loss: 0.15222, adv_train_accuracy: 95.12, clean_train_accuracy : 70.70\n",
      "[61,    31] loss: 0.12767, adv_train_accuracy: 95.31, clean_train_accuracy : 68.36\n",
      "[61,    41] loss: 0.14474, adv_train_accuracy: 94.92, clean_train_accuracy : 62.30\n",
      "[61,    51] loss: 0.13768, adv_train_accuracy: 95.70, clean_train_accuracy : 66.80\n",
      "[61,    61] loss: 0.11095, adv_train_accuracy: 95.70, clean_train_accuracy : 63.87\n",
      "[61,    71] loss: 0.14872, adv_train_accuracy: 95.12, clean_train_accuracy : 67.97\n",
      "[61,    81] loss: 0.14791, adv_train_accuracy: 95.31, clean_train_accuracy : 63.28\n",
      "[61,    91] loss: 0.11152, adv_train_accuracy: 95.70, clean_train_accuracy : 66.80\n",
      "0.0255859375\n",
      "0.0736328125\n",
      "duration: 193 s - train loss: 0.14365 - train accuracy: 95.24 - validation loss: 0.98147 - validation accuracy: 78.31 \n",
      "[62,     1] loss: 0.13414, adv_train_accuracy: 94.92, clean_train_accuracy : 63.48\n",
      "[62,    11] loss: 0.14449, adv_train_accuracy: 95.70, clean_train_accuracy : 65.82\n",
      "[62,    21] loss: 0.12417, adv_train_accuracy: 96.68, clean_train_accuracy : 69.14\n",
      "[62,    31] loss: 0.14828, adv_train_accuracy: 95.70, clean_train_accuracy : 64.06\n",
      "[62,    41] loss: 0.17334, adv_train_accuracy: 94.73, clean_train_accuracy : 65.04\n",
      "[62,    51] loss: 0.15121, adv_train_accuracy: 94.92, clean_train_accuracy : 68.36\n",
      "[62,    61] loss: 0.14872, adv_train_accuracy: 95.51, clean_train_accuracy : 66.21\n",
      "[62,    71] loss: 0.11167, adv_train_accuracy: 96.09, clean_train_accuracy : 71.09\n",
      "[62,    81] loss: 0.13320, adv_train_accuracy: 95.31, clean_train_accuracy : 67.58\n",
      "[62,    91] loss: 0.10921, adv_train_accuracy: 95.90, clean_train_accuracy : 66.99\n",
      "0.0234375\n",
      "0.07734375\n",
      "duration: 192 s - train loss: 0.13628 - train accuracy: 95.50 - validation loss: 1.00660 - validation accuracy: 77.50 \n",
      "[63,     1] loss: 0.15234, adv_train_accuracy: 93.55, clean_train_accuracy : 65.62\n",
      "[63,    11] loss: 0.15681, adv_train_accuracy: 95.12, clean_train_accuracy : 67.38\n",
      "[63,    21] loss: 0.10495, adv_train_accuracy: 96.48, clean_train_accuracy : 72.66\n",
      "[63,    31] loss: 0.14420, adv_train_accuracy: 94.92, clean_train_accuracy : 64.65\n",
      "[63,    41] loss: 0.14746, adv_train_accuracy: 94.53, clean_train_accuracy : 69.92\n",
      "[63,    51] loss: 0.13774, adv_train_accuracy: 95.12, clean_train_accuracy : 67.97\n",
      "[63,    61] loss: 0.24479, adv_train_accuracy: 92.19, clean_train_accuracy : 66.21\n",
      "[63,    71] loss: 0.16083, adv_train_accuracy: 93.95, clean_train_accuracy : 69.34\n",
      "[63,    81] loss: 0.15228, adv_train_accuracy: 94.53, clean_train_accuracy : 64.45\n",
      "[63,    91] loss: 0.11967, adv_train_accuracy: 96.48, clean_train_accuracy : 64.26\n",
      "0.02734375\n",
      "0.0810546875\n",
      "duration: 192 s - train loss: 0.15064 - train accuracy: 94.99 - validation loss: 1.03778 - validation accuracy: 78.48 \n",
      "[64,     1] loss: 0.19846, adv_train_accuracy: 92.77, clean_train_accuracy : 64.45\n",
      "[64,    11] loss: 0.11337, adv_train_accuracy: 95.70, clean_train_accuracy : 66.02\n",
      "[64,    21] loss: 0.17058, adv_train_accuracy: 94.73, clean_train_accuracy : 68.16\n",
      "[64,    31] loss: 0.15977, adv_train_accuracy: 94.73, clean_train_accuracy : 66.02\n",
      "[64,    41] loss: 0.14319, adv_train_accuracy: 95.31, clean_train_accuracy : 68.75\n",
      "[64,    51] loss: 0.13542, adv_train_accuracy: 94.53, clean_train_accuracy : 65.43\n",
      "[64,    61] loss: 0.10599, adv_train_accuracy: 95.12, clean_train_accuracy : 66.80\n",
      "[64,    71] loss: 0.14197, adv_train_accuracy: 96.09, clean_train_accuracy : 66.80\n",
      "[64,    81] loss: 0.11967, adv_train_accuracy: 95.51, clean_train_accuracy : 63.48\n",
      "[64,    91] loss: 0.11843, adv_train_accuracy: 96.48, clean_train_accuracy : 70.70\n",
      "0.02421875\n",
      "0.0673828125\n",
      "duration: 192 s - train loss: 0.13409 - train accuracy: 95.51 - validation loss: 1.07369 - validation accuracy: 78.75 \n",
      "[65,     1] loss: 0.12594, adv_train_accuracy: 96.29, clean_train_accuracy : 67.77\n",
      "[65,    11] loss: 0.15232, adv_train_accuracy: 95.51, clean_train_accuracy : 71.29\n",
      "[65,    21] loss: 0.17938, adv_train_accuracy: 93.95, clean_train_accuracy : 66.21\n",
      "[65,    31] loss: 0.13910, adv_train_accuracy: 95.51, clean_train_accuracy : 62.89\n",
      "[65,    41] loss: 0.08361, adv_train_accuracy: 97.07, clean_train_accuracy : 66.99\n",
      "[65,    51] loss: 0.13090, adv_train_accuracy: 94.73, clean_train_accuracy : 67.38\n",
      "[65,    61] loss: 0.11968, adv_train_accuracy: 95.90, clean_train_accuracy : 65.62\n",
      "[65,    71] loss: 0.13853, adv_train_accuracy: 96.48, clean_train_accuracy : 69.34\n",
      "[65,    81] loss: 0.15977, adv_train_accuracy: 95.31, clean_train_accuracy : 67.38\n",
      "[65,    91] loss: 0.14418, adv_train_accuracy: 95.51, clean_train_accuracy : 69.53\n",
      "0.0400390625\n",
      "0.0970703125\n",
      "duration: 192 s - train loss: 0.14535 - train accuracy: 95.25 - validation loss: 0.99360 - validation accuracy: 78.71 \n",
      "[66,     1] loss: 0.10838, adv_train_accuracy: 95.70, clean_train_accuracy : 71.68\n",
      "[66,    11] loss: 0.11018, adv_train_accuracy: 96.68, clean_train_accuracy : 69.73\n",
      "[66,    21] loss: 0.18251, adv_train_accuracy: 94.53, clean_train_accuracy : 67.58\n",
      "[66,    31] loss: 0.15379, adv_train_accuracy: 95.12, clean_train_accuracy : 65.04\n",
      "[66,    41] loss: 0.19069, adv_train_accuracy: 93.55, clean_train_accuracy : 65.82\n",
      "[66,    51] loss: 0.14081, adv_train_accuracy: 95.51, clean_train_accuracy : 69.73\n",
      "[66,    61] loss: 0.12890, adv_train_accuracy: 95.51, clean_train_accuracy : 68.55\n",
      "[66,    71] loss: 0.18184, adv_train_accuracy: 94.73, clean_train_accuracy : 69.73\n",
      "[66,    81] loss: 0.15789, adv_train_accuracy: 94.34, clean_train_accuracy : 68.75\n",
      "[66,    91] loss: 0.18698, adv_train_accuracy: 93.75, clean_train_accuracy : 63.28\n",
      "0.0326171875\n",
      "0.0693359375\n",
      "duration: 192 s - train loss: 0.14656 - train accuracy: 95.16 - validation loss: 0.95564 - validation accuracy: 78.24 \n",
      "[67,     1] loss: 0.14301, adv_train_accuracy: 95.12, clean_train_accuracy : 66.99\n",
      "[67,    11] loss: 0.14548, adv_train_accuracy: 95.12, clean_train_accuracy : 64.45\n",
      "[67,    21] loss: 0.16596, adv_train_accuracy: 93.95, clean_train_accuracy : 68.95\n",
      "[67,    31] loss: 0.08970, adv_train_accuracy: 97.07, clean_train_accuracy : 69.34\n",
      "[67,    41] loss: 0.10863, adv_train_accuracy: 96.48, clean_train_accuracy : 64.45\n",
      "[67,    51] loss: 0.18851, adv_train_accuracy: 92.38, clean_train_accuracy : 68.75\n",
      "[67,    61] loss: 0.11736, adv_train_accuracy: 95.51, clean_train_accuracy : 67.97\n",
      "[67,    71] loss: 0.14276, adv_train_accuracy: 94.92, clean_train_accuracy : 66.80\n",
      "[67,    81] loss: 0.16172, adv_train_accuracy: 94.73, clean_train_accuracy : 67.77\n",
      "[67,    91] loss: 0.20432, adv_train_accuracy: 94.92, clean_train_accuracy : 66.80\n",
      "0.06015625\n",
      "0.1095703125\n",
      "duration: 192 s - train loss: 0.14692 - train accuracy: 95.12 - validation loss: 1.08824 - validation accuracy: 77.37 \n",
      "[68,     1] loss: 0.14844, adv_train_accuracy: 95.31, clean_train_accuracy : 67.97\n",
      "[68,    11] loss: 0.22189, adv_train_accuracy: 93.75, clean_train_accuracy : 66.80\n",
      "[68,    21] loss: 0.18833, adv_train_accuracy: 93.55, clean_train_accuracy : 64.65\n",
      "[68,    31] loss: 0.15418, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[68,    41] loss: 0.15656, adv_train_accuracy: 93.95, clean_train_accuracy : 65.23\n",
      "[68,    51] loss: 0.15657, adv_train_accuracy: 95.51, clean_train_accuracy : 65.43\n",
      "[68,    61] loss: 0.14142, adv_train_accuracy: 95.12, clean_train_accuracy : 66.41\n",
      "[68,    71] loss: 0.14154, adv_train_accuracy: 95.31, clean_train_accuracy : 64.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68,    81] loss: 0.15281, adv_train_accuracy: 94.34, clean_train_accuracy : 65.23\n",
      "[68,    91] loss: 0.17229, adv_train_accuracy: 94.92, clean_train_accuracy : 70.12\n",
      "0.0333984375\n",
      "0.119140625\n",
      "duration: 192 s - train loss: 0.16150 - train accuracy: 94.67 - validation loss: 1.13768 - validation accuracy: 77.21 \n",
      "[69,     1] loss: 0.21420, adv_train_accuracy: 94.14, clean_train_accuracy : 66.02\n",
      "[69,    11] loss: 0.13169, adv_train_accuracy: 94.92, clean_train_accuracy : 69.53\n",
      "[69,    21] loss: 0.23536, adv_train_accuracy: 92.77, clean_train_accuracy : 67.77\n",
      "[69,    31] loss: 0.15819, adv_train_accuracy: 94.53, clean_train_accuracy : 66.41\n",
      "[69,    41] loss: 0.18316, adv_train_accuracy: 94.34, clean_train_accuracy : 63.48\n",
      "[69,    51] loss: 0.16375, adv_train_accuracy: 95.51, clean_train_accuracy : 67.97\n",
      "[69,    61] loss: 0.19051, adv_train_accuracy: 93.16, clean_train_accuracy : 63.67\n",
      "[69,    71] loss: 0.16217, adv_train_accuracy: 93.55, clean_train_accuracy : 68.55\n",
      "[69,    81] loss: 0.21232, adv_train_accuracy: 92.97, clean_train_accuracy : 65.82\n",
      "[69,    91] loss: 0.16449, adv_train_accuracy: 94.34, clean_train_accuracy : 65.23\n",
      "0.0333984375\n",
      "0.1029296875\n",
      "duration: 192 s - train loss: 0.17313 - train accuracy: 94.24 - validation loss: 1.10748 - validation accuracy: 78.12 \n",
      "[70,     1] loss: 0.14053, adv_train_accuracy: 96.29, clean_train_accuracy : 64.65\n",
      "[70,    11] loss: 0.18759, adv_train_accuracy: 94.14, clean_train_accuracy : 67.97\n",
      "[70,    21] loss: 0.17096, adv_train_accuracy: 93.75, clean_train_accuracy : 65.43\n",
      "[70,    31] loss: 0.13722, adv_train_accuracy: 94.92, clean_train_accuracy : 69.53\n",
      "[70,    41] loss: 0.14464, adv_train_accuracy: 94.34, clean_train_accuracy : 65.04\n",
      "[70,    51] loss: 0.18666, adv_train_accuracy: 93.36, clean_train_accuracy : 64.65\n",
      "[70,    61] loss: 0.18192, adv_train_accuracy: 94.14, clean_train_accuracy : 65.62\n",
      "[70,    71] loss: 0.20993, adv_train_accuracy: 94.34, clean_train_accuracy : 67.58\n",
      "[70,    81] loss: 0.14510, adv_train_accuracy: 96.09, clean_train_accuracy : 65.82\n",
      "[70,    91] loss: 0.19521, adv_train_accuracy: 93.55, clean_train_accuracy : 66.02\n",
      "0.03671875\n",
      "0.0978515625\n",
      "duration: 193 s - train loss: 0.15769 - train accuracy: 94.75 - validation loss: 1.08389 - validation accuracy: 78.54 \n",
      "[71,     1] loss: 0.11043, adv_train_accuracy: 97.07, clean_train_accuracy : 67.97\n",
      "[71,    11] loss: 0.14816, adv_train_accuracy: 95.51, clean_train_accuracy : 67.19\n",
      "[71,    21] loss: 0.17182, adv_train_accuracy: 93.55, clean_train_accuracy : 63.87\n",
      "[71,    31] loss: 0.10608, adv_train_accuracy: 95.90, clean_train_accuracy : 70.51\n",
      "[71,    41] loss: 0.20655, adv_train_accuracy: 93.55, clean_train_accuracy : 66.02\n",
      "[71,    51] loss: 0.10512, adv_train_accuracy: 96.09, clean_train_accuracy : 68.16\n",
      "[71,    61] loss: 0.14458, adv_train_accuracy: 95.31, clean_train_accuracy : 62.70\n",
      "[71,    71] loss: 0.15585, adv_train_accuracy: 95.12, clean_train_accuracy : 68.55\n",
      "[71,    81] loss: 0.12617, adv_train_accuracy: 95.31, clean_train_accuracy : 65.43\n",
      "[71,    91] loss: 0.15160, adv_train_accuracy: 95.31, clean_train_accuracy : 69.14\n",
      "0.03359375\n",
      "0.1015625\n",
      "duration: 192 s - train loss: 0.14221 - train accuracy: 95.27 - validation loss: 1.10250 - validation accuracy: 79.11 \n",
      "[72,     1] loss: 0.13123, adv_train_accuracy: 95.70, clean_train_accuracy : 66.80\n",
      "[72,    11] loss: 0.15294, adv_train_accuracy: 95.31, clean_train_accuracy : 66.02\n",
      "[72,    21] loss: 0.14635, adv_train_accuracy: 94.92, clean_train_accuracy : 67.58\n",
      "[72,    31] loss: 0.12938, adv_train_accuracy: 95.70, clean_train_accuracy : 65.82\n",
      "[72,    41] loss: 0.19667, adv_train_accuracy: 92.97, clean_train_accuracy : 67.19\n",
      "[72,    51] loss: 0.11756, adv_train_accuracy: 95.90, clean_train_accuracy : 64.65\n",
      "[72,    61] loss: 0.11320, adv_train_accuracy: 96.09, clean_train_accuracy : 66.02\n",
      "[72,    71] loss: 0.12986, adv_train_accuracy: 94.73, clean_train_accuracy : 66.41\n",
      "[72,    81] loss: 0.15319, adv_train_accuracy: 94.92, clean_train_accuracy : 65.43\n",
      "[72,    91] loss: 0.15270, adv_train_accuracy: 95.31, clean_train_accuracy : 68.36\n",
      "0.0265625\n",
      "0.0810546875\n",
      "duration: 192 s - train loss: 0.14592 - train accuracy: 95.20 - validation loss: 1.06017 - validation accuracy: 78.42 \n",
      "[73,     1] loss: 0.15365, adv_train_accuracy: 93.95, clean_train_accuracy : 69.53\n",
      "[73,    11] loss: 0.13928, adv_train_accuracy: 95.12, clean_train_accuracy : 69.73\n",
      "[73,    21] loss: 0.14217, adv_train_accuracy: 96.09, clean_train_accuracy : 65.04\n",
      "[73,    31] loss: 0.13377, adv_train_accuracy: 95.12, clean_train_accuracy : 72.07\n",
      "[73,    41] loss: 0.12074, adv_train_accuracy: 95.70, clean_train_accuracy : 68.75\n",
      "[73,    51] loss: 0.12536, adv_train_accuracy: 95.12, clean_train_accuracy : 63.09\n",
      "[73,    61] loss: 0.13544, adv_train_accuracy: 95.70, clean_train_accuracy : 66.60\n",
      "[73,    71] loss: 0.12887, adv_train_accuracy: 96.09, clean_train_accuracy : 66.80\n",
      "[73,    81] loss: 0.13057, adv_train_accuracy: 95.51, clean_train_accuracy : 68.36\n",
      "[73,    91] loss: 0.13503, adv_train_accuracy: 95.70, clean_train_accuracy : 67.58\n",
      "0.0154296875\n",
      "0.052734375\n",
      "duration: 192 s - train loss: 0.13288 - train accuracy: 95.65 - validation loss: 1.03599 - validation accuracy: 77.41 \n",
      "[74,     1] loss: 0.11671, adv_train_accuracy: 96.88, clean_train_accuracy : 65.62\n",
      "[74,    11] loss: 0.07750, adv_train_accuracy: 97.85, clean_train_accuracy : 64.06\n",
      "[74,    21] loss: 0.12313, adv_train_accuracy: 95.70, clean_train_accuracy : 67.19\n",
      "[74,    31] loss: 0.14676, adv_train_accuracy: 95.90, clean_train_accuracy : 66.41\n",
      "[74,    41] loss: 0.11808, adv_train_accuracy: 95.70, clean_train_accuracy : 68.95\n",
      "[74,    51] loss: 0.15340, adv_train_accuracy: 94.34, clean_train_accuracy : 62.89\n",
      "[74,    61] loss: 0.11009, adv_train_accuracy: 95.70, clean_train_accuracy : 67.19\n",
      "[74,    71] loss: 0.15807, adv_train_accuracy: 94.14, clean_train_accuracy : 64.26\n",
      "[74,    81] loss: 0.18350, adv_train_accuracy: 94.92, clean_train_accuracy : 68.95\n",
      "[74,    91] loss: 0.11403, adv_train_accuracy: 96.09, clean_train_accuracy : 67.97\n",
      "0.0271484375\n",
      "0.0865234375\n",
      "duration: 192 s - train loss: 0.13928 - train accuracy: 95.49 - validation loss: 1.18604 - validation accuracy: 77.94 \n",
      "[75,     1] loss: 0.11907, adv_train_accuracy: 96.09, clean_train_accuracy : 66.60\n",
      "[75,    11] loss: 0.11994, adv_train_accuracy: 96.29, clean_train_accuracy : 65.23\n",
      "[75,    21] loss: 0.08268, adv_train_accuracy: 97.66, clean_train_accuracy : 66.80\n",
      "[75,    31] loss: 0.19198, adv_train_accuracy: 95.12, clean_train_accuracy : 69.14\n",
      "[75,    41] loss: 0.10643, adv_train_accuracy: 96.88, clean_train_accuracy : 67.38\n",
      "[75,    51] loss: 0.18850, adv_train_accuracy: 94.14, clean_train_accuracy : 68.75\n",
      "[75,    61] loss: 0.09491, adv_train_accuracy: 97.07, clean_train_accuracy : 68.75\n",
      "[75,    71] loss: 0.11351, adv_train_accuracy: 96.29, clean_train_accuracy : 67.19\n",
      "[75,    81] loss: 0.09285, adv_train_accuracy: 96.68, clean_train_accuracy : 69.73\n",
      "[75,    91] loss: 0.18900, adv_train_accuracy: 94.53, clean_train_accuracy : 66.02\n",
      "0.0310546875\n",
      "0.092578125\n",
      "duration: 192 s - train loss: 0.12797 - train accuracy: 95.95 - validation loss: 1.05223 - validation accuracy: 77.29 \n",
      "[76,     1] loss: 0.10513, adv_train_accuracy: 96.48, clean_train_accuracy : 68.16\n",
      "[76,    11] loss: 0.12546, adv_train_accuracy: 94.92, clean_train_accuracy : 66.99\n",
      "[76,    21] loss: 0.10903, adv_train_accuracy: 95.31, clean_train_accuracy : 68.16\n",
      "[76,    31] loss: 0.13750, adv_train_accuracy: 95.12, clean_train_accuracy : 66.80\n",
      "[76,    41] loss: 0.16016, adv_train_accuracy: 94.34, clean_train_accuracy : 63.48\n",
      "[76,    51] loss: 0.10246, adv_train_accuracy: 97.27, clean_train_accuracy : 68.55\n",
      "[76,    61] loss: 0.18138, adv_train_accuracy: 94.14, clean_train_accuracy : 65.23\n",
      "[76,    71] loss: 0.13182, adv_train_accuracy: 95.31, clean_train_accuracy : 65.43\n",
      "[76,    81] loss: 0.12183, adv_train_accuracy: 95.31, clean_train_accuracy : 69.34\n",
      "[76,    91] loss: 0.12777, adv_train_accuracy: 96.09, clean_train_accuracy : 68.36\n",
      "0.0224609375\n",
      "0.0841796875\n",
      "duration: 192 s - train loss: 0.13065 - train accuracy: 95.63 - validation loss: 1.29434 - validation accuracy: 76.47 \n",
      "[77,     1] loss: 0.22298, adv_train_accuracy: 93.16, clean_train_accuracy : 68.95\n",
      "[77,    11] loss: 0.29486, adv_train_accuracy: 93.55, clean_train_accuracy : 59.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77,    21] loss: 0.12347, adv_train_accuracy: 95.90, clean_train_accuracy : 68.55\n",
      "[77,    31] loss: 0.17013, adv_train_accuracy: 95.31, clean_train_accuracy : 67.19\n",
      "[77,    41] loss: 0.14210, adv_train_accuracy: 94.73, clean_train_accuracy : 62.11\n",
      "[77,    51] loss: 0.13495, adv_train_accuracy: 94.92, clean_train_accuracy : 65.62\n",
      "[77,    61] loss: 0.13966, adv_train_accuracy: 94.73, clean_train_accuracy : 67.38\n",
      "[77,    71] loss: 0.19890, adv_train_accuracy: 93.75, clean_train_accuracy : 65.62\n",
      "[77,    81] loss: 0.08717, adv_train_accuracy: 97.27, clean_train_accuracy : 68.75\n",
      "[77,    91] loss: 0.11449, adv_train_accuracy: 96.48, clean_train_accuracy : 66.99\n",
      "0.0189453125\n",
      "0.064453125\n",
      "duration: 192 s - train loss: 0.14366 - train accuracy: 95.30 - validation loss: 1.02123 - validation accuracy: 78.42 \n",
      "[78,     1] loss: 0.18957, adv_train_accuracy: 94.53, clean_train_accuracy : 68.16\n",
      "[78,    11] loss: 0.10030, adv_train_accuracy: 96.88, clean_train_accuracy : 66.02\n",
      "[78,    21] loss: 0.13036, adv_train_accuracy: 95.51, clean_train_accuracy : 63.87\n",
      "[78,    31] loss: 0.09486, adv_train_accuracy: 96.68, clean_train_accuracy : 67.19\n",
      "[78,    41] loss: 0.15116, adv_train_accuracy: 93.95, clean_train_accuracy : 63.67\n",
      "[78,    51] loss: 0.11422, adv_train_accuracy: 97.07, clean_train_accuracy : 70.12\n",
      "[78,    61] loss: 0.13038, adv_train_accuracy: 95.51, clean_train_accuracy : 63.48\n",
      "[78,    71] loss: 0.10208, adv_train_accuracy: 96.29, clean_train_accuracy : 67.58\n",
      "[78,    81] loss: 0.15075, adv_train_accuracy: 95.31, clean_train_accuracy : 70.70\n",
      "[78,    91] loss: 0.12119, adv_train_accuracy: 95.90, clean_train_accuracy : 66.80\n",
      "0.012890625\n",
      "0.0505859375\n",
      "duration: 192 s - train loss: 0.11976 - train accuracy: 96.11 - validation loss: 1.00488 - validation accuracy: 78.07 \n",
      "[79,     1] loss: 0.12593, adv_train_accuracy: 95.51, clean_train_accuracy : 65.82\n",
      "[79,    11] loss: 0.10023, adv_train_accuracy: 96.09, clean_train_accuracy : 71.29\n",
      "[79,    21] loss: 0.11477, adv_train_accuracy: 96.48, clean_train_accuracy : 66.60\n",
      "[79,    31] loss: 0.10009, adv_train_accuracy: 96.68, clean_train_accuracy : 70.51\n",
      "[79,    41] loss: 0.06968, adv_train_accuracy: 98.24, clean_train_accuracy : 70.31\n",
      "[79,    51] loss: 0.13436, adv_train_accuracy: 94.53, clean_train_accuracy : 67.77\n",
      "[79,    61] loss: 0.08021, adv_train_accuracy: 97.66, clean_train_accuracy : 69.92\n",
      "[79,    71] loss: 0.15281, adv_train_accuracy: 94.92, clean_train_accuracy : 68.36\n",
      "[79,    81] loss: 0.10718, adv_train_accuracy: 96.09, clean_train_accuracy : 67.19\n",
      "[79,    91] loss: 0.13508, adv_train_accuracy: 95.70, clean_train_accuracy : 68.36\n",
      "0.021484375\n",
      "0.0580078125\n",
      "duration: 192 s - train loss: 0.12082 - train accuracy: 96.11 - validation loss: 1.05472 - validation accuracy: 78.67 \n",
      "[80,     1] loss: 0.08077, adv_train_accuracy: 97.07, clean_train_accuracy : 70.51\n",
      "[80,    11] loss: 0.10062, adv_train_accuracy: 96.48, clean_train_accuracy : 66.21\n",
      "[80,    21] loss: 0.13757, adv_train_accuracy: 96.09, clean_train_accuracy : 69.34\n",
      "[80,    31] loss: 0.15607, adv_train_accuracy: 94.73, clean_train_accuracy : 64.84\n",
      "[80,    41] loss: 0.10102, adv_train_accuracy: 96.09, clean_train_accuracy : 69.73\n",
      "[80,    51] loss: 0.09127, adv_train_accuracy: 96.88, clean_train_accuracy : 67.58\n",
      "[80,    61] loss: 0.09724, adv_train_accuracy: 97.46, clean_train_accuracy : 71.09\n",
      "[80,    71] loss: 0.09800, adv_train_accuracy: 96.48, clean_train_accuracy : 66.21\n",
      "[80,    81] loss: 0.17620, adv_train_accuracy: 93.16, clean_train_accuracy : 68.16\n",
      "[80,    91] loss: 0.08975, adv_train_accuracy: 96.29, clean_train_accuracy : 69.34\n",
      "0.0244140625\n",
      "0.0685546875\n",
      "duration: 192 s - train loss: 0.12237 - train accuracy: 96.05 - validation loss: 1.06904 - validation accuracy: 79.02 \n",
      "[81,     1] loss: 0.09204, adv_train_accuracy: 97.27, clean_train_accuracy : 70.90\n",
      "[81,    11] loss: 0.10775, adv_train_accuracy: 96.29, clean_train_accuracy : 70.51\n",
      "[81,    21] loss: 0.11462, adv_train_accuracy: 96.68, clean_train_accuracy : 67.77\n",
      "[81,    31] loss: 0.11166, adv_train_accuracy: 95.70, clean_train_accuracy : 65.04\n",
      "[81,    41] loss: 0.12202, adv_train_accuracy: 96.29, clean_train_accuracy : 65.43\n",
      "[81,    51] loss: 0.09210, adv_train_accuracy: 96.68, clean_train_accuracy : 69.14\n",
      "[81,    61] loss: 0.10839, adv_train_accuracy: 96.48, clean_train_accuracy : 65.82\n",
      "[81,    71] loss: 0.09311, adv_train_accuracy: 96.88, clean_train_accuracy : 68.95\n",
      "[81,    81] loss: 0.14069, adv_train_accuracy: 95.12, clean_train_accuracy : 67.19\n",
      "[81,    91] loss: 0.10249, adv_train_accuracy: 97.07, clean_train_accuracy : 67.19\n",
      "0.01875\n",
      "0.077734375\n",
      "duration: 192 s - train loss: 0.12250 - train accuracy: 95.97 - validation loss: 1.18115 - validation accuracy: 77.25 \n",
      "[82,     1] loss: 0.12425, adv_train_accuracy: 95.70, clean_train_accuracy : 69.53\n",
      "[82,    11] loss: 0.14578, adv_train_accuracy: 95.12, clean_train_accuracy : 65.23\n",
      "[82,    21] loss: 0.12130, adv_train_accuracy: 96.09, clean_train_accuracy : 65.62\n",
      "[82,    31] loss: 0.09861, adv_train_accuracy: 97.66, clean_train_accuracy : 64.06\n",
      "[82,    41] loss: 0.09052, adv_train_accuracy: 96.29, clean_train_accuracy : 65.82\n",
      "[82,    51] loss: 0.11429, adv_train_accuracy: 96.09, clean_train_accuracy : 64.06\n",
      "[82,    61] loss: 0.11563, adv_train_accuracy: 95.90, clean_train_accuracy : 67.77\n",
      "[82,    71] loss: 0.18404, adv_train_accuracy: 93.16, clean_train_accuracy : 69.14\n",
      "[82,    81] loss: 0.12645, adv_train_accuracy: 96.68, clean_train_accuracy : 67.19\n",
      "[82,    91] loss: 0.13342, adv_train_accuracy: 95.70, clean_train_accuracy : 63.67\n",
      "0.023828125\n",
      "0.066015625\n",
      "duration: 192 s - train loss: 0.14047 - train accuracy: 95.52 - validation loss: 1.09153 - validation accuracy: 77.81 \n",
      "[83,     1] loss: 0.13887, adv_train_accuracy: 95.51, clean_train_accuracy : 67.58\n",
      "[83,    11] loss: 0.11887, adv_train_accuracy: 96.68, clean_train_accuracy : 67.58\n",
      "[83,    21] loss: 0.13588, adv_train_accuracy: 96.09, clean_train_accuracy : 66.41\n",
      "[83,    31] loss: 0.11014, adv_train_accuracy: 95.90, clean_train_accuracy : 70.51\n",
      "[83,    41] loss: 0.10956, adv_train_accuracy: 96.09, clean_train_accuracy : 67.97\n",
      "[83,    51] loss: 0.14522, adv_train_accuracy: 95.31, clean_train_accuracy : 67.77\n",
      "[83,    61] loss: 0.10661, adv_train_accuracy: 96.48, clean_train_accuracy : 64.26\n",
      "[83,    71] loss: 0.12285, adv_train_accuracy: 96.88, clean_train_accuracy : 70.90\n",
      "[83,    81] loss: 0.08580, adv_train_accuracy: 97.85, clean_train_accuracy : 68.55\n",
      "[83,    91] loss: 0.16302, adv_train_accuracy: 95.51, clean_train_accuracy : 66.60\n",
      "0.0140625\n",
      "0.0595703125\n",
      "duration: 192 s - train loss: 0.13306 - train accuracy: 95.74 - validation loss: 1.14610 - validation accuracy: 78.00 \n",
      "[84,     1] loss: 0.12903, adv_train_accuracy: 96.68, clean_train_accuracy : 68.75\n",
      "[84,    11] loss: 0.10047, adv_train_accuracy: 96.09, clean_train_accuracy : 72.27\n",
      "[84,    21] loss: 0.14481, adv_train_accuracy: 94.34, clean_train_accuracy : 67.97\n",
      "[84,    31] loss: 0.11543, adv_train_accuracy: 96.68, clean_train_accuracy : 73.05\n",
      "[84,    41] loss: 0.15298, adv_train_accuracy: 95.12, clean_train_accuracy : 70.51\n",
      "[84,    51] loss: 0.19515, adv_train_accuracy: 93.75, clean_train_accuracy : 66.60\n",
      "[84,    61] loss: 0.16174, adv_train_accuracy: 94.34, clean_train_accuracy : 66.41\n",
      "[84,    71] loss: 0.15695, adv_train_accuracy: 95.31, clean_train_accuracy : 67.97\n",
      "[84,    81] loss: 0.14136, adv_train_accuracy: 95.90, clean_train_accuracy : 68.36\n",
      "[84,    91] loss: 0.10004, adv_train_accuracy: 97.46, clean_train_accuracy : 63.09\n",
      "0.0166015625\n",
      "0.05390625\n",
      "duration: 192 s - train loss: 0.13864 - train accuracy: 95.42 - validation loss: 1.09690 - validation accuracy: 78.15 \n",
      "[85,     1] loss: 0.19454, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[85,    11] loss: 0.15046, adv_train_accuracy: 94.53, clean_train_accuracy : 65.23\n",
      "[85,    21] loss: 0.13737, adv_train_accuracy: 95.70, clean_train_accuracy : 66.02\n",
      "[85,    31] loss: 0.14442, adv_train_accuracy: 95.12, clean_train_accuracy : 65.62\n",
      "[85,    41] loss: 0.23773, adv_train_accuracy: 94.14, clean_train_accuracy : 64.65\n",
      "[85,    51] loss: 0.09011, adv_train_accuracy: 97.27, clean_train_accuracy : 72.66\n",
      "[85,    61] loss: 0.14329, adv_train_accuracy: 94.92, clean_train_accuracy : 69.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85,    71] loss: 0.16134, adv_train_accuracy: 93.36, clean_train_accuracy : 68.75\n",
      "[85,    81] loss: 0.12724, adv_train_accuracy: 95.31, clean_train_accuracy : 68.16\n",
      "[85,    91] loss: 0.11326, adv_train_accuracy: 96.68, clean_train_accuracy : 66.60\n",
      "0.0205078125\n",
      "0.057421875\n",
      "duration: 192 s - train loss: 0.13304 - train accuracy: 95.61 - validation loss: 1.16432 - validation accuracy: 78.52 \n",
      "[86,     1] loss: 0.08425, adv_train_accuracy: 97.85, clean_train_accuracy : 67.19\n",
      "[86,    11] loss: 0.07268, adv_train_accuracy: 97.27, clean_train_accuracy : 72.27\n",
      "[86,    21] loss: 0.09589, adv_train_accuracy: 97.07, clean_train_accuracy : 66.41\n",
      "[86,    31] loss: 0.11353, adv_train_accuracy: 96.09, clean_train_accuracy : 68.36\n",
      "[86,    41] loss: 0.10039, adv_train_accuracy: 97.07, clean_train_accuracy : 70.51\n",
      "[86,    51] loss: 0.08708, adv_train_accuracy: 97.07, clean_train_accuracy : 70.31\n",
      "[86,    61] loss: 0.09067, adv_train_accuracy: 97.07, clean_train_accuracy : 72.27\n",
      "[86,    71] loss: 0.08544, adv_train_accuracy: 96.48, clean_train_accuracy : 71.68\n",
      "[86,    81] loss: 0.11707, adv_train_accuracy: 95.51, clean_train_accuracy : 69.92\n",
      "[86,    91] loss: 0.19416, adv_train_accuracy: 93.75, clean_train_accuracy : 68.95\n",
      "0.0369140625\n",
      "0.08984375\n",
      "duration: 192 s - train loss: 0.11115 - train accuracy: 96.36 - validation loss: 1.18323 - validation accuracy: 77.94 \n",
      "[87,     1] loss: 0.14361, adv_train_accuracy: 95.31, clean_train_accuracy : 67.19\n",
      "[87,    11] loss: 0.12662, adv_train_accuracy: 95.12, clean_train_accuracy : 65.62\n",
      "[87,    21] loss: 0.10324, adv_train_accuracy: 97.07, clean_train_accuracy : 67.38\n",
      "[87,    31] loss: 0.13268, adv_train_accuracy: 97.27, clean_train_accuracy : 70.12\n",
      "[87,    41] loss: 0.08313, adv_train_accuracy: 97.07, clean_train_accuracy : 72.46\n",
      "[87,    51] loss: 0.15319, adv_train_accuracy: 95.70, clean_train_accuracy : 68.55\n",
      "[87,    61] loss: 0.10343, adv_train_accuracy: 96.48, clean_train_accuracy : 65.04\n",
      "[87,    71] loss: 0.09779, adv_train_accuracy: 96.88, clean_train_accuracy : 66.21\n",
      "[87,    81] loss: 0.09497, adv_train_accuracy: 96.48, clean_train_accuracy : 66.80\n",
      "[87,    91] loss: 0.08625, adv_train_accuracy: 96.48, clean_train_accuracy : 68.16\n",
      "0.018359375\n",
      "0.0494140625\n",
      "duration: 192 s - train loss: 0.11750 - train accuracy: 96.09 - validation loss: 1.06607 - validation accuracy: 78.89 \n",
      "[88,     1] loss: 0.13356, adv_train_accuracy: 94.14, clean_train_accuracy : 68.36\n",
      "[88,    11] loss: 0.13404, adv_train_accuracy: 96.48, clean_train_accuracy : 65.23\n",
      "[88,    21] loss: 0.10527, adv_train_accuracy: 96.48, clean_train_accuracy : 69.34\n",
      "[88,    31] loss: 0.10431, adv_train_accuracy: 95.90, clean_train_accuracy : 65.23\n",
      "[88,    41] loss: 0.14761, adv_train_accuracy: 95.31, clean_train_accuracy : 66.80\n",
      "[88,    51] loss: 0.13614, adv_train_accuracy: 95.70, clean_train_accuracy : 66.60\n",
      "[88,    61] loss: 0.08151, adv_train_accuracy: 97.07, clean_train_accuracy : 70.12\n",
      "[88,    71] loss: 0.10756, adv_train_accuracy: 96.29, clean_train_accuracy : 67.77\n",
      "[88,    81] loss: 0.11047, adv_train_accuracy: 95.90, clean_train_accuracy : 69.14\n",
      "[88,    91] loss: 0.13948, adv_train_accuracy: 95.51, clean_train_accuracy : 67.77\n",
      "0.0158203125\n",
      "0.0671875\n",
      "duration: 192 s - train loss: 0.12453 - train accuracy: 95.93 - validation loss: 1.04411 - validation accuracy: 79.00 \n",
      "[89,     1] loss: 0.13077, adv_train_accuracy: 95.70, clean_train_accuracy : 66.80\n",
      "[89,    11] loss: 0.08227, adv_train_accuracy: 97.27, clean_train_accuracy : 72.46\n",
      "[89,    21] loss: 0.08572, adv_train_accuracy: 97.07, clean_train_accuracy : 68.75\n",
      "[89,    31] loss: 0.13716, adv_train_accuracy: 95.31, clean_train_accuracy : 68.75\n",
      "[89,    41] loss: 0.11093, adv_train_accuracy: 96.09, clean_train_accuracy : 68.36\n",
      "[89,    51] loss: 0.12028, adv_train_accuracy: 96.48, clean_train_accuracy : 66.41\n",
      "[89,    61] loss: 0.10471, adv_train_accuracy: 96.48, clean_train_accuracy : 69.73\n",
      "[89,    71] loss: 0.14250, adv_train_accuracy: 95.70, clean_train_accuracy : 66.41\n",
      "[89,    81] loss: 0.15966, adv_train_accuracy: 94.14, clean_train_accuracy : 69.14\n",
      "[89,    91] loss: 0.10027, adv_train_accuracy: 96.68, clean_train_accuracy : 72.27\n",
      "0.0228515625\n",
      "0.0548828125\n",
      "duration: 192 s - train loss: 0.12202 - train accuracy: 95.99 - validation loss: 1.13459 - validation accuracy: 78.84 \n",
      "[90,     1] loss: 0.18907, adv_train_accuracy: 94.14, clean_train_accuracy : 65.23\n",
      "[90,    11] loss: 0.12110, adv_train_accuracy: 95.90, clean_train_accuracy : 69.53\n",
      "[90,    21] loss: 0.12407, adv_train_accuracy: 96.09, clean_train_accuracy : 63.87\n",
      "[90,    31] loss: 0.10529, adv_train_accuracy: 96.48, clean_train_accuracy : 69.14\n",
      "[90,    41] loss: 0.10581, adv_train_accuracy: 95.90, clean_train_accuracy : 67.58\n",
      "[90,    51] loss: 0.11192, adv_train_accuracy: 95.90, clean_train_accuracy : 74.02\n",
      "[90,    61] loss: 0.15109, adv_train_accuracy: 94.92, clean_train_accuracy : 70.31\n",
      "[90,    71] loss: 0.13774, adv_train_accuracy: 95.70, clean_train_accuracy : 67.97\n",
      "[90,    81] loss: 0.12219, adv_train_accuracy: 95.70, clean_train_accuracy : 66.21\n",
      "[90,    91] loss: 0.18083, adv_train_accuracy: 94.53, clean_train_accuracy : 71.88\n",
      "0.0197265625\n",
      "0.072265625\n",
      "duration: 192 s - train loss: 0.13194 - train accuracy: 95.62 - validation loss: 1.01836 - validation accuracy: 79.32 \n",
      "[91,     1] loss: 0.10474, adv_train_accuracy: 96.09, clean_train_accuracy : 66.41\n",
      "[91,    11] loss: 0.13273, adv_train_accuracy: 95.70, clean_train_accuracy : 67.77\n",
      "[91,    21] loss: 0.09422, adv_train_accuracy: 97.07, clean_train_accuracy : 68.36\n",
      "[91,    31] loss: 0.15492, adv_train_accuracy: 95.31, clean_train_accuracy : 69.14\n",
      "[91,    41] loss: 0.13134, adv_train_accuracy: 94.73, clean_train_accuracy : 65.82\n",
      "[91,    51] loss: 0.11144, adv_train_accuracy: 96.29, clean_train_accuracy : 67.77\n",
      "[91,    61] loss: 0.15290, adv_train_accuracy: 95.31, clean_train_accuracy : 69.14\n",
      "[91,    71] loss: 0.11920, adv_train_accuracy: 95.51, clean_train_accuracy : 66.99\n",
      "[91,    81] loss: 0.16871, adv_train_accuracy: 94.53, clean_train_accuracy : 65.23\n",
      "[91,    91] loss: 0.14533, adv_train_accuracy: 95.51, clean_train_accuracy : 69.34\n",
      "0.025\n",
      "0.0630859375\n",
      "duration: 192 s - train loss: 0.13115 - train accuracy: 95.65 - validation loss: 1.03512 - validation accuracy: 78.24 \n",
      "[92,     1] loss: 0.14543, adv_train_accuracy: 95.90, clean_train_accuracy : 65.23\n",
      "[92,    11] loss: 0.13559, adv_train_accuracy: 94.53, clean_train_accuracy : 68.36\n",
      "[92,    21] loss: 0.12183, adv_train_accuracy: 96.29, clean_train_accuracy : 68.55\n",
      "[92,    31] loss: 0.17617, adv_train_accuracy: 94.14, clean_train_accuracy : 69.92\n",
      "[92,    41] loss: 0.10457, adv_train_accuracy: 96.48, clean_train_accuracy : 66.21\n",
      "[92,    51] loss: 0.14013, adv_train_accuracy: 95.90, clean_train_accuracy : 66.80\n",
      "[92,    61] loss: 0.15678, adv_train_accuracy: 94.73, clean_train_accuracy : 67.58\n",
      "[92,    71] loss: 0.17643, adv_train_accuracy: 94.14, clean_train_accuracy : 66.60\n",
      "[92,    81] loss: 0.09138, adv_train_accuracy: 97.07, clean_train_accuracy : 69.14\n",
      "[92,    91] loss: 0.11033, adv_train_accuracy: 96.29, clean_train_accuracy : 72.27\n",
      "0.0181640625\n",
      "0.065234375\n",
      "duration: 192 s - train loss: 0.13744 - train accuracy: 95.58 - validation loss: 1.02702 - validation accuracy: 78.07 \n",
      "[93,     1] loss: 0.12096, adv_train_accuracy: 97.07, clean_train_accuracy : 67.38\n",
      "[93,    11] loss: 0.14790, adv_train_accuracy: 95.51, clean_train_accuracy : 71.68\n",
      "[93,    21] loss: 0.11089, adv_train_accuracy: 96.29, clean_train_accuracy : 66.80\n",
      "[93,    31] loss: 0.18130, adv_train_accuracy: 93.36, clean_train_accuracy : 67.58\n",
      "[93,    41] loss: 0.14887, adv_train_accuracy: 95.31, clean_train_accuracy : 60.74\n",
      "[93,    51] loss: 0.14962, adv_train_accuracy: 94.92, clean_train_accuracy : 69.73\n",
      "[93,    61] loss: 0.10964, adv_train_accuracy: 96.88, clean_train_accuracy : 68.16\n",
      "[93,    71] loss: 0.12183, adv_train_accuracy: 95.70, clean_train_accuracy : 68.75\n",
      "[93,    81] loss: 0.13066, adv_train_accuracy: 96.09, clean_train_accuracy : 69.53\n",
      "[93,    91] loss: 0.16736, adv_train_accuracy: 94.53, clean_train_accuracy : 67.19\n",
      "0.030078125\n",
      "0.0798828125\n",
      "duration: 192 s - train loss: 0.13634 - train accuracy: 95.63 - validation loss: 1.02674 - validation accuracy: 79.48 \n",
      "[94,     1] loss: 0.13330, adv_train_accuracy: 95.31, clean_train_accuracy : 69.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94,    11] loss: 0.07365, adv_train_accuracy: 97.85, clean_train_accuracy : 68.75\n",
      "[94,    21] loss: 0.14953, adv_train_accuracy: 95.31, clean_train_accuracy : 68.75\n",
      "[94,    31] loss: 0.15975, adv_train_accuracy: 94.14, clean_train_accuracy : 66.21\n",
      "[94,    41] loss: 0.14811, adv_train_accuracy: 95.12, clean_train_accuracy : 64.06\n",
      "[94,    51] loss: 0.14137, adv_train_accuracy: 95.31, clean_train_accuracy : 70.51\n",
      "[94,    61] loss: 0.12818, adv_train_accuracy: 95.12, clean_train_accuracy : 67.58\n",
      "[94,    71] loss: 0.11358, adv_train_accuracy: 96.68, clean_train_accuracy : 63.28\n",
      "[94,    81] loss: 0.15220, adv_train_accuracy: 95.90, clean_train_accuracy : 64.84\n",
      "[94,    91] loss: 0.09331, adv_train_accuracy: 96.48, clean_train_accuracy : 71.88\n",
      "0.02890625\n",
      "0.08125\n",
      "duration: 192 s - train loss: 0.15171 - train accuracy: 95.06 - validation loss: 1.03223 - validation accuracy: 79.69 \n",
      "[95,     1] loss: 0.16078, adv_train_accuracy: 95.90, clean_train_accuracy : 69.92\n",
      "[95,    11] loss: 0.11356, adv_train_accuracy: 96.88, clean_train_accuracy : 72.46\n",
      "[95,    21] loss: 0.16586, adv_train_accuracy: 94.92, clean_train_accuracy : 68.95\n",
      "[95,    31] loss: 0.17168, adv_train_accuracy: 94.14, clean_train_accuracy : 68.36\n",
      "[95,    41] loss: 0.10611, adv_train_accuracy: 96.29, clean_train_accuracy : 66.80\n",
      "[95,    51] loss: 0.11957, adv_train_accuracy: 96.48, clean_train_accuracy : 65.62\n",
      "[95,    61] loss: 0.12975, adv_train_accuracy: 95.12, clean_train_accuracy : 70.70\n",
      "[95,    71] loss: 0.11166, adv_train_accuracy: 96.09, clean_train_accuracy : 67.97\n",
      "[95,    81] loss: 0.14845, adv_train_accuracy: 95.12, clean_train_accuracy : 70.12\n",
      "[95,    91] loss: 0.17520, adv_train_accuracy: 95.31, clean_train_accuracy : 71.48\n",
      "0.043359375\n",
      "0.108203125\n",
      "duration: 192 s - train loss: 0.13665 - train accuracy: 95.59 - validation loss: 1.15715 - validation accuracy: 76.37 \n",
      "[96,     1] loss: 0.14119, adv_train_accuracy: 94.73, clean_train_accuracy : 66.80\n",
      "[96,    11] loss: 0.12641, adv_train_accuracy: 95.70, clean_train_accuracy : 69.53\n",
      "[96,    21] loss: 0.12749, adv_train_accuracy: 96.09, clean_train_accuracy : 67.38\n",
      "[96,    31] loss: 0.08736, adv_train_accuracy: 97.46, clean_train_accuracy : 72.27\n",
      "[96,    41] loss: 0.18335, adv_train_accuracy: 93.95, clean_train_accuracy : 68.16\n",
      "[96,    51] loss: 0.12292, adv_train_accuracy: 95.70, clean_train_accuracy : 71.29\n",
      "[96,    61] loss: 0.21468, adv_train_accuracy: 93.16, clean_train_accuracy : 68.95\n",
      "[96,    71] loss: 0.16960, adv_train_accuracy: 94.14, clean_train_accuracy : 67.19\n",
      "[96,    81] loss: 0.09093, adv_train_accuracy: 97.27, clean_train_accuracy : 66.80\n",
      "[96,    91] loss: 0.14249, adv_train_accuracy: 95.12, clean_train_accuracy : 67.19\n",
      "0.0279296875\n",
      "0.1328125\n",
      "duration: 192 s - train loss: 0.14007 - train accuracy: 95.30 - validation loss: 1.24525 - validation accuracy: 76.22 \n",
      "[97,     1] loss: 0.17446, adv_train_accuracy: 94.92, clean_train_accuracy : 65.62\n",
      "[97,    11] loss: 0.06928, adv_train_accuracy: 97.46, clean_train_accuracy : 71.68\n",
      "[97,    21] loss: 0.13404, adv_train_accuracy: 96.09, clean_train_accuracy : 69.92\n",
      "[97,    31] loss: 0.11633, adv_train_accuracy: 96.29, clean_train_accuracy : 67.97\n",
      "[97,    41] loss: 0.13549, adv_train_accuracy: 94.53, clean_train_accuracy : 65.82\n",
      "[97,    51] loss: 0.09223, adv_train_accuracy: 96.88, clean_train_accuracy : 69.53\n",
      "[97,    61] loss: 0.13224, adv_train_accuracy: 95.12, clean_train_accuracy : 69.34\n",
      "[97,    71] loss: 0.15602, adv_train_accuracy: 95.70, clean_train_accuracy : 66.99\n",
      "[97,    81] loss: 0.06914, adv_train_accuracy: 97.46, clean_train_accuracy : 68.36\n",
      "[97,    91] loss: 0.13086, adv_train_accuracy: 96.48, clean_train_accuracy : 71.09\n",
      "0.1162109375\n",
      "0.1689453125\n",
      "duration: 192 s - train loss: 0.12916 - train accuracy: 95.80 - validation loss: 1.06241 - validation accuracy: 80.04 \n",
      "[98,     1] loss: 0.18209, adv_train_accuracy: 95.12, clean_train_accuracy : 66.99\n",
      "[98,    11] loss: 0.12850, adv_train_accuracy: 96.29, clean_train_accuracy : 71.48\n",
      "[98,    21] loss: 0.20280, adv_train_accuracy: 92.97, clean_train_accuracy : 65.04\n",
      "[98,    31] loss: 0.13206, adv_train_accuracy: 94.73, clean_train_accuracy : 67.77\n",
      "[98,    41] loss: 0.11046, adv_train_accuracy: 96.68, clean_train_accuracy : 67.38\n",
      "[98,    51] loss: 0.09621, adv_train_accuracy: 96.88, clean_train_accuracy : 67.58\n",
      "[98,    61] loss: 0.16799, adv_train_accuracy: 93.95, clean_train_accuracy : 70.51\n",
      "[98,    71] loss: 0.12301, adv_train_accuracy: 97.07, clean_train_accuracy : 68.95\n",
      "[98,    81] loss: 0.13859, adv_train_accuracy: 95.90, clean_train_accuracy : 70.51\n",
      "[98,    91] loss: 0.14844, adv_train_accuracy: 95.51, clean_train_accuracy : 69.92\n",
      "0.01796875\n",
      "0.0623046875\n",
      "duration: 192 s - train loss: 0.13510 - train accuracy: 95.53 - validation loss: 0.94593 - validation accuracy: 80.90 \n",
      "[99,     1] loss: 0.09992, adv_train_accuracy: 95.70, clean_train_accuracy : 66.41\n",
      "[99,    11] loss: 0.16384, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[99,    21] loss: 0.10589, adv_train_accuracy: 96.68, clean_train_accuracy : 66.02\n",
      "[99,    31] loss: 0.15189, adv_train_accuracy: 93.95, clean_train_accuracy : 65.82\n",
      "[99,    41] loss: 0.12159, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[99,    51] loss: 0.10919, adv_train_accuracy: 96.48, clean_train_accuracy : 69.53\n",
      "[99,    61] loss: 0.13944, adv_train_accuracy: 96.09, clean_train_accuracy : 68.36\n",
      "[99,    71] loss: 0.09320, adv_train_accuracy: 97.46, clean_train_accuracy : 70.12\n",
      "[99,    81] loss: 0.07197, adv_train_accuracy: 97.46, clean_train_accuracy : 69.92\n",
      "[99,    91] loss: 0.14976, adv_train_accuracy: 94.92, clean_train_accuracy : 66.02\n",
      "0.015234375\n",
      "0.081640625\n",
      "duration: 192 s - train loss: 0.11956 - train accuracy: 96.03 - validation loss: 0.90085 - validation accuracy: 80.19 \n",
      "[100,     1] loss: 0.12166, adv_train_accuracy: 95.70, clean_train_accuracy : 66.60\n",
      "[100,    11] loss: 0.14690, adv_train_accuracy: 95.70, clean_train_accuracy : 70.51\n",
      "[100,    21] loss: 0.10492, adv_train_accuracy: 96.48, clean_train_accuracy : 74.02\n",
      "[100,    31] loss: 0.12357, adv_train_accuracy: 96.88, clean_train_accuracy : 71.68\n",
      "[100,    41] loss: 0.10884, adv_train_accuracy: 97.07, clean_train_accuracy : 69.34\n",
      "[100,    51] loss: 0.15511, adv_train_accuracy: 94.92, clean_train_accuracy : 67.77\n",
      "[100,    61] loss: 0.07690, adv_train_accuracy: 97.27, clean_train_accuracy : 70.12\n",
      "[100,    71] loss: 0.13979, adv_train_accuracy: 95.12, clean_train_accuracy : 65.04\n",
      "[100,    81] loss: 0.25194, adv_train_accuracy: 91.80, clean_train_accuracy : 63.67\n",
      "[100,    91] loss: 0.08610, adv_train_accuracy: 97.07, clean_train_accuracy : 73.44\n",
      "0.0748046875\n",
      "0.1396484375\n",
      "duration: 192 s - train loss: 0.12315 - train accuracy: 95.95 - validation loss: 1.04992 - validation accuracy: 79.75 \n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_stats1 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-200-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 200,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast adversarial training\n",
      "fast adv. train.\n",
      "[1,     1] loss: 0.16923, adv_train_accuracy: 93.55, clean_train_accuracy : 65.62\n",
      "[1,    11] loss: 0.12603, adv_train_accuracy: 95.51, clean_train_accuracy : 66.99\n",
      "[1,    21] loss: 0.12352, adv_train_accuracy: 95.12, clean_train_accuracy : 68.55\n",
      "[1,    31] loss: 0.12536, adv_train_accuracy: 95.70, clean_train_accuracy : 71.88\n",
      "[1,    41] loss: 0.10198, adv_train_accuracy: 96.09, clean_train_accuracy : 66.21\n",
      "[1,    51] loss: 0.11760, adv_train_accuracy: 96.29, clean_train_accuracy : 67.19\n",
      "[1,    61] loss: 0.08857, adv_train_accuracy: 97.46, clean_train_accuracy : 68.55\n",
      "[1,    71] loss: 0.12697, adv_train_accuracy: 95.51, clean_train_accuracy : 69.73\n",
      "[1,    81] loss: 0.05962, adv_train_accuracy: 97.85, clean_train_accuracy : 68.95\n",
      "[1,    91] loss: 0.15804, adv_train_accuracy: 94.34, clean_train_accuracy : 67.38\n",
      "0.0341796875\n",
      "0.1130859375\n",
      "duration: 193 s - train loss: 0.12273 - train accuracy: 95.97 - validation loss: 1.08391 - validation accuracy: 79.27 \n",
      "[2,     1] loss: 0.08941, adv_train_accuracy: 96.88, clean_train_accuracy : 71.29\n",
      "[2,    11] loss: 0.12050, adv_train_accuracy: 96.68, clean_train_accuracy : 67.77\n",
      "[2,    21] loss: 0.15424, adv_train_accuracy: 94.53, clean_train_accuracy : 63.09\n",
      "[2,    31] loss: 0.14629, adv_train_accuracy: 94.73, clean_train_accuracy : 68.36\n",
      "[2,    41] loss: 0.14847, adv_train_accuracy: 94.92, clean_train_accuracy : 65.43\n",
      "[2,    51] loss: 0.11365, adv_train_accuracy: 95.90, clean_train_accuracy : 68.55\n",
      "[2,    61] loss: 0.08785, adv_train_accuracy: 97.07, clean_train_accuracy : 67.19\n",
      "[2,    71] loss: 0.11181, adv_train_accuracy: 95.70, clean_train_accuracy : 69.73\n",
      "[2,    81] loss: 0.13456, adv_train_accuracy: 94.92, clean_train_accuracy : 65.82\n",
      "[2,    91] loss: 0.15356, adv_train_accuracy: 95.31, clean_train_accuracy : 67.38\n",
      "0.0302734375\n",
      "0.1041015625\n",
      "duration: 193 s - train loss: 0.12221 - train accuracy: 95.96 - validation loss: 1.10756 - validation accuracy: 78.78 \n",
      "[3,     1] loss: 0.11900, adv_train_accuracy: 95.51, clean_train_accuracy : 64.84\n",
      "[3,    11] loss: 0.12330, adv_train_accuracy: 95.31, clean_train_accuracy : 68.16\n",
      "[3,    21] loss: 0.17399, adv_train_accuracy: 94.53, clean_train_accuracy : 64.84\n",
      "[3,    31] loss: 0.22238, adv_train_accuracy: 93.36, clean_train_accuracy : 65.04\n",
      "[3,    41] loss: 0.16098, adv_train_accuracy: 94.92, clean_train_accuracy : 64.06\n",
      "[3,    51] loss: 0.11553, adv_train_accuracy: 95.70, clean_train_accuracy : 67.77\n",
      "[3,    61] loss: 0.13770, adv_train_accuracy: 94.73, clean_train_accuracy : 67.19\n",
      "[3,    71] loss: 0.09220, adv_train_accuracy: 97.27, clean_train_accuracy : 67.38\n",
      "[3,    81] loss: 0.08585, adv_train_accuracy: 96.09, clean_train_accuracy : 67.58\n",
      "[3,    91] loss: 0.15565, adv_train_accuracy: 94.14, clean_train_accuracy : 65.82\n",
      "0.059765625\n",
      "0.1478515625\n",
      "duration: 192 s - train loss: 0.12915 - train accuracy: 95.80 - validation loss: 1.25756 - validation accuracy: 77.28 \n",
      "[4,     1] loss: 0.14123, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[4,    11] loss: 0.18311, adv_train_accuracy: 95.12, clean_train_accuracy : 66.02\n",
      "[4,    21] loss: 0.15360, adv_train_accuracy: 94.34, clean_train_accuracy : 66.21\n",
      "[4,    31] loss: 0.13208, adv_train_accuracy: 96.29, clean_train_accuracy : 62.70\n",
      "[4,    41] loss: 0.09017, adv_train_accuracy: 96.68, clean_train_accuracy : 71.29\n",
      "[4,    51] loss: 0.08546, adv_train_accuracy: 96.48, clean_train_accuracy : 67.58\n",
      "[4,    61] loss: 0.14194, adv_train_accuracy: 94.73, clean_train_accuracy : 63.67\n",
      "[4,    71] loss: 0.14009, adv_train_accuracy: 96.09, clean_train_accuracy : 69.34\n",
      "[4,    81] loss: 0.09640, adv_train_accuracy: 96.29, clean_train_accuracy : 73.44\n",
      "[4,    91] loss: 0.04959, adv_train_accuracy: 98.24, clean_train_accuracy : 71.09\n",
      "0.081640625\n",
      "0.1390625\n",
      "duration: 192 s - train loss: 0.12723 - train accuracy: 95.88 - validation loss: 1.01757 - validation accuracy: 79.19 \n",
      "[5,     1] loss: 0.10003, adv_train_accuracy: 96.48, clean_train_accuracy : 66.41\n",
      "[5,    11] loss: 0.14774, adv_train_accuracy: 95.51, clean_train_accuracy : 67.38\n",
      "[5,    21] loss: 0.08877, adv_train_accuracy: 96.29, clean_train_accuracy : 66.21\n",
      "[5,    31] loss: 0.13320, adv_train_accuracy: 95.12, clean_train_accuracy : 70.90\n",
      "[5,    41] loss: 0.09609, adv_train_accuracy: 96.48, clean_train_accuracy : 67.97\n",
      "[5,    51] loss: 0.09951, adv_train_accuracy: 96.68, clean_train_accuracy : 66.21\n",
      "[5,    61] loss: 0.10730, adv_train_accuracy: 96.68, clean_train_accuracy : 65.43\n",
      "[5,    71] loss: 0.18521, adv_train_accuracy: 93.95, clean_train_accuracy : 70.31\n",
      "[5,    81] loss: 0.11208, adv_train_accuracy: 95.31, clean_train_accuracy : 64.45\n",
      "[5,    91] loss: 0.09859, adv_train_accuracy: 95.90, clean_train_accuracy : 70.12\n",
      "0.019140625\n",
      "0.0697265625\n",
      "duration: 194 s - train loss: 0.12671 - train accuracy: 95.85 - validation loss: 1.13446 - validation accuracy: 78.29 \n",
      "[6,     1] loss: 0.13587, adv_train_accuracy: 95.12, clean_train_accuracy : 71.48\n",
      "[6,    11] loss: 0.18226, adv_train_accuracy: 93.95, clean_train_accuracy : 67.58\n",
      "[6,    21] loss: 0.12127, adv_train_accuracy: 95.70, clean_train_accuracy : 68.36\n",
      "[6,    31] loss: 0.13217, adv_train_accuracy: 95.51, clean_train_accuracy : 66.41\n",
      "[6,    41] loss: 0.10553, adv_train_accuracy: 96.68, clean_train_accuracy : 69.73\n",
      "[6,    51] loss: 0.09784, adv_train_accuracy: 96.09, clean_train_accuracy : 68.16\n",
      "[6,    61] loss: 0.10510, adv_train_accuracy: 96.48, clean_train_accuracy : 64.84\n",
      "[6,    71] loss: 0.17068, adv_train_accuracy: 93.95, clean_train_accuracy : 70.31\n",
      "[6,    81] loss: 0.08871, adv_train_accuracy: 96.68, clean_train_accuracy : 66.60\n",
      "[6,    91] loss: 0.11410, adv_train_accuracy: 96.09, clean_train_accuracy : 63.48\n",
      "0.0169921875\n",
      "0.0607421875\n",
      "duration: 192 s - train loss: 0.12014 - train accuracy: 96.10 - validation loss: 1.04144 - validation accuracy: 78.68 \n",
      "[7,     1] loss: 0.11882, adv_train_accuracy: 96.88, clean_train_accuracy : 65.82\n",
      "[7,    11] loss: 0.07945, adv_train_accuracy: 96.68, clean_train_accuracy : 69.14\n",
      "[7,    21] loss: 0.09427, adv_train_accuracy: 97.07, clean_train_accuracy : 67.38\n",
      "[7,    31] loss: 0.10388, adv_train_accuracy: 97.27, clean_train_accuracy : 73.05\n",
      "[7,    41] loss: 0.09676, adv_train_accuracy: 96.48, clean_train_accuracy : 70.51\n",
      "[7,    51] loss: 0.09027, adv_train_accuracy: 96.48, clean_train_accuracy : 69.14\n",
      "[7,    61] loss: 0.14379, adv_train_accuracy: 95.12, clean_train_accuracy : 67.77\n",
      "[7,    71] loss: 0.15325, adv_train_accuracy: 94.34, clean_train_accuracy : 65.43\n",
      "[7,    81] loss: 0.10922, adv_train_accuracy: 96.48, clean_train_accuracy : 68.55\n",
      "[7,    91] loss: 0.10055, adv_train_accuracy: 96.29, clean_train_accuracy : 70.31\n",
      "0.0216796875\n",
      "0.0646484375\n",
      "duration: 193 s - train loss: 0.10846 - train accuracy: 96.43 - validation loss: 1.11676 - validation accuracy: 79.52 \n",
      "[8,     1] loss: 0.08967, adv_train_accuracy: 96.88, clean_train_accuracy : 68.95\n",
      "[8,    11] loss: 0.12005, adv_train_accuracy: 95.70, clean_train_accuracy : 67.77\n",
      "[8,    21] loss: 0.17303, adv_train_accuracy: 93.95, clean_train_accuracy : 67.97\n",
      "[8,    31] loss: 0.09873, adv_train_accuracy: 96.88, clean_train_accuracy : 69.73\n",
      "[8,    41] loss: 0.13420, adv_train_accuracy: 96.48, clean_train_accuracy : 67.38\n",
      "[8,    51] loss: 0.15115, adv_train_accuracy: 95.12, clean_train_accuracy : 73.44\n",
      "[8,    61] loss: 0.14388, adv_train_accuracy: 94.92, clean_train_accuracy : 69.73\n",
      "[8,    71] loss: 0.14754, adv_train_accuracy: 95.31, clean_train_accuracy : 66.02\n",
      "[8,    81] loss: 0.08128, adv_train_accuracy: 97.07, clean_train_accuracy : 67.38\n",
      "[8,    91] loss: 0.13492, adv_train_accuracy: 95.90, clean_train_accuracy : 65.23\n",
      "0.04609375\n",
      "0.099609375\n",
      "duration: 193 s - train loss: 0.12435 - train accuracy: 95.94 - validation loss: 1.13070 - validation accuracy: 77.97 \n",
      "[9,     1] loss: 0.13355, adv_train_accuracy: 95.90, clean_train_accuracy : 67.58\n",
      "[9,    11] loss: 0.10672, adv_train_accuracy: 96.29, clean_train_accuracy : 65.23\n",
      "[9,    21] loss: 0.14142, adv_train_accuracy: 95.51, clean_train_accuracy : 62.89\n",
      "[9,    31] loss: 0.11474, adv_train_accuracy: 96.68, clean_train_accuracy : 65.82\n",
      "[9,    41] loss: 0.09474, adv_train_accuracy: 96.88, clean_train_accuracy : 67.38\n",
      "[9,    51] loss: 0.10046, adv_train_accuracy: 96.48, clean_train_accuracy : 73.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9,    61] loss: 0.06920, adv_train_accuracy: 98.05, clean_train_accuracy : 70.70\n",
      "[9,    71] loss: 0.12082, adv_train_accuracy: 96.48, clean_train_accuracy : 68.75\n",
      "[9,    81] loss: 0.13744, adv_train_accuracy: 95.51, clean_train_accuracy : 69.92\n",
      "[9,    91] loss: 0.09700, adv_train_accuracy: 96.29, clean_train_accuracy : 71.48\n",
      "0.024609375\n",
      "0.0580078125\n",
      "duration: 192 s - train loss: 0.11087 - train accuracy: 96.46 - validation loss: 1.01707 - validation accuracy: 80.10 \n",
      "[10,     1] loss: 0.08070, adv_train_accuracy: 96.48, clean_train_accuracy : 65.62\n",
      "[10,    11] loss: 0.16325, adv_train_accuracy: 94.34, clean_train_accuracy : 67.97\n",
      "[10,    21] loss: 0.08608, adv_train_accuracy: 96.88, clean_train_accuracy : 68.16\n",
      "[10,    31] loss: 0.18510, adv_train_accuracy: 93.55, clean_train_accuracy : 64.26\n",
      "[10,    41] loss: 0.09882, adv_train_accuracy: 96.88, clean_train_accuracy : 67.97\n",
      "[10,    51] loss: 0.09764, adv_train_accuracy: 96.68, clean_train_accuracy : 67.58\n",
      "[10,    61] loss: 0.10136, adv_train_accuracy: 96.29, clean_train_accuracy : 68.75\n",
      "[10,    71] loss: 0.13415, adv_train_accuracy: 95.31, clean_train_accuracy : 68.75\n",
      "[10,    81] loss: 0.09957, adv_train_accuracy: 96.68, clean_train_accuracy : 67.38\n",
      "[10,    91] loss: 0.11343, adv_train_accuracy: 96.09, clean_train_accuracy : 69.14\n",
      "0.01953125\n",
      "0.0919921875\n",
      "duration: 191 s - train loss: 0.11776 - train accuracy: 96.01 - validation loss: 1.20346 - validation accuracy: 77.90 \n",
      "[11,     1] loss: 0.12250, adv_train_accuracy: 96.29, clean_train_accuracy : 73.44\n",
      "[11,    11] loss: 0.13493, adv_train_accuracy: 96.29, clean_train_accuracy : 68.16\n",
      "[11,    21] loss: 0.11466, adv_train_accuracy: 96.48, clean_train_accuracy : 71.88\n",
      "[11,    31] loss: 0.10511, adv_train_accuracy: 96.48, clean_train_accuracy : 67.19\n",
      "[11,    41] loss: 0.08484, adv_train_accuracy: 96.68, clean_train_accuracy : 70.51\n",
      "[11,    51] loss: 0.11793, adv_train_accuracy: 96.48, clean_train_accuracy : 72.66\n",
      "[11,    61] loss: 0.08822, adv_train_accuracy: 97.07, clean_train_accuracy : 69.14\n",
      "[11,    71] loss: 0.09098, adv_train_accuracy: 97.27, clean_train_accuracy : 66.99\n",
      "[11,    81] loss: 0.08454, adv_train_accuracy: 97.46, clean_train_accuracy : 68.95\n",
      "[11,    91] loss: 0.08028, adv_train_accuracy: 97.07, clean_train_accuracy : 68.16\n",
      "0.0251953125\n",
      "0.0875\n",
      "duration: 192 s - train loss: 0.10724 - train accuracy: 96.49 - validation loss: 1.02576 - validation accuracy: 80.04 \n",
      "[12,     1] loss: 0.13987, adv_train_accuracy: 95.90, clean_train_accuracy : 69.34\n",
      "[12,    11] loss: 0.13654, adv_train_accuracy: 95.12, clean_train_accuracy : 66.80\n",
      "[12,    21] loss: 0.16357, adv_train_accuracy: 93.75, clean_train_accuracy : 64.65\n",
      "[12,    31] loss: 0.15650, adv_train_accuracy: 94.92, clean_train_accuracy : 68.16\n",
      "[12,    41] loss: 0.09482, adv_train_accuracy: 96.29, clean_train_accuracy : 70.51\n",
      "[12,    51] loss: 0.10362, adv_train_accuracy: 96.29, clean_train_accuracy : 71.88\n",
      "[12,    61] loss: 0.09508, adv_train_accuracy: 96.68, clean_train_accuracy : 64.84\n",
      "[12,    71] loss: 0.08178, adv_train_accuracy: 97.07, clean_train_accuracy : 67.19\n",
      "[12,    81] loss: 0.06897, adv_train_accuracy: 98.05, clean_train_accuracy : 71.88\n",
      "[12,    91] loss: 0.09422, adv_train_accuracy: 97.27, clean_train_accuracy : 69.34\n",
      "0.01953125\n",
      "0.076171875\n",
      "duration: 192 s - train loss: 0.11365 - train accuracy: 96.36 - validation loss: 1.22897 - validation accuracy: 79.31 \n",
      "[13,     1] loss: 0.08307, adv_train_accuracy: 96.68, clean_train_accuracy : 70.90\n",
      "[13,    11] loss: 0.06665, adv_train_accuracy: 97.46, clean_train_accuracy : 67.58\n",
      "[13,    21] loss: 0.05096, adv_train_accuracy: 98.44, clean_train_accuracy : 66.99\n",
      "[13,    31] loss: 0.11630, adv_train_accuracy: 95.90, clean_train_accuracy : 70.12\n",
      "[13,    41] loss: 0.11729, adv_train_accuracy: 95.90, clean_train_accuracy : 64.84\n",
      "[13,    51] loss: 0.07418, adv_train_accuracy: 97.85, clean_train_accuracy : 67.58\n",
      "[13,    61] loss: 0.06287, adv_train_accuracy: 97.27, clean_train_accuracy : 67.38\n",
      "[13,    71] loss: 0.09587, adv_train_accuracy: 97.07, clean_train_accuracy : 66.02\n",
      "[13,    81] loss: 0.08148, adv_train_accuracy: 97.27, clean_train_accuracy : 71.09\n",
      "[13,    91] loss: 0.09096, adv_train_accuracy: 97.66, clean_train_accuracy : 68.55\n",
      "0.009375\n",
      "0.0791015625\n",
      "duration: 192 s - train loss: 0.09913 - train accuracy: 96.75 - validation loss: 1.11881 - validation accuracy: 80.13 \n",
      "[14,     1] loss: 0.12571, adv_train_accuracy: 96.09, clean_train_accuracy : 70.70\n",
      "[14,    11] loss: 0.16524, adv_train_accuracy: 94.92, clean_train_accuracy : 70.90\n",
      "[14,    21] loss: 0.14673, adv_train_accuracy: 95.12, clean_train_accuracy : 68.36\n",
      "[14,    31] loss: 0.11320, adv_train_accuracy: 95.90, clean_train_accuracy : 64.45\n",
      "[14,    41] loss: 0.11572, adv_train_accuracy: 97.07, clean_train_accuracy : 64.45\n",
      "[14,    51] loss: 0.14097, adv_train_accuracy: 95.51, clean_train_accuracy : 68.55\n",
      "[14,    61] loss: 0.07983, adv_train_accuracy: 97.27, clean_train_accuracy : 65.04\n",
      "[14,    71] loss: 0.10593, adv_train_accuracy: 97.66, clean_train_accuracy : 67.19\n",
      "[14,    81] loss: 0.07855, adv_train_accuracy: 98.05, clean_train_accuracy : 70.31\n",
      "[14,    91] loss: 0.09290, adv_train_accuracy: 97.07, clean_train_accuracy : 66.99\n",
      "0.0228515625\n",
      "0.0896484375\n",
      "duration: 192 s - train loss: 0.11291 - train accuracy: 96.41 - validation loss: 1.17989 - validation accuracy: 79.30 \n",
      "[15,     1] loss: 0.14286, adv_train_accuracy: 95.31, clean_train_accuracy : 67.58\n",
      "[15,    11] loss: 0.11181, adv_train_accuracy: 96.09, clean_train_accuracy : 67.19\n",
      "[15,    21] loss: 0.11941, adv_train_accuracy: 96.29, clean_train_accuracy : 69.14\n",
      "[15,    31] loss: 0.08242, adv_train_accuracy: 97.85, clean_train_accuracy : 68.55\n",
      "[15,    41] loss: 0.12558, adv_train_accuracy: 95.51, clean_train_accuracy : 65.23\n",
      "[15,    51] loss: 0.11337, adv_train_accuracy: 96.29, clean_train_accuracy : 69.92\n",
      "[15,    61] loss: 0.15212, adv_train_accuracy: 94.34, clean_train_accuracy : 68.95\n",
      "[15,    71] loss: 0.08643, adv_train_accuracy: 96.88, clean_train_accuracy : 73.44\n",
      "[15,    81] loss: 0.12075, adv_train_accuracy: 96.29, clean_train_accuracy : 70.12\n",
      "[15,    91] loss: 0.11657, adv_train_accuracy: 96.29, clean_train_accuracy : 67.38\n",
      "0.044921875\n",
      "0.1208984375\n",
      "duration: 192 s - train loss: 0.12751 - train accuracy: 95.79 - validation loss: 1.08728 - validation accuracy: 79.28 \n",
      "[16,     1] loss: 0.13688, adv_train_accuracy: 95.12, clean_train_accuracy : 67.19\n",
      "[16,    11] loss: 0.07882, adv_train_accuracy: 97.27, clean_train_accuracy : 68.55\n",
      "[16,    21] loss: 0.14472, adv_train_accuracy: 96.09, clean_train_accuracy : 68.55\n",
      "[16,    31] loss: 0.14200, adv_train_accuracy: 95.12, clean_train_accuracy : 68.16\n",
      "[16,    41] loss: 0.09078, adv_train_accuracy: 96.88, clean_train_accuracy : 68.95\n",
      "[16,    51] loss: 0.17581, adv_train_accuracy: 96.48, clean_train_accuracy : 66.80\n",
      "[16,    61] loss: 0.12606, adv_train_accuracy: 96.48, clean_train_accuracy : 73.83\n",
      "[16,    71] loss: 0.12491, adv_train_accuracy: 96.09, clean_train_accuracy : 75.00\n",
      "[16,    81] loss: 0.13815, adv_train_accuracy: 95.90, clean_train_accuracy : 65.82\n",
      "[16,    91] loss: 0.13102, adv_train_accuracy: 94.53, clean_train_accuracy : 70.90\n",
      "0.0193359375\n",
      "0.08359375\n",
      "duration: 192 s - train loss: 0.11706 - train accuracy: 96.16 - validation loss: 1.04277 - validation accuracy: 79.51 \n",
      "[17,     1] loss: 0.12281, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[17,    11] loss: 0.09779, adv_train_accuracy: 96.68, clean_train_accuracy : 67.19\n",
      "[17,    21] loss: 0.08588, adv_train_accuracy: 97.85, clean_train_accuracy : 71.68\n",
      "[17,    31] loss: 0.08640, adv_train_accuracy: 97.27, clean_train_accuracy : 69.53\n",
      "[17,    41] loss: 0.08623, adv_train_accuracy: 95.70, clean_train_accuracy : 73.05\n",
      "[17,    51] loss: 0.08391, adv_train_accuracy: 96.88, clean_train_accuracy : 65.62\n",
      "[17,    61] loss: 0.15586, adv_train_accuracy: 95.51, clean_train_accuracy : 70.51\n",
      "[17,    71] loss: 0.13377, adv_train_accuracy: 95.31, clean_train_accuracy : 66.80\n",
      "[17,    81] loss: 0.09209, adv_train_accuracy: 97.27, clean_train_accuracy : 68.75\n",
      "[17,    91] loss: 0.13792, adv_train_accuracy: 94.53, clean_train_accuracy : 70.51\n",
      "0.0970703125\n",
      "0.1376953125\n",
      "duration: 192 s - train loss: 0.10675 - train accuracy: 96.46 - validation loss: 1.15801 - validation accuracy: 79.59 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18,     1] loss: 0.09879, adv_train_accuracy: 96.88, clean_train_accuracy : 70.70\n",
      "[18,    11] loss: 0.11271, adv_train_accuracy: 97.27, clean_train_accuracy : 70.90\n",
      "[18,    21] loss: 0.12798, adv_train_accuracy: 96.48, clean_train_accuracy : 68.16\n",
      "[18,    31] loss: 0.15513, adv_train_accuracy: 95.31, clean_train_accuracy : 71.09\n",
      "[18,    41] loss: 0.11850, adv_train_accuracy: 96.09, clean_train_accuracy : 69.53\n",
      "[18,    51] loss: 0.09113, adv_train_accuracy: 97.07, clean_train_accuracy : 70.51\n",
      "[18,    61] loss: 0.15887, adv_train_accuracy: 95.51, clean_train_accuracy : 71.88\n",
      "[18,    71] loss: 0.08790, adv_train_accuracy: 96.88, clean_train_accuracy : 68.75\n",
      "[18,    81] loss: 0.10243, adv_train_accuracy: 96.48, clean_train_accuracy : 66.02\n",
      "[18,    91] loss: 0.13359, adv_train_accuracy: 94.34, clean_train_accuracy : 68.55\n",
      "0.0361328125\n",
      "0.090625\n",
      "duration: 192 s - train loss: 0.11104 - train accuracy: 96.43 - validation loss: 1.06316 - validation accuracy: 78.79 \n",
      "[19,     1] loss: 0.06642, adv_train_accuracy: 97.27, clean_train_accuracy : 65.82\n",
      "[19,    11] loss: 0.06840, adv_train_accuracy: 98.05, clean_train_accuracy : 70.90\n",
      "[19,    21] loss: 0.16957, adv_train_accuracy: 93.95, clean_train_accuracy : 68.16\n",
      "[19,    31] loss: 0.13694, adv_train_accuracy: 95.12, clean_train_accuracy : 64.84\n",
      "[19,    41] loss: 0.07804, adv_train_accuracy: 97.46, clean_train_accuracy : 72.27\n",
      "[19,    51] loss: 0.08908, adv_train_accuracy: 96.88, clean_train_accuracy : 68.75\n",
      "[19,    61] loss: 0.14621, adv_train_accuracy: 95.12, clean_train_accuracy : 67.58\n",
      "[19,    71] loss: 0.16676, adv_train_accuracy: 94.14, clean_train_accuracy : 67.77\n",
      "[19,    81] loss: 0.13637, adv_train_accuracy: 94.73, clean_train_accuracy : 68.95\n",
      "[19,    91] loss: 0.08430, adv_train_accuracy: 96.48, clean_train_accuracy : 71.09\n",
      "0.017578125\n",
      "0.0759765625\n",
      "duration: 192 s - train loss: 0.11375 - train accuracy: 96.21 - validation loss: 0.97889 - validation accuracy: 80.52 \n",
      "[20,     1] loss: 0.11137, adv_train_accuracy: 97.07, clean_train_accuracy : 70.51\n",
      "[20,    11] loss: 0.12930, adv_train_accuracy: 94.73, clean_train_accuracy : 70.70\n",
      "[20,    21] loss: 0.15771, adv_train_accuracy: 95.31, clean_train_accuracy : 67.77\n",
      "[20,    31] loss: 0.06986, adv_train_accuracy: 97.85, clean_train_accuracy : 73.63\n",
      "[20,    41] loss: 0.12263, adv_train_accuracy: 96.09, clean_train_accuracy : 70.51\n",
      "[20,    51] loss: 0.06161, adv_train_accuracy: 97.46, clean_train_accuracy : 71.09\n",
      "[20,    61] loss: 0.08520, adv_train_accuracy: 98.05, clean_train_accuracy : 66.80\n",
      "[20,    71] loss: 0.11965, adv_train_accuracy: 96.88, clean_train_accuracy : 71.88\n",
      "[20,    81] loss: 0.13048, adv_train_accuracy: 95.51, clean_train_accuracy : 71.09\n",
      "[20,    91] loss: 0.08573, adv_train_accuracy: 97.27, clean_train_accuracy : 70.12\n",
      "0.0330078125\n",
      "0.0978515625\n",
      "duration: 192 s - train loss: 0.11336 - train accuracy: 96.34 - validation loss: 0.98453 - validation accuracy: 80.41 \n",
      "[21,     1] loss: 0.06085, adv_train_accuracy: 98.05, clean_train_accuracy : 71.29\n",
      "[21,    11] loss: 0.08581, adv_train_accuracy: 96.68, clean_train_accuracy : 70.51\n",
      "[21,    21] loss: 0.09391, adv_train_accuracy: 96.88, clean_train_accuracy : 69.34\n",
      "[21,    31] loss: 0.13737, adv_train_accuracy: 96.88, clean_train_accuracy : 65.62\n",
      "[21,    41] loss: 0.09688, adv_train_accuracy: 96.29, clean_train_accuracy : 67.38\n",
      "[21,    51] loss: 0.14363, adv_train_accuracy: 96.09, clean_train_accuracy : 67.19\n",
      "[21,    61] loss: 0.09678, adv_train_accuracy: 97.46, clean_train_accuracy : 66.02\n",
      "[21,    71] loss: 0.10244, adv_train_accuracy: 97.66, clean_train_accuracy : 71.88\n",
      "[21,    81] loss: 0.09977, adv_train_accuracy: 97.07, clean_train_accuracy : 70.70\n",
      "[21,    91] loss: 0.12241, adv_train_accuracy: 96.29, clean_train_accuracy : 64.45\n",
      "0.0228515625\n",
      "0.0837890625\n",
      "duration: 192 s - train loss: 0.10637 - train accuracy: 96.55 - validation loss: 1.02378 - validation accuracy: 81.27 \n",
      "[22,     1] loss: 0.08327, adv_train_accuracy: 97.46, clean_train_accuracy : 73.63\n",
      "[22,    11] loss: 0.09010, adv_train_accuracy: 97.07, clean_train_accuracy : 68.36\n",
      "[22,    21] loss: 0.10093, adv_train_accuracy: 95.70, clean_train_accuracy : 70.12\n",
      "[22,    31] loss: 0.13097, adv_train_accuracy: 95.90, clean_train_accuracy : 71.09\n",
      "[22,    41] loss: 0.08224, adv_train_accuracy: 96.68, clean_train_accuracy : 71.09\n",
      "[22,    51] loss: 0.13025, adv_train_accuracy: 95.90, clean_train_accuracy : 64.84\n",
      "[22,    61] loss: 0.19663, adv_train_accuracy: 94.92, clean_train_accuracy : 64.45\n",
      "[22,    71] loss: 0.13530, adv_train_accuracy: 94.92, clean_train_accuracy : 61.33\n",
      "[22,    81] loss: 0.17250, adv_train_accuracy: 94.73, clean_train_accuracy : 70.31\n",
      "[22,    91] loss: 0.07165, adv_train_accuracy: 96.88, clean_train_accuracy : 65.62\n",
      "0.0203125\n",
      "0.0921875\n",
      "duration: 192 s - train loss: 0.10683 - train accuracy: 96.52 - validation loss: 1.06510 - validation accuracy: 78.71 \n",
      "[23,     1] loss: 0.07611, adv_train_accuracy: 97.27, clean_train_accuracy : 66.21\n",
      "[23,    11] loss: 0.07289, adv_train_accuracy: 97.66, clean_train_accuracy : 69.92\n",
      "[23,    21] loss: 0.11776, adv_train_accuracy: 95.51, clean_train_accuracy : 68.16\n",
      "[23,    31] loss: 0.07712, adv_train_accuracy: 97.66, clean_train_accuracy : 67.77\n",
      "[23,    41] loss: 0.07628, adv_train_accuracy: 97.07, clean_train_accuracy : 67.19\n",
      "[23,    51] loss: 0.07993, adv_train_accuracy: 97.27, clean_train_accuracy : 66.21\n",
      "[23,    61] loss: 0.11039, adv_train_accuracy: 96.29, clean_train_accuracy : 70.51\n",
      "[23,    71] loss: 0.09775, adv_train_accuracy: 97.27, clean_train_accuracy : 66.60\n",
      "[23,    81] loss: 0.08607, adv_train_accuracy: 97.46, clean_train_accuracy : 70.12\n",
      "[23,    91] loss: 0.07213, adv_train_accuracy: 97.66, clean_train_accuracy : 71.29\n",
      "0.0353515625\n",
      "0.0837890625\n",
      "duration: 192 s - train loss: 0.09696 - train accuracy: 96.81 - validation loss: 1.11840 - validation accuracy: 79.43 \n",
      "[24,     1] loss: 0.13328, adv_train_accuracy: 96.09, clean_train_accuracy : 70.12\n",
      "[24,    11] loss: 0.12137, adv_train_accuracy: 95.70, clean_train_accuracy : 66.41\n",
      "[24,    21] loss: 0.06890, adv_train_accuracy: 97.85, clean_train_accuracy : 68.75\n",
      "[24,    31] loss: 0.06659, adv_train_accuracy: 97.07, clean_train_accuracy : 67.77\n",
      "[24,    41] loss: 0.08026, adv_train_accuracy: 96.48, clean_train_accuracy : 73.05\n",
      "[24,    51] loss: 0.10734, adv_train_accuracy: 95.90, clean_train_accuracy : 64.06\n",
      "[24,    61] loss: 0.06762, adv_train_accuracy: 97.07, clean_train_accuracy : 68.95\n",
      "[24,    71] loss: 0.11344, adv_train_accuracy: 96.09, clean_train_accuracy : 66.21\n",
      "[24,    81] loss: 0.10205, adv_train_accuracy: 96.88, clean_train_accuracy : 64.26\n",
      "[24,    91] loss: 0.09822, adv_train_accuracy: 96.48, clean_train_accuracy : 66.60\n",
      "0.073828125\n",
      "0.1517578125\n",
      "duration: 192 s - train loss: 0.09800 - train accuracy: 96.71 - validation loss: 1.13374 - validation accuracy: 79.81 \n",
      "[25,     1] loss: 0.10577, adv_train_accuracy: 96.68, clean_train_accuracy : 67.38\n",
      "[25,    11] loss: 0.13284, adv_train_accuracy: 95.70, clean_train_accuracy : 69.14\n",
      "[25,    21] loss: 0.10583, adv_train_accuracy: 95.51, clean_train_accuracy : 67.19\n",
      "[25,    31] loss: 0.11137, adv_train_accuracy: 96.09, clean_train_accuracy : 68.75\n",
      "[25,    41] loss: 0.13004, adv_train_accuracy: 95.70, clean_train_accuracy : 68.95\n",
      "[25,    51] loss: 0.10315, adv_train_accuracy: 96.09, clean_train_accuracy : 70.90\n",
      "[25,    61] loss: 0.07951, adv_train_accuracy: 97.85, clean_train_accuracy : 67.58\n",
      "[25,    71] loss: 0.13694, adv_train_accuracy: 94.53, clean_train_accuracy : 69.14\n",
      "[25,    81] loss: 0.09715, adv_train_accuracy: 97.27, clean_train_accuracy : 66.21\n",
      "[25,    91] loss: 0.09633, adv_train_accuracy: 95.90, clean_train_accuracy : 71.68\n",
      "0.0216796875\n",
      "0.1041015625\n",
      "duration: 192 s - train loss: 0.10145 - train accuracy: 96.65 - validation loss: 1.21643 - validation accuracy: 78.53 \n",
      "[26,     1] loss: 0.08234, adv_train_accuracy: 96.48, clean_train_accuracy : 72.85\n",
      "[26,    11] loss: 0.09436, adv_train_accuracy: 95.51, clean_train_accuracy : 65.82\n",
      "[26,    21] loss: 0.14809, adv_train_accuracy: 95.51, clean_train_accuracy : 69.53\n",
      "[26,    31] loss: 0.12840, adv_train_accuracy: 96.09, clean_train_accuracy : 69.34\n",
      "[26,    41] loss: 0.09418, adv_train_accuracy: 97.46, clean_train_accuracy : 67.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26,    51] loss: 0.07994, adv_train_accuracy: 97.27, clean_train_accuracy : 66.41\n",
      "[26,    61] loss: 0.05914, adv_train_accuracy: 97.46, clean_train_accuracy : 72.07\n",
      "[26,    71] loss: 0.16224, adv_train_accuracy: 94.53, clean_train_accuracy : 64.65\n",
      "[26,    81] loss: 0.14334, adv_train_accuracy: 95.70, clean_train_accuracy : 71.09\n",
      "[26,    91] loss: 0.10099, adv_train_accuracy: 96.29, clean_train_accuracy : 67.97\n",
      "0.2146484375\n",
      "0.18359375\n",
      "duration: 192 s - train loss: 0.11743 - train accuracy: 96.17 - validation loss: 1.01117 - validation accuracy: 79.25 \n",
      "[27,     1] loss: 0.06742, adv_train_accuracy: 97.85, clean_train_accuracy : 68.75\n",
      "[27,    11] loss: 0.11630, adv_train_accuracy: 96.48, clean_train_accuracy : 69.14\n",
      "[27,    21] loss: 0.12577, adv_train_accuracy: 96.48, clean_train_accuracy : 71.68\n",
      "[27,    31] loss: 0.14362, adv_train_accuracy: 94.53, clean_train_accuracy : 65.62\n",
      "[27,    41] loss: 0.15481, adv_train_accuracy: 95.12, clean_train_accuracy : 67.77\n",
      "[27,    51] loss: 0.18916, adv_train_accuracy: 94.14, clean_train_accuracy : 66.41\n",
      "[27,    61] loss: 0.15503, adv_train_accuracy: 95.31, clean_train_accuracy : 70.31\n",
      "[27,    71] loss: 0.12668, adv_train_accuracy: 96.48, clean_train_accuracy : 68.16\n",
      "[27,    81] loss: 0.10091, adv_train_accuracy: 96.88, clean_train_accuracy : 66.80\n",
      "[27,    91] loss: 0.11051, adv_train_accuracy: 95.90, clean_train_accuracy : 71.09\n",
      "0.019921875\n",
      "0.08671875\n",
      "duration: 193 s - train loss: 0.12314 - train accuracy: 96.02 - validation loss: 1.16133 - validation accuracy: 79.27 \n",
      "[28,     1] loss: 0.14736, adv_train_accuracy: 94.92, clean_train_accuracy : 71.68\n",
      "[28,    11] loss: 0.08917, adv_train_accuracy: 97.07, clean_train_accuracy : 67.19\n",
      "[28,    21] loss: 0.12120, adv_train_accuracy: 95.70, clean_train_accuracy : 71.09\n",
      "[28,    31] loss: 0.09138, adv_train_accuracy: 96.88, clean_train_accuracy : 69.73\n",
      "[28,    41] loss: 0.09841, adv_train_accuracy: 98.05, clean_train_accuracy : 65.82\n",
      "[28,    51] loss: 0.10041, adv_train_accuracy: 95.70, clean_train_accuracy : 67.38\n",
      "[28,    61] loss: 0.11864, adv_train_accuracy: 96.88, clean_train_accuracy : 72.85\n",
      "[28,    71] loss: 0.08289, adv_train_accuracy: 98.05, clean_train_accuracy : 65.82\n",
      "[28,    81] loss: 0.10174, adv_train_accuracy: 95.70, clean_train_accuracy : 69.92\n",
      "[28,    91] loss: 0.05174, adv_train_accuracy: 98.05, clean_train_accuracy : 73.63\n",
      "0.08515625\n",
      "0.12890625\n",
      "duration: 193 s - train loss: 0.11083 - train accuracy: 96.34 - validation loss: 1.03946 - validation accuracy: 80.89 \n",
      "[29,     1] loss: 0.09144, adv_train_accuracy: 97.27, clean_train_accuracy : 71.68\n",
      "[29,    11] loss: 0.11081, adv_train_accuracy: 96.88, clean_train_accuracy : 71.09\n",
      "[29,    21] loss: 0.06954, adv_train_accuracy: 98.24, clean_train_accuracy : 64.84\n",
      "[29,    31] loss: 0.06435, adv_train_accuracy: 98.05, clean_train_accuracy : 66.41\n",
      "[29,    41] loss: 0.13519, adv_train_accuracy: 95.70, clean_train_accuracy : 67.58\n",
      "[29,    51] loss: 0.09350, adv_train_accuracy: 96.68, clean_train_accuracy : 64.84\n",
      "[29,    61] loss: 0.07344, adv_train_accuracy: 97.27, clean_train_accuracy : 67.19\n",
      "[29,    71] loss: 0.11960, adv_train_accuracy: 96.68, clean_train_accuracy : 66.21\n",
      "[29,    81] loss: 0.12556, adv_train_accuracy: 95.31, clean_train_accuracy : 67.97\n",
      "[29,    91] loss: 0.10173, adv_train_accuracy: 96.68, clean_train_accuracy : 67.38\n",
      "0.040234375\n",
      "0.1365234375\n",
      "duration: 192 s - train loss: 0.10222 - train accuracy: 96.68 - validation loss: 1.28193 - validation accuracy: 77.26 \n",
      "[30,     1] loss: 0.12582, adv_train_accuracy: 95.31, clean_train_accuracy : 67.77\n",
      "[30,    11] loss: 0.11007, adv_train_accuracy: 96.29, clean_train_accuracy : 71.09\n",
      "[30,    21] loss: 0.14001, adv_train_accuracy: 95.51, clean_train_accuracy : 70.70\n",
      "[30,    31] loss: 0.06362, adv_train_accuracy: 97.85, clean_train_accuracy : 69.73\n",
      "[30,    41] loss: 0.07029, adv_train_accuracy: 97.27, clean_train_accuracy : 73.05\n",
      "[30,    51] loss: 0.14410, adv_train_accuracy: 96.48, clean_train_accuracy : 67.19\n",
      "[30,    61] loss: 0.09138, adv_train_accuracy: 96.88, clean_train_accuracy : 69.73\n",
      "[30,    71] loss: 0.07422, adv_train_accuracy: 96.88, clean_train_accuracy : 72.66\n",
      "[30,    81] loss: 0.11796, adv_train_accuracy: 96.48, clean_train_accuracy : 70.90\n",
      "[30,    91] loss: 0.08218, adv_train_accuracy: 96.68, clean_train_accuracy : 66.41\n",
      "0.0283203125\n",
      "0.0966796875\n",
      "duration: 192 s - train loss: 0.10247 - train accuracy: 96.63 - validation loss: 1.00651 - validation accuracy: 80.34 \n",
      "[31,     1] loss: 0.09184, adv_train_accuracy: 96.88, clean_train_accuracy : 67.77\n",
      "[31,    11] loss: 0.08315, adv_train_accuracy: 96.68, clean_train_accuracy : 73.24\n",
      "[31,    21] loss: 0.08869, adv_train_accuracy: 97.07, clean_train_accuracy : 67.97\n",
      "[31,    31] loss: 0.08150, adv_train_accuracy: 96.88, clean_train_accuracy : 68.75\n",
      "[31,    41] loss: 0.06634, adv_train_accuracy: 97.85, clean_train_accuracy : 71.29\n",
      "[31,    51] loss: 0.10385, adv_train_accuracy: 96.68, clean_train_accuracy : 67.58\n",
      "[31,    61] loss: 0.10653, adv_train_accuracy: 96.29, clean_train_accuracy : 72.66\n",
      "[31,    71] loss: 0.08879, adv_train_accuracy: 96.09, clean_train_accuracy : 71.09\n",
      "[31,    81] loss: 0.08863, adv_train_accuracy: 96.88, clean_train_accuracy : 67.38\n",
      "[31,    91] loss: 0.06199, adv_train_accuracy: 97.66, clean_train_accuracy : 71.48\n",
      "0.0638671875\n",
      "0.1595703125\n",
      "duration: 193 s - train loss: 0.09728 - train accuracy: 96.80 - validation loss: 1.20033 - validation accuracy: 80.19 \n",
      "[32,     1] loss: 0.11314, adv_train_accuracy: 96.48, clean_train_accuracy : 70.12\n",
      "[32,    11] loss: 0.10063, adv_train_accuracy: 95.90, clean_train_accuracy : 69.34\n",
      "[32,    21] loss: 0.17962, adv_train_accuracy: 95.31, clean_train_accuracy : 66.80\n",
      "[32,    31] loss: 0.09375, adv_train_accuracy: 97.85, clean_train_accuracy : 71.29\n",
      "[32,    41] loss: 0.12186, adv_train_accuracy: 95.90, clean_train_accuracy : 73.05\n",
      "[32,    51] loss: 0.09447, adv_train_accuracy: 97.07, clean_train_accuracy : 71.48\n",
      "[32,    61] loss: 0.14136, adv_train_accuracy: 95.51, clean_train_accuracy : 59.77\n",
      "[32,    71] loss: 0.16507, adv_train_accuracy: 93.36, clean_train_accuracy : 63.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d6eaaadd884a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_stats2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pytorch-network-pruning/src/models.py\u001b[0m in \u001b[0;36mfit_fast\u001b[0;34m(self, train_loader, val_loader, epochs, device, eps, number_of_replays, patience, evaluate_robustness)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_replays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_robustness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fast adversarial training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_fit_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_robustness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_robustness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_fast_with_double_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_replays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_robustness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-network-pruning/src/training.py\u001b[0m in \u001b[0;36m_fit_fast\u001b[0;34m(model, train_loader, val_loader, epochs, device, eps, patience, evaluate_robustness)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mclean_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mclean_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0madv_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-network-pruning/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-network-pruning/src/custom_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m#print('inputs',shortcut.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m#print('after conv 1',x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-network-pruning/src/custom_modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m#reshape the bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_stats2 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-300-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 300,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats3 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-400-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 400,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats4 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-500-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 500,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats5 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-600-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 600,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats3 = model.fit_fast(train_loader, test_loader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved-models/cifar-resnet-fast-700-epochs.pth'\n",
    "torch.save({\n",
    "        'epoch': 700,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 100,\n",
       " 'model_state_dict': OrderedDict([('c1.weights',\n",
       "               tensor([[[[ 5.2705e-02, -3.1870e-02,  1.8360e-02],\n",
       "                         [ 3.0918e-01, -3.1207e-01, -5.9072e-02],\n",
       "                         [ 6.1119e-02, -6.2451e-02,  1.5322e-02]],\n",
       "               \n",
       "                        [[-5.4788e-03, -2.2965e-03,  8.3076e-03],\n",
       "                         [ 5.2809e-02, -4.9895e-02, -1.3867e-03],\n",
       "                         [ 8.3724e-04, -5.3421e-03,  1.0070e-02]],\n",
       "               \n",
       "                        [[-9.4907e-03, -3.1119e-03,  1.9843e-02],\n",
       "                         [ 1.9212e-01, -1.8495e-01, -1.4722e-02],\n",
       "                         [ 6.4316e-03, -3.2145e-02,  1.8856e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.6953e-03,  7.5328e-02,  5.5013e-02],\n",
       "                         [-1.2339e-01, -2.9748e-01, -1.9194e-01],\n",
       "                         [-9.5028e-02, -2.1170e-01, -1.7418e-01]],\n",
       "               \n",
       "                        [[-3.1035e-03,  2.0645e-02,  4.3436e-03],\n",
       "                         [ 5.5452e-02,  6.0133e-02,  3.6876e-02],\n",
       "                         [ 6.8718e-02,  6.3885e-02,  7.8496e-02]],\n",
       "               \n",
       "                        [[-2.0834e-02,  2.3314e-02,  6.5808e-02],\n",
       "                         [ 4.7407e-02, -4.2632e-02,  1.2692e-03],\n",
       "                         [-2.4531e-03,  8.1738e-02,  7.4319e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.6715e-02, -1.2034e-01, -4.7139e-02],\n",
       "                         [-6.8255e-02, -6.2232e-02, -7.2463e-02],\n",
       "                         [-6.0648e-02,  3.0261e-03, -6.3002e-02]],\n",
       "               \n",
       "                        [[-7.0291e-02, -7.7312e-02, -9.1394e-02],\n",
       "                         [-5.0926e-02, -9.1161e-02, -4.0684e-02],\n",
       "                         [-3.3765e-02, -2.3188e-02, -3.6185e-02]],\n",
       "               \n",
       "                        [[-1.1936e-01, -8.7445e-02, -2.0942e-01],\n",
       "                         [-9.0948e-02, -1.5259e-01, -1.5694e-01],\n",
       "                         [-6.1760e-02, -1.6654e-02, -5.8290e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.0652e-01,  1.2354e-01,  6.3562e-02],\n",
       "                         [-2.0216e-01, -1.0634e-01,  2.0673e-03],\n",
       "                         [-8.5127e-02, -7.9258e-02,  4.3428e-02]],\n",
       "               \n",
       "                        [[ 1.3833e-02,  2.0761e-02, -1.1746e-02],\n",
       "                         [-1.5877e-02, -2.2171e-02,  8.1341e-04],\n",
       "                         [ 6.3433e-03,  7.6239e-03,  8.0818e-03]],\n",
       "               \n",
       "                        [[-1.8748e-02, -1.9224e-02, -6.6505e-02],\n",
       "                         [-9.3051e-03, -3.2394e-02, -3.8184e-02],\n",
       "                         [ 1.0112e-01,  3.3454e-02,  4.1951e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.4736e-02,  7.3291e-02,  5.7752e-03],\n",
       "                         [-6.8800e-02, -1.9499e-02, -9.0259e-02],\n",
       "                         [-1.0017e-02,  6.1916e-02,  1.5214e-05]],\n",
       "               \n",
       "                        [[ 4.1135e-02,  1.9830e-02,  1.0661e-01],\n",
       "                         [-1.0898e-01, -4.8078e-02,  2.7785e-02],\n",
       "                         [-2.7336e-02,  7.8396e-03,  3.1876e-02]],\n",
       "               \n",
       "                        [[-1.1169e-02, -1.7411e-01,  3.1565e-02],\n",
       "                         [-6.0966e-02, -2.1609e-01, -8.4083e-02],\n",
       "                         [ 1.5798e-02, -2.6543e-02,  4.7669e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.7374e-02, -7.5443e-02, -1.2844e-01],\n",
       "                         [-3.1091e-02, -2.3226e-01, -2.2640e-01],\n",
       "                         [ 1.6277e-02, -1.6385e-02, -1.1486e-01]],\n",
       "               \n",
       "                        [[-5.1189e-02,  4.9168e-02,  4.2002e-02],\n",
       "                         [ 1.2566e-03,  5.6985e-02, -5.2265e-02],\n",
       "                         [ 7.4671e-03,  3.6676e-02, -3.2859e-03]],\n",
       "               \n",
       "                        [[-1.5516e-02, -4.7016e-02,  6.8654e-02],\n",
       "                         [ 1.2133e-01,  1.1883e-01, -4.9112e-02],\n",
       "                         [ 9.6796e-03, -7.5185e-02, -5.9569e-02]]]], device='cuda:0')),\n",
       "              ('c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r1.bn1.weight',\n",
       "               tensor([1.2816, 0.9722, 0.9185, 0.8831, 0.9738, 0.9756, 1.0276, 0.9956, 0.9547,\n",
       "                       0.9535, 0.9362, 0.9839, 0.9189, 0.9759, 0.9174, 1.0161, 0.9902, 0.9005,\n",
       "                       0.8566, 1.0383, 0.8998, 1.0012, 0.9617, 0.9269, 0.8817, 0.9587, 1.1377,\n",
       "                       1.0516, 1.0207, 0.9414, 1.0388, 1.0688, 0.9530, 0.9784, 0.9943, 1.0619,\n",
       "                       1.1273, 0.8785, 0.8874, 0.8907, 0.9905, 0.9749, 1.0267, 1.0174, 0.9501,\n",
       "                       0.9275, 0.8973, 1.0621, 1.0043, 1.0094, 1.4511, 0.9265, 0.9312, 1.0034,\n",
       "                       1.0100, 0.8945, 1.0179, 0.8915, 1.0124, 0.9312, 1.0339, 0.9330, 0.9059,\n",
       "                       0.9849], device='cuda:0')),\n",
       "              ('r1.bn1.bias',\n",
       "               tensor([-0.1799, -0.0117, -0.4129, -0.1016, -0.2002,  0.0511,  0.0457, -0.0642,\n",
       "                        0.0777, -0.1643, -0.3819, -0.3469,  0.0614, -0.2536, -0.0424, -0.2230,\n",
       "                       -0.1524,  0.1284, -0.0415, -0.0307, -0.1795, -0.0625, -0.2598, -0.3681,\n",
       "                        0.1093, -0.1401, -0.1025,  0.2111, -0.2481,  0.0367,  0.0044, -0.0458,\n",
       "                        0.0078,  0.0271, -0.2102,  0.0918,  0.0672,  0.3272, -0.0448, -0.1178,\n",
       "                        0.2067, -0.0918,  0.2825, -0.0241, -0.1208,  0.2503, -0.1158,  0.0136,\n",
       "                        0.0231, -0.0916, -0.0098,  0.1175, -0.0936, -0.0102, -0.2116,  0.5472,\n",
       "                        0.0628, -0.1895, -0.0248, -0.3414, -0.1236, -0.1754, -0.1194,  0.0023],\n",
       "                      device='cuda:0')),\n",
       "              ('r1.bn1.running_mean',\n",
       "               tensor([-1.5990e-02, -1.8685e-01, -8.4871e-01, -1.7486e-01, -1.7429e-01,\n",
       "                       -1.8308e-01, -5.3756e-01,  3.1323e-01,  2.3984e-01, -1.2570e-01,\n",
       "                       -4.1279e-01, -1.7986e-01,  6.4399e-02, -3.3730e-01, -6.0625e-02,\n",
       "                        1.6739e-01,  6.1147e-01,  1.3844e-01, -5.9776e-02, -3.4410e-02,\n",
       "                       -8.8423e-02,  4.9020e-01, -3.7647e-01, -7.6609e-01,  3.8268e-01,\n",
       "                       -2.1518e-01, -1.9748e-02,  3.8342e-02, -2.8622e-04, -2.9263e-01,\n",
       "                        4.5335e-01, -4.8064e-01, -2.9651e-01,  3.8956e-01, -3.9310e-01,\n",
       "                        2.2992e-01,  7.0902e-02,  9.6480e-02, -2.0738e-01, -1.6449e-01,\n",
       "                       -6.7995e-02, -2.0662e-01,  1.4089e-01,  5.3087e-01,  1.0223e-02,\n",
       "                        2.1564e-01, -3.0257e-01,  6.1714e-01, -2.7491e-01,  5.7097e-01,\n",
       "                        5.6819e-02, -1.5864e-01, -4.9237e-01, -2.9438e-01, -4.7367e-01,\n",
       "                        1.6954e-01, -6.0705e-01,  2.8033e-01,  4.7290e-01, -1.1350e-02,\n",
       "                        6.3571e-01, -1.3933e-02, -2.4139e-01, -3.3841e-01], device='cuda:0')),\n",
       "              ('r1.bn1.running_var',\n",
       "               tensor([0.0047, 0.0159, 0.1776, 0.0114, 0.0112, 0.0108, 0.0701, 0.0274, 0.0199,\n",
       "                       0.0096, 0.0417, 0.0135, 0.0134, 0.0309, 0.0142, 0.0116, 0.0940, 0.0079,\n",
       "                       0.0076, 0.0030, 0.0047, 0.0546, 0.0380, 0.1404, 0.0363, 0.0127, 0.0069,\n",
       "                       0.0078, 0.0016, 0.0220, 0.0616, 0.0526, 0.0277, 0.0375, 0.0386, 0.0261,\n",
       "                       0.0036, 0.0082, 0.0168, 0.0072, 0.0040, 0.0203, 0.0175, 0.0684, 0.0144,\n",
       "                       0.0286, 0.0223, 0.1011, 0.0178, 0.0745, 0.0056, 0.0085, 0.0734, 0.0210,\n",
       "                       0.0518, 0.0237, 0.0851, 0.0196, 0.0559, 0.0033, 0.0930, 0.0016, 0.0175,\n",
       "                       0.0297], device='cuda:0')),\n",
       "              ('r1.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r1.c1.weights',\n",
       "               tensor([[[[-0.0244,  0.0230,  0.0096],\n",
       "                         [-0.0145, -0.0636, -0.0576],\n",
       "                         [-0.0273,  0.0214, -0.0340]],\n",
       "               \n",
       "                        [[ 0.0369,  0.0488,  0.0765],\n",
       "                         [ 0.0109,  0.0162,  0.1055],\n",
       "                         [ 0.1131,  0.0671,  0.0868]],\n",
       "               \n",
       "                        [[-0.0370,  0.0753, -0.0203],\n",
       "                         [-0.0568, -0.0109,  0.0366],\n",
       "                         [ 0.0016, -0.0574, -0.0278]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0052, -0.0172, -0.0604],\n",
       "                         [ 0.0345,  0.0841,  0.0369],\n",
       "                         [-0.0022,  0.0024,  0.0167]],\n",
       "               \n",
       "                        [[ 0.0049,  0.0052,  0.0266],\n",
       "                         [-0.0120, -0.0592, -0.0064],\n",
       "                         [-0.0801,  0.0097, -0.0583]],\n",
       "               \n",
       "                        [[-0.0009, -0.0050,  0.0775],\n",
       "                         [-0.0364,  0.0817,  0.0630],\n",
       "                         [ 0.0267,  0.0363,  0.1182]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0884,  0.0791,  0.0189],\n",
       "                         [-0.0478, -0.0679, -0.0497],\n",
       "                         [-0.0168, -0.1348, -0.1135]],\n",
       "               \n",
       "                        [[ 0.0283, -0.0359, -0.0255],\n",
       "                         [-0.0233,  0.0243, -0.0712],\n",
       "                         [-0.0350,  0.0294,  0.0073]],\n",
       "               \n",
       "                        [[ 0.0037, -0.1038, -0.0462],\n",
       "                         [-0.1532, -0.0785, -0.0198],\n",
       "                         [-0.0494, -0.1702, -0.1445]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0186,  0.0228,  0.0174],\n",
       "                         [-0.0206, -0.0720, -0.0220],\n",
       "                         [-0.0461, -0.0111, -0.0249]],\n",
       "               \n",
       "                        [[-0.0156, -0.0137, -0.0006],\n",
       "                         [-0.0856, -0.1568, -0.0819],\n",
       "                         [-0.0665, -0.0863, -0.1201]],\n",
       "               \n",
       "                        [[ 0.0502,  0.0078,  0.0547],\n",
       "                         [-0.0920, -0.1201, -0.1141],\n",
       "                         [-0.0423, -0.0958, -0.0882]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0011,  0.0316, -0.0137],\n",
       "                         [ 0.0609, -0.0181, -0.0040],\n",
       "                         [-0.0056, -0.0078,  0.0202]],\n",
       "               \n",
       "                        [[-0.0839,  0.0115, -0.0176],\n",
       "                         [-0.0891,  0.0084,  0.0037],\n",
       "                         [ 0.0062,  0.0254,  0.0753]],\n",
       "               \n",
       "                        [[-0.1765, -0.1331, -0.0763],\n",
       "                         [-0.2028, -0.1273, -0.0750],\n",
       "                         [-0.1421, -0.1130, -0.1398]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0101,  0.0529,  0.0814],\n",
       "                         [ 0.0517,  0.0295,  0.0757],\n",
       "                         [ 0.0223,  0.0831,  0.0703]],\n",
       "               \n",
       "                        [[ 0.0058, -0.0051, -0.0796],\n",
       "                         [-0.0594, -0.1557, -0.1152],\n",
       "                         [-0.0824, -0.0542, -0.0163]],\n",
       "               \n",
       "                        [[-0.0833, -0.0149, -0.0034],\n",
       "                         [-0.0475, -0.0405, -0.0315],\n",
       "                         [-0.0816, -0.0771, -0.0464]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0828, -0.2050, -0.3043],\n",
       "                         [-0.1883, -0.4100, -0.3408],\n",
       "                         [-0.2071, -0.2686, -0.2169]],\n",
       "               \n",
       "                        [[-0.0577, -0.0065, -0.0616],\n",
       "                         [ 0.0248, -0.0255, -0.0055],\n",
       "                         [ 0.1031,  0.0423,  0.0331]],\n",
       "               \n",
       "                        [[-0.2832, -0.2188, -0.1684],\n",
       "                         [-0.2489, -0.1398, -0.2127],\n",
       "                         [-0.2302, -0.2281, -0.1792]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1371, -0.0727, -0.1127],\n",
       "                         [-0.1718, -0.0883, -0.1466],\n",
       "                         [-0.1309, -0.1464, -0.1704]],\n",
       "               \n",
       "                        [[-0.1856, -0.1900, -0.2600],\n",
       "                         [-0.1761, -0.1212, -0.1697],\n",
       "                         [-0.1461, -0.1846, -0.2253]],\n",
       "               \n",
       "                        [[-0.1185, -0.0672, -0.0570],\n",
       "                         [-0.0788, -0.0485, -0.0573],\n",
       "                         [-0.0511, -0.0418, -0.0319]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0105,  0.0386,  0.0029],\n",
       "                         [-0.0692, -0.0830, -0.0487],\n",
       "                         [-0.0220, -0.0568,  0.0152]],\n",
       "               \n",
       "                        [[ 0.0021,  0.0800, -0.0270],\n",
       "                         [ 0.0693,  0.0629,  0.0550],\n",
       "                         [ 0.0363,  0.0546,  0.0153]],\n",
       "               \n",
       "                        [[-0.0817, -0.0914, -0.0036],\n",
       "                         [-0.1161, -0.0788, -0.0192],\n",
       "                         [-0.0539, -0.0620, -0.0345]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0387, -0.0225, -0.0226],\n",
       "                         [ 0.0082, -0.0586,  0.0493],\n",
       "                         [ 0.0821,  0.0019,  0.0447]],\n",
       "               \n",
       "                        [[ 0.0084,  0.0269, -0.0370],\n",
       "                         [ 0.0356,  0.0245, -0.0616],\n",
       "                         [ 0.0930, -0.0322,  0.0250]],\n",
       "               \n",
       "                        [[-0.0308,  0.1018,  0.0324],\n",
       "                         [-0.0155,  0.0733,  0.0472],\n",
       "                         [ 0.0304, -0.0370, -0.0384]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0329,  0.0292, -0.0030],\n",
       "                         [ 0.0085, -0.0308,  0.0488],\n",
       "                         [ 0.0074, -0.0161, -0.0039]],\n",
       "               \n",
       "                        [[ 0.0213,  0.0394,  0.0343],\n",
       "                         [ 0.1261,  0.0944,  0.0477],\n",
       "                         [ 0.0132, -0.0092, -0.0609]],\n",
       "               \n",
       "                        [[-0.0148,  0.0853,  0.0184],\n",
       "                         [ 0.0056,  0.0304,  0.0064],\n",
       "                         [-0.0529, -0.0838, -0.0881]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0458,  0.0279,  0.0593],\n",
       "                         [ 0.0517,  0.1177,  0.1205],\n",
       "                         [ 0.0111,  0.0149,  0.0720]],\n",
       "               \n",
       "                        [[ 0.0276,  0.0452, -0.0129],\n",
       "                         [ 0.0641,  0.0440, -0.0606],\n",
       "                         [-0.0686, -0.0933, -0.0619]],\n",
       "               \n",
       "                        [[ 0.0351,  0.0299, -0.0519],\n",
       "                         [-0.0475,  0.0225,  0.0669],\n",
       "                         [ 0.0142, -0.0114,  0.0499]]]], device='cuda:0')),\n",
       "              ('r1.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r1.bn2.weight',\n",
       "               tensor([0.6421, 0.6307, 0.6430, 0.5615, 0.7064, 0.7195, 0.7079, 0.5936, 0.7788,\n",
       "                       0.7056, 0.6541, 0.6398, 0.6845, 0.6527, 0.6449, 0.7349, 0.6296, 0.7195,\n",
       "                       0.6501, 0.6047, 0.6495, 0.6440, 0.6460, 0.6579, 0.7057, 0.6121, 0.6277,\n",
       "                       0.7190, 0.7664, 0.6631, 0.5338, 0.7008, 0.5061, 0.7908, 0.6278, 0.7046,\n",
       "                       0.6640, 0.7330, 0.7303, 0.7178, 0.6991, 0.7971, 0.6111, 0.6260, 0.7381,\n",
       "                       0.6263, 0.6290, 0.7574, 0.7344, 0.7580, 0.6800, 0.6120, 0.7173, 0.7292,\n",
       "                       0.6991, 0.5265, 0.7181, 0.6431, 0.6162, 0.6379, 0.6131, 0.6621, 0.7783,\n",
       "                       0.7222], device='cuda:0')),\n",
       "              ('r1.bn2.bias',\n",
       "               tensor([-0.4117, -0.3060, -0.2718, -0.5838, -0.2783, -0.1862, -0.1393, -0.3494,\n",
       "                       -0.0812, -0.0439, -0.0215, -0.3270, -0.2008, -0.1714, -0.3010, -0.3097,\n",
       "                       -0.2248, -0.1409,  0.0685, -0.3300, -0.0761, -0.2091, -0.3446, -0.2100,\n",
       "                       -0.1884, -0.1495, -0.1123, -0.2103, -0.3477, -0.2370, -0.4429, -0.2258,\n",
       "                       -0.0900, -0.3224, -0.1945,  0.0835, -0.2083, -0.2517, -0.2275, -0.0913,\n",
       "                       -0.0451, -0.0609, -0.0862, -0.2413,  0.0955, -0.2247, -0.0917, -0.1986,\n",
       "                       -0.4465,  0.0253, -0.3569, -0.0757, -0.2985,  0.0248, -0.0797, -0.0646,\n",
       "                       -0.0424, -0.3897, -0.0286, -0.2087, -0.1244, -0.0259, -0.1751, -0.3304],\n",
       "                      device='cuda:0')),\n",
       "              ('r1.bn2.running_mean',\n",
       "               tensor([ 2.1748e+00, -4.0289e+00, -2.2909e+00,  1.7528e+00,  8.5807e-01,\n",
       "                       -1.4529e+00, -2.3492e+00, -3.2950e+00, -4.8862e+00, -2.7734e+00,\n",
       "                       -5.1059e+00, -3.1792e+00,  1.9410e+00, -2.4584e+00, -6.2566e-01,\n",
       "                       -1.1000e+00,  4.6842e+00,  5.4098e-01, -4.8507e+00, -1.7185e+00,\n",
       "                       -4.7726e+00, -4.1611e+00, -4.6159e-01,  5.8768e+00, -2.3797e+00,\n",
       "                       -5.6956e+00, -3.3117e+00, -3.3250e+00, -5.9743e-01, -4.4248e-01,\n",
       "                        5.4062e+00, -3.2553e+00, -5.5631e-01,  1.6568e+00,  4.8949e-03,\n",
       "                       -4.8916e+00,  4.3425e+00, -1.6965e+00, -6.7354e-01, -2.0231e+00,\n",
       "                       -4.9632e+00,  2.8032e+00, -2.0236e+00, -2.8016e+00, -3.8855e+00,\n",
       "                       -4.3836e+00, -2.4680e+00, -2.6499e+00,  1.1143e+00, -7.4613e+00,\n",
       "                        9.1608e-01, -4.8670e+00, -7.9766e-01, -3.4269e+00, -6.4280e+00,\n",
       "                       -7.6390e-01, -6.6748e+00,  4.5437e-01, -4.2666e+00, -2.1764e+00,\n",
       "                       -2.0881e+00, -5.8248e+00,  2.2961e+00, -2.5660e+00], device='cuda:0')),\n",
       "              ('r1.bn2.running_var',\n",
       "               tensor([ 5.7744, 46.1659, 43.3529, 20.3697, 32.3890, 52.6635,  8.8112, 15.8051,\n",
       "                       39.3418, 30.4802, 26.5360, 24.1386, 49.8437, 39.4710, 33.2021, 14.7258,\n",
       "                       14.5531, 36.4728, 38.8579, 22.1330, 31.6740, 53.6944, 10.5319, 19.2525,\n",
       "                       56.1377, 62.4168, 18.4240, 28.7230,  1.4243,  7.0106,  6.7167, 51.0463,\n",
       "                        2.8007, 11.6663,  1.4059, 39.6114, 14.1855, 23.1205, 18.5247, 10.3487,\n",
       "                       39.1799, 19.4201, 19.3473, 24.4886, 29.7191, 40.4125, 35.6972, 27.7604,\n",
       "                       16.6812, 27.5851, 32.8304, 17.4170,  9.9143, 27.2875, 22.3744,  7.6209,\n",
       "                       46.9026,  4.2314, 24.7127, 10.2679, 17.2027, 38.3407, 32.7388, 28.5889],\n",
       "                      device='cuda:0')),\n",
       "              ('r1.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r1.c2.weights',\n",
       "               tensor([[[[-0.0128, -0.0077, -0.0348],\n",
       "                         [-0.0143,  0.0139,  0.0023],\n",
       "                         [-0.1077, -0.1096, -0.0826]],\n",
       "               \n",
       "                        [[ 0.0273, -0.0761,  0.0223],\n",
       "                         [ 0.0696,  0.0027,  0.0096],\n",
       "                         [-0.0073, -0.0634, -0.0117]],\n",
       "               \n",
       "                        [[ 0.0708,  0.0246,  0.0444],\n",
       "                         [ 0.0134, -0.1018,  0.0460],\n",
       "                         [-0.0202,  0.0097, -0.0171]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0241,  0.0598,  0.0460],\n",
       "                         [ 0.0203, -0.0343,  0.0081],\n",
       "                         [-0.0548,  0.0069,  0.0164]],\n",
       "               \n",
       "                        [[-0.0199, -0.0025, -0.0255],\n",
       "                         [-0.0822, -0.0170, -0.0240],\n",
       "                         [-0.0563, -0.0337, -0.0373]],\n",
       "               \n",
       "                        [[ 0.0026, -0.0823, -0.0233],\n",
       "                         [-0.0342,  0.0048, -0.0355],\n",
       "                         [-0.0219, -0.1002, -0.0859]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1023,  0.0260,  0.0345],\n",
       "                         [ 0.0054,  0.0549,  0.0027],\n",
       "                         [-0.0276, -0.0123,  0.0422]],\n",
       "               \n",
       "                        [[ 0.0955, -0.0291,  0.0110],\n",
       "                         [ 0.0706,  0.0390,  0.0334],\n",
       "                         [-0.0043, -0.0029,  0.0612]],\n",
       "               \n",
       "                        [[ 0.0472,  0.0175, -0.0111],\n",
       "                         [-0.0193,  0.0235,  0.0572],\n",
       "                         [ 0.1089,  0.0843,  0.0636]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0633,  0.0606,  0.0441],\n",
       "                         [ 0.0671, -0.0271,  0.0563],\n",
       "                         [-0.0111,  0.0885, -0.0708]],\n",
       "               \n",
       "                        [[ 0.0493, -0.0309, -0.0127],\n",
       "                         [-0.0396, -0.0043, -0.0380],\n",
       "                         [-0.0209, -0.0778, -0.0034]],\n",
       "               \n",
       "                        [[ 0.0294,  0.0035,  0.0504],\n",
       "                         [ 0.0128,  0.0915, -0.0039],\n",
       "                         [ 0.0841,  0.0384, -0.0164]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0090, -0.0405, -0.0159],\n",
       "                         [ 0.0047,  0.0093,  0.0178],\n",
       "                         [-0.0209,  0.0095, -0.0481]],\n",
       "               \n",
       "                        [[ 0.0754,  0.0455,  0.0827],\n",
       "                         [ 0.0860,  0.0297,  0.0197],\n",
       "                         [ 0.0864,  0.1351,  0.1094]],\n",
       "               \n",
       "                        [[-0.0735, -0.0188,  0.0216],\n",
       "                         [ 0.0341, -0.0489, -0.0630],\n",
       "                         [ 0.0267, -0.0324, -0.0213]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0715, -0.0507, -0.0280],\n",
       "                         [-0.0565, -0.0201, -0.0332],\n",
       "                         [-0.0079, -0.0355, -0.0601]],\n",
       "               \n",
       "                        [[-0.0396, -0.0854, -0.1244],\n",
       "                         [-0.0510, -0.0562, -0.0554],\n",
       "                         [-0.1015, -0.1100, -0.0409]],\n",
       "               \n",
       "                        [[-0.0100,  0.0581, -0.0207],\n",
       "                         [-0.0136,  0.0615,  0.0370],\n",
       "                         [ 0.0080,  0.0744,  0.0470]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0489, -0.0506,  0.0291],\n",
       "                         [ 0.0533,  0.0184, -0.0263],\n",
       "                         [ 0.0233,  0.0402,  0.0310]],\n",
       "               \n",
       "                        [[ 0.0338, -0.0640, -0.0155],\n",
       "                         [-0.0429,  0.0573,  0.0126],\n",
       "                         [ 0.0678, -0.0177,  0.0218]],\n",
       "               \n",
       "                        [[ 0.0051,  0.0537,  0.0248],\n",
       "                         [-0.0276, -0.0497, -0.0238],\n",
       "                         [ 0.0140,  0.0510,  0.0707]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0104,  0.0720,  0.0643],\n",
       "                         [ 0.0109,  0.0114,  0.0284],\n",
       "                         [ 0.1033,  0.0137,  0.0570]],\n",
       "               \n",
       "                        [[ 0.0321,  0.0324, -0.0529],\n",
       "                         [ 0.0173, -0.0448, -0.0164],\n",
       "                         [ 0.0319,  0.0289,  0.0168]],\n",
       "               \n",
       "                        [[-0.0367, -0.0236,  0.0030],\n",
       "                         [-0.0141, -0.0497, -0.0409],\n",
       "                         [-0.0689, -0.0284, -0.0060]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0693,  0.1029,  0.0487],\n",
       "                         [ 0.0793,  0.1018,  0.0240],\n",
       "                         [ 0.0161,  0.0845, -0.0345]],\n",
       "               \n",
       "                        [[ 0.0727,  0.0179, -0.0818],\n",
       "                         [ 0.0100, -0.0097, -0.0974],\n",
       "                         [-0.0034, -0.0085, -0.1057]],\n",
       "               \n",
       "                        [[ 0.0437, -0.0506, -0.0156],\n",
       "                         [-0.0334,  0.0406, -0.0247],\n",
       "                         [-0.0405,  0.0013,  0.0325]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0584, -0.0137,  0.0143],\n",
       "                         [ 0.0472, -0.0420, -0.0196],\n",
       "                         [-0.0082,  0.0142,  0.0576]],\n",
       "               \n",
       "                        [[ 0.0763, -0.0010, -0.0625],\n",
       "                         [-0.0230, -0.0262,  0.0454],\n",
       "                         [ 0.0825, -0.0296,  0.0353]],\n",
       "               \n",
       "                        [[-0.0076, -0.0530, -0.0533],\n",
       "                         [ 0.0208, -0.0347, -0.0495],\n",
       "                         [-0.0899,  0.0002, -0.0665]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0156,  0.0755, -0.0542],\n",
       "                         [ 0.0347, -0.0241,  0.0733],\n",
       "                         [ 0.0070,  0.0236, -0.0490]],\n",
       "               \n",
       "                        [[-0.0291,  0.0369,  0.0512],\n",
       "                         [ 0.1012,  0.0379,  0.0324],\n",
       "                         [ 0.0280,  0.0501, -0.0025]],\n",
       "               \n",
       "                        [[-0.0039,  0.0935, -0.0043],\n",
       "                         [ 0.0236,  0.0796, -0.0218],\n",
       "                         [ 0.0484, -0.0093, -0.0425]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0019, -0.0334, -0.0967],\n",
       "                         [-0.0203,  0.1296, -0.0133],\n",
       "                         [-0.0235,  0.1008,  0.0143]],\n",
       "               \n",
       "                        [[ 0.0943,  0.0094,  0.0308],\n",
       "                         [ 0.0169,  0.0550,  0.0480],\n",
       "                         [-0.0016,  0.0709,  0.0285]],\n",
       "               \n",
       "                        [[ 0.0285, -0.1021, -0.0206],\n",
       "                         [-0.0222, -0.0835, -0.0348],\n",
       "                         [ 0.0202, -0.0805,  0.0417]]]], device='cuda:0')),\n",
       "              ('r1.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r2.bn1.weight',\n",
       "               tensor([0.9315, 1.0611, 1.0158, 1.0677, 1.0694, 0.9943, 0.9060, 0.9760, 1.0775,\n",
       "                       0.9218, 1.0171, 1.0093, 0.8717, 0.8451, 1.0122, 0.9836, 0.8981, 1.0790,\n",
       "                       1.0257, 0.8974, 0.9769, 0.9149, 0.9472, 0.9250, 1.1417, 0.9260, 1.0213,\n",
       "                       0.9554, 0.9431, 1.1184, 0.9678, 1.0684, 1.3603, 0.9962, 0.8848, 0.9013,\n",
       "                       0.9291, 0.9939, 0.9528, 0.9654, 0.9247, 0.9459, 0.9050, 0.9289, 1.0468,\n",
       "                       0.9970, 0.9852, 1.0697, 1.0014, 0.9163, 0.9655, 1.0061, 0.9018, 0.9780,\n",
       "                       0.9832, 0.9287, 0.9578, 0.9552, 0.9671, 0.9986, 0.9753, 0.9973, 0.9984,\n",
       "                       1.0604], device='cuda:0')),\n",
       "              ('r2.bn1.bias',\n",
       "               tensor([ 0.0508, -0.1367,  0.1436,  0.4895, -0.0166, -0.1245, -0.3272,  0.1330,\n",
       "                       -0.1329,  0.1207, -0.2124,  0.3404, -0.5220, -0.2747,  0.1210,  0.0627,\n",
       "                       -0.2469, -0.2933, -0.1272, -0.0060,  0.1244, -0.1433, -0.3041, -0.1642,\n",
       "                        0.1208, -0.0180, -0.2471, -0.3627, -0.5225, -0.0246,  0.1415,  0.2485,\n",
       "                        0.0175, -0.2060,  0.2310, -0.1014,  0.2431,  0.1906, -0.3023,  0.0395,\n",
       "                       -0.0042, -0.0357, -0.1169, -0.3481,  0.0530,  0.0768, -0.0663, -0.1854,\n",
       "                       -0.4063, -0.2327,  0.1266,  0.2532, -0.1863, -0.0434, -0.0759, -0.0987,\n",
       "                        0.0691, -0.3148, -0.3407, -0.1610, -0.4931,  0.0511, -0.5535, -0.0992],\n",
       "                      device='cuda:0')),\n",
       "              ('r2.bn1.running_mean',\n",
       "               tensor([-0.4216, -0.1822, -0.5886, -0.7755,  0.1992, -0.5908, -0.5837, -0.3429,\n",
       "                        0.2739, -0.2332, -0.1786, -0.6781, -0.0843, -0.7679,  0.3356, -0.3323,\n",
       "                        0.7718,  0.6850,  0.5751, -0.3880, -0.4360, -0.3424, -0.8021, -0.3389,\n",
       "                        0.4091, -0.2847,  1.0234,  0.5868,  0.6867,  0.2234, -0.4230, -0.7441,\n",
       "                       -0.1754,  0.1760, -0.7361, -0.1496, -0.5222, -0.4932,  0.0166, -0.4265,\n",
       "                       -0.4735, -0.6368,  0.1453,  0.0183,  0.0363, -0.1983, -0.3368,  1.0638,\n",
       "                       -0.0617, -0.0695, -0.5666, -0.0257, -0.8958, -0.1571, -0.0614,  0.5688,\n",
       "                       -0.7077,  0.2427,  0.3985,  0.1156, -0.1574,  0.0487,  0.0803,  0.1361],\n",
       "                      device='cuda:0')),\n",
       "              ('r2.bn1.running_var',\n",
       "               tensor([0.3018, 0.5362, 3.9534, 1.2075, 1.1218, 0.6933, 0.3778, 1.2308, 0.7697,\n",
       "                       0.4837, 0.3667, 1.1032, 0.6273, 1.0318, 1.1967, 1.3184, 0.7734, 0.7856,\n",
       "                       0.4360, 0.5933, 0.7224, 0.7647, 0.7668, 2.0429, 0.7910, 0.8403, 0.6454,\n",
       "                       0.4383, 0.3859, 0.5621, 0.4683, 0.4103, 0.6505, 0.7360, 0.6011, 0.0691,\n",
       "                       0.6722, 1.2543, 0.3555, 0.5523, 0.7569, 0.5913, 0.4618, 0.8862, 1.1782,\n",
       "                       0.7679, 0.6720, 0.6578, 3.2565, 0.2189, 0.5604, 0.5608, 0.4571, 1.2315,\n",
       "                       0.9152, 0.9704, 0.6181, 0.9853, 0.3165, 1.0569, 0.8873, 0.8551, 0.2161,\n",
       "                       0.8076], device='cuda:0')),\n",
       "              ('r2.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r2.c1.weights',\n",
       "               tensor([[[[-0.0641, -0.0755, -0.0464],\n",
       "                         [ 0.0404, -0.0354, -0.1046],\n",
       "                         [ 0.0736, -0.0605, -0.1179]],\n",
       "               \n",
       "                        [[-0.0393,  0.0158,  0.0414],\n",
       "                         [ 0.0475,  0.0810,  0.0746],\n",
       "                         [-0.0016, -0.0302,  0.0122]],\n",
       "               \n",
       "                        [[-0.0406, -0.1485, -0.0274],\n",
       "                         [-0.1116, -0.1381, -0.1040],\n",
       "                         [-0.0247, -0.0537,  0.0065]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0139, -0.0188, -0.0312],\n",
       "                         [ 0.0157, -0.0243,  0.0307],\n",
       "                         [-0.0461, -0.0257, -0.0715]],\n",
       "               \n",
       "                        [[ 0.0228, -0.0532, -0.0724],\n",
       "                         [-0.0375, -0.0474, -0.1324],\n",
       "                         [-0.0188, -0.0037,  0.0071]],\n",
       "               \n",
       "                        [[-0.0549, -0.0374,  0.0013],\n",
       "                         [-0.0353, -0.0363, -0.0365],\n",
       "                         [ 0.0333, -0.0361, -0.0357]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0420, -0.0192, -0.0156],\n",
       "                         [-0.0763, -0.0563, -0.0355],\n",
       "                         [-0.0322, -0.0499,  0.0213]],\n",
       "               \n",
       "                        [[-0.0626, -0.0565,  0.0193],\n",
       "                         [ 0.0014,  0.0463,  0.0751],\n",
       "                         [ 0.1392,  0.1212,  0.1750]],\n",
       "               \n",
       "                        [[ 0.0787, -0.0738, -0.0243],\n",
       "                         [ 0.0279, -0.0913, -0.1165],\n",
       "                         [-0.0064,  0.0407,  0.0235]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0297,  0.0232, -0.0278],\n",
       "                         [-0.0493,  0.0327,  0.0326],\n",
       "                         [ 0.0590,  0.1593,  0.1019]],\n",
       "               \n",
       "                        [[-0.1315, -0.0915, -0.0952],\n",
       "                         [-0.1860, -0.1180, -0.1301],\n",
       "                         [-0.0343, -0.1293, -0.1506]],\n",
       "               \n",
       "                        [[ 0.0673,  0.0013,  0.0711],\n",
       "                         [ 0.0803,  0.0206, -0.0022],\n",
       "                         [-0.0445, -0.0446, -0.1229]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0061,  0.1242,  0.0578],\n",
       "                         [ 0.0055, -0.0115,  0.0991],\n",
       "                         [ 0.0101, -0.0183,  0.1107]],\n",
       "               \n",
       "                        [[ 0.0004,  0.1428,  0.1654],\n",
       "                         [ 0.1270,  0.1451,  0.0683],\n",
       "                         [ 0.0613,  0.0931,  0.0980]],\n",
       "               \n",
       "                        [[-0.2047, -0.1781, -0.0993],\n",
       "                         [-0.1949, -0.1470, -0.1087],\n",
       "                         [-0.0782, -0.0137, -0.1221]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0848,  0.0453, -0.0456],\n",
       "                         [-0.0670,  0.0263,  0.0079],\n",
       "                         [-0.0217, -0.0233,  0.0292]],\n",
       "               \n",
       "                        [[-0.0272, -0.0201,  0.0684],\n",
       "                         [ 0.0675,  0.0405,  0.0485],\n",
       "                         [-0.0495,  0.0918,  0.0643]],\n",
       "               \n",
       "                        [[ 0.0741,  0.0459,  0.0879],\n",
       "                         [ 0.0698, -0.0305, -0.0165],\n",
       "                         [ 0.0693,  0.0719, -0.0167]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0726, -0.0383, -0.0811],\n",
       "                         [-0.0225, -0.0448,  0.0104],\n",
       "                         [-0.0649,  0.0127,  0.0507]],\n",
       "               \n",
       "                        [[-0.1293, -0.0711, -0.1793],\n",
       "                         [-0.1526, -0.1422, -0.1689],\n",
       "                         [-0.1588, -0.1465, -0.1388]],\n",
       "               \n",
       "                        [[-0.0548, -0.0542,  0.0422],\n",
       "                         [ 0.0358,  0.0656, -0.0487],\n",
       "                         [-0.0594,  0.0093, -0.0576]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1153, -0.0597, -0.0874],\n",
       "                         [-0.0682, -0.0117, -0.1241],\n",
       "                         [-0.0598, -0.0639, -0.0679]],\n",
       "               \n",
       "                        [[ 0.0948,  0.0037, -0.0564],\n",
       "                         [ 0.0130, -0.0141, -0.0311],\n",
       "                         [-0.0560,  0.0531,  0.0616]],\n",
       "               \n",
       "                        [[-0.0062, -0.0328, -0.0262],\n",
       "                         [ 0.0454,  0.0303, -0.0595],\n",
       "                         [ 0.0525,  0.0586, -0.0637]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0272,  0.0129, -0.0470],\n",
       "                         [ 0.0298, -0.0382, -0.1180],\n",
       "                         [ 0.0140,  0.0170, -0.0314]],\n",
       "               \n",
       "                        [[-0.0879, -0.1531, -0.1734],\n",
       "                         [-0.0274, -0.0468, -0.1361],\n",
       "                         [-0.0065, -0.1347, -0.1056]],\n",
       "               \n",
       "                        [[ 0.0157,  0.0187,  0.0314],\n",
       "                         [ 0.0450,  0.0273, -0.0381],\n",
       "                         [ 0.0366,  0.0618,  0.0611]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0167,  0.0056, -0.0317],\n",
       "                         [-0.0742, -0.0974, -0.0510],\n",
       "                         [ 0.0122, -0.0893, -0.1018]],\n",
       "               \n",
       "                        [[-0.0144, -0.1177, -0.0502],\n",
       "                         [-0.0084, -0.0603, -0.0087],\n",
       "                         [ 0.0321,  0.0067,  0.0043]],\n",
       "               \n",
       "                        [[ 0.0405,  0.0915,  0.0142],\n",
       "                         [ 0.0768,  0.0973,  0.0397],\n",
       "                         [-0.0053,  0.0659,  0.0682]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0267,  0.0940,  0.0398],\n",
       "                         [-0.0069,  0.1114,  0.0613],\n",
       "                         [-0.0029, -0.0386,  0.0811]],\n",
       "               \n",
       "                        [[-0.0141, -0.0377,  0.0097],\n",
       "                         [-0.0325,  0.0991,  0.0553],\n",
       "                         [ 0.0720,  0.1347,  0.1221]],\n",
       "               \n",
       "                        [[ 0.0203,  0.0726,  0.0324],\n",
       "                         [ 0.0253,  0.0006,  0.0835],\n",
       "                         [ 0.0525,  0.0103,  0.0200]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0028, -0.0104,  0.0478],\n",
       "                         [-0.0270,  0.0373, -0.0039],\n",
       "                         [ 0.0500,  0.0265,  0.0512]],\n",
       "               \n",
       "                        [[-0.0082,  0.0191, -0.0083],\n",
       "                         [-0.0500,  0.0371,  0.0736],\n",
       "                         [-0.0017, -0.0148,  0.0710]],\n",
       "               \n",
       "                        [[ 0.0455, -0.0084, -0.0347],\n",
       "                         [ 0.0490,  0.1221,  0.0837],\n",
       "                         [-0.0272,  0.0672,  0.0117]]]], device='cuda:0')),\n",
       "              ('r2.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r2.bn2.weight',\n",
       "               tensor([0.7713, 0.6093, 0.5010, 0.5247, 0.7389, 0.6880, 0.5534, 0.6292, 0.6476,\n",
       "                       0.7027, 0.6202, 0.6286, 0.7389, 0.6146, 0.7341, 0.6972, 0.6622, 0.6783,\n",
       "                       0.7165, 0.7656, 0.5107, 0.7850, 0.5763, 0.6902, 0.6530, 0.6801, 0.7090,\n",
       "                       0.8178, 0.4175, 0.6693, 0.6717, 0.6999, 0.6971, 0.6445, 0.6425, 0.5714,\n",
       "                       0.6781, 0.6717, 0.6940, 0.6772, 0.7180, 0.6934, 0.5441, 0.7682, 0.7231,\n",
       "                       0.7496, 0.3735, 0.4228, 0.7074, 0.5656, 0.7641, 0.6462, 0.5269, 0.7220,\n",
       "                       0.5837, 0.7019, 0.6932, 0.6000, 0.6606, 0.6982, 0.7704, 0.6127, 0.6525,\n",
       "                       0.4448], device='cuda:0')),\n",
       "              ('r2.bn2.bias',\n",
       "               tensor([-0.0415, -0.2657, -0.3495, -0.6118, -0.2854, -0.2771, -0.3798, -0.6762,\n",
       "                       -0.4385, -0.2575, -0.3080, -0.3517, -0.3319, -0.2993, -0.4635, -0.2933,\n",
       "                       -0.3016, -0.3685, -0.2058, -0.4350, -0.5010, -0.1309, -0.6413, -0.3588,\n",
       "                       -0.3090, -0.2255, -0.5674, -0.3433, -0.4083, -0.2082, -0.7025, -0.1787,\n",
       "                       -0.2245, -0.4399, -0.1834, -0.9854, -0.2362, -0.5596, -0.1705, -0.2003,\n",
       "                       -0.2458, -0.1598, -0.2711, -0.2531, -0.1653, -0.1638, -0.3699, -0.3579,\n",
       "                       -0.2677, -0.4255, -0.2374, -0.2634, -0.4005, -0.2203, -0.2804, -0.1372,\n",
       "                       -0.2685, -0.1808, -0.2052, -0.2339, -0.4437, -0.4255, -0.3309, -0.6152],\n",
       "                      device='cuda:0')),\n",
       "              ('r2.bn2.running_mean',\n",
       "               tensor([-7.3346, -2.9928,  0.3517,  5.4859, -6.3821,  0.5989, -1.1403,  4.4414,\n",
       "                       -4.7221, -5.1114, -2.0598,  2.0980, -3.3928, -1.8730,  0.4824, -3.0370,\n",
       "                       -2.9259, -5.8292, -6.2644,  0.6631,  1.0425, -4.6375,  6.9278, -1.9784,\n",
       "                       -0.7079, -3.6245,  3.6190, -7.2246,  3.0662, -5.0289,  1.1360, -5.4831,\n",
       "                       -4.1711,  2.6551, -3.4142,  2.6518, -4.7086,  4.1039, -4.4769, -6.5433,\n",
       "                       -4.7916, -3.5790, -0.6632, -4.0997, -8.3781, -4.6433,  2.0149, -3.8055,\n",
       "                       -3.3584, -3.3101, -5.0213, -2.8585, -1.3023, -5.2455, -2.4119, -4.7427,\n",
       "                       -0.1817, -3.5494, -3.9185, -1.5422,  2.2699, -0.9345, -4.1560,  6.0039],\n",
       "                      device='cuda:0')),\n",
       "              ('r2.bn2.running_var',\n",
       "               tensor([22.2020, 19.1591, 19.0513, 11.2062, 24.1207, 32.0563,  6.9091, 39.5237,\n",
       "                        2.7405, 33.6531,  5.2549, 12.1522, 21.8637, 47.2639, 22.1682, 44.0558,\n",
       "                        8.6089,  8.0760, 28.5265, 15.2726,  8.5586, 34.2451, 11.0746, 12.0717,\n",
       "                       40.5902, 36.1385,  8.2679, 17.4517, 11.2169, 27.0480,  8.9711, 31.0896,\n",
       "                       48.2872, 12.7651, 50.1036,  7.4395, 42.0074, 30.2991, 30.9081,  3.7814,\n",
       "                       39.7582, 49.2843, 24.9968, 31.9250, 31.0274, 32.9395,  8.1202,  8.9899,\n",
       "                       39.2758,  5.2339, 34.2638,  7.1550, 18.3148, 19.2968, 40.9252, 32.0896,\n",
       "                       27.9403,  5.5990, 10.9813, 22.6656, 27.4739, 21.7973, 15.6757,  3.1502],\n",
       "                      device='cuda:0')),\n",
       "              ('r2.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r2.c2.weights',\n",
       "               tensor([[[[ 1.3626e-02, -6.8358e-02, -5.1477e-02],\n",
       "                         [ 1.1928e-01,  9.5664e-02, -5.0309e-02],\n",
       "                         [ 5.3605e-02, -2.9898e-02, -6.5801e-02]],\n",
       "               \n",
       "                        [[-9.8226e-02,  1.7403e-02, -6.5519e-02],\n",
       "                         [-3.0325e-02, -5.6643e-02,  3.4487e-02],\n",
       "                         [ 1.1929e-01,  6.9949e-02, -4.8318e-03]],\n",
       "               \n",
       "                        [[ 2.0532e-02, -8.1889e-03,  9.9637e-02],\n",
       "                         [ 8.9509e-02,  5.5043e-02,  1.0488e-01],\n",
       "                         [ 1.9431e-02, -5.9340e-02,  7.2210e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.7165e-02, -2.9513e-02, -2.2383e-02],\n",
       "                         [-1.9230e-02, -7.8899e-04, -1.6071e-02],\n",
       "                         [ 5.3580e-02,  6.6282e-02,  7.9785e-02]],\n",
       "               \n",
       "                        [[-3.1150e-03,  2.5008e-02, -9.5131e-04],\n",
       "                         [ 6.0443e-02,  2.9062e-02,  5.2587e-02],\n",
       "                         [-8.5304e-03,  1.2712e-02, -5.8248e-02]],\n",
       "               \n",
       "                        [[-3.0733e-03, -5.7391e-03, -5.5552e-02],\n",
       "                         [ 4.9968e-02,  3.4485e-02,  4.5788e-02],\n",
       "                         [-2.6785e-02,  4.2054e-02,  4.3132e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.8888e-02, -1.0275e-01, -9.0183e-02],\n",
       "                         [-2.0844e-02,  7.9704e-02, -1.3240e-02],\n",
       "                         [-1.3740e-02,  2.0024e-02, -1.8250e-02]],\n",
       "               \n",
       "                        [[-1.2955e-01,  1.9862e-02, -9.7786e-02],\n",
       "                         [-5.3229e-02, -3.1672e-02, -2.7652e-02],\n",
       "                         [-2.5569e-02,  6.5696e-02,  5.1046e-02]],\n",
       "               \n",
       "                        [[-3.0912e-02, -9.1194e-02, -4.1006e-02],\n",
       "                         [-9.8514e-02, -1.2195e-01, -7.7233e-03],\n",
       "                         [ 2.8343e-02, -7.9114e-02, -8.5855e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4081e-02, -6.2128e-02, -3.8945e-02],\n",
       "                         [-7.8532e-03, -1.2634e-02, -6.6983e-03],\n",
       "                         [ 5.7394e-02, -1.7709e-02, -1.4023e-02]],\n",
       "               \n",
       "                        [[ 3.1037e-02,  9.2975e-02,  4.4297e-03],\n",
       "                         [ 4.9360e-03, -1.9493e-02,  1.7518e-02],\n",
       "                         [ 5.6367e-02,  6.5541e-02,  5.4910e-02]],\n",
       "               \n",
       "                        [[-6.3843e-03,  2.6718e-02,  2.0362e-02],\n",
       "                         [-1.0270e-01, -4.3825e-02, -4.5441e-03],\n",
       "                         [ 3.0419e-02, -3.4045e-03,  4.9981e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.1860e-02,  4.8469e-02,  8.4451e-02],\n",
       "                         [ 2.1703e-02,  8.3901e-02,  3.3681e-02],\n",
       "                         [ 5.8883e-03, -9.9883e-03,  1.8229e-02]],\n",
       "               \n",
       "                        [[-1.6230e-02, -2.2432e-02, -4.5919e-02],\n",
       "                         [ 2.8810e-02,  4.6254e-03, -4.4304e-03],\n",
       "                         [-4.3686e-02, -7.9055e-02, -9.7150e-02]],\n",
       "               \n",
       "                        [[-9.4199e-03,  2.7157e-02,  5.2385e-02],\n",
       "                         [ 1.1134e-01,  3.3927e-02,  6.0531e-02],\n",
       "                         [ 3.7984e-02,  6.9757e-02,  6.7052e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.5265e-02,  7.0173e-02, -1.5970e-02],\n",
       "                         [-2.2702e-02,  8.4571e-02, -3.9819e-02],\n",
       "                         [ 6.6034e-03, -1.5587e-02, -2.8216e-02]],\n",
       "               \n",
       "                        [[ 5.5456e-02, -1.2689e-02, -2.8938e-02],\n",
       "                         [ 4.5384e-03, -1.8012e-02, -3.1622e-02],\n",
       "                         [-3.5134e-02, -2.7154e-02, -5.3664e-02]],\n",
       "               \n",
       "                        [[ 2.6583e-02,  3.4783e-02,  7.0252e-03],\n",
       "                         [ 1.1145e-02,  5.6146e-04,  8.8584e-02],\n",
       "                         [ 2.8509e-02,  7.9499e-03,  5.5590e-04]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-7.3756e-02, -1.1860e-01, -1.0659e-01],\n",
       "                         [-3.4047e-03,  4.9518e-02, -5.2307e-02],\n",
       "                         [ 7.6111e-02,  8.4403e-02,  8.1662e-02]],\n",
       "               \n",
       "                        [[ 2.5725e-02, -9.2130e-02,  1.1915e-02],\n",
       "                         [-1.0971e-02, -5.8872e-02, -2.6337e-02],\n",
       "                         [ 4.4216e-03, -9.0963e-02,  4.6940e-02]],\n",
       "               \n",
       "                        [[-8.0425e-03,  9.4902e-02, -7.8374e-03],\n",
       "                         [ 2.4257e-03,  5.4358e-02,  4.8809e-03],\n",
       "                         [ 1.4766e-02, -6.9985e-02, -1.3759e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.2050e-03, -3.6623e-02,  8.6436e-03],\n",
       "                         [-6.6071e-03, -6.2263e-02,  7.9198e-03],\n",
       "                         [-6.7422e-02, -3.0419e-03, -5.7873e-02]],\n",
       "               \n",
       "                        [[ 7.5965e-03,  3.2241e-02,  2.0492e-02],\n",
       "                         [-5.0130e-02,  5.0981e-02,  1.2561e-02],\n",
       "                         [ 3.5007e-02, -3.2691e-02, -1.3510e-02]],\n",
       "               \n",
       "                        [[ 2.2407e-02,  3.4537e-02,  3.9552e-02],\n",
       "                         [ 1.6540e-02, -3.8400e-02, -7.9329e-02],\n",
       "                         [-3.6639e-02, -1.4739e-03,  7.3491e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.6144e-02, -2.7145e-02,  5.1616e-02],\n",
       "                         [ 2.9047e-02, -2.0483e-02, -3.8437e-02],\n",
       "                         [ 2.0170e-02, -6.9393e-02,  5.6791e-02]],\n",
       "               \n",
       "                        [[ 2.0136e-02, -1.0716e-02, -1.6600e-02],\n",
       "                         [ 1.0411e-01,  7.7156e-02,  7.5909e-02],\n",
       "                         [ 6.2575e-02,  7.4644e-02,  3.5333e-02]],\n",
       "               \n",
       "                        [[-4.4657e-02,  1.5210e-02, -1.1611e-02],\n",
       "                         [ 1.3732e-02, -5.8254e-02, -5.5432e-02],\n",
       "                         [-9.8536e-02, -5.4974e-02, -1.0038e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0816e-02, -4.1557e-03,  5.2648e-02],\n",
       "                         [-5.2811e-03, -1.9215e-02, -8.3469e-02],\n",
       "                         [-5.8865e-03, -6.5343e-02, -9.3282e-02]],\n",
       "               \n",
       "                        [[ 1.0199e-01, -4.8577e-02, -5.3353e-02],\n",
       "                         [ 4.6783e-02,  4.2069e-02, -1.3016e-01],\n",
       "                         [ 4.5592e-02, -4.4078e-02, -8.2260e-02]],\n",
       "               \n",
       "                        [[-7.1444e-02, -8.2415e-02, -2.7123e-02],\n",
       "                         [-3.2927e-02,  7.5434e-03,  3.1310e-02],\n",
       "                         [-4.8816e-02,  7.3237e-03, -5.0550e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9629e-03,  5.8003e-02, -6.3973e-02],\n",
       "                         [-2.0476e-02,  7.2941e-02, -3.1413e-03],\n",
       "                         [ 1.5243e-02,  1.1872e-01, -5.4421e-02]],\n",
       "               \n",
       "                        [[-1.9199e-02, -6.2585e-02, -4.7157e-02],\n",
       "                         [ 2.2619e-02, -5.4837e-02,  3.1060e-02],\n",
       "                         [-8.3700e-02, -8.3555e-02,  4.1549e-02]],\n",
       "               \n",
       "                        [[-8.6728e-03, -8.9646e-02, -7.5560e-02],\n",
       "                         [-2.3023e-02, -7.1367e-02, -1.1514e-01],\n",
       "                         [-5.1551e-02, -1.5674e-02,  2.7686e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.5113e-02,  5.2722e-02, -1.1122e-01],\n",
       "                         [ 3.8945e-02,  5.1104e-02, -8.9759e-02],\n",
       "                         [ 5.1348e-02, -1.7937e-04, -8.0560e-02]],\n",
       "               \n",
       "                        [[-1.1206e-02, -5.7805e-02, -1.7951e-01],\n",
       "                         [-6.5696e-02, -1.3595e-02, -1.4455e-01],\n",
       "                         [-2.4444e-02,  3.2087e-02, -3.8379e-02]],\n",
       "               \n",
       "                        [[-2.2949e-02, -7.5686e-02,  3.8370e-02],\n",
       "                         [ 9.1141e-03, -1.3558e-01,  1.2710e-02],\n",
       "                         [ 4.2331e-02,  2.3819e-03,  1.7632e-01]]]], device='cuda:0')),\n",
       "              ('r2.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r3.bn1.weight',\n",
       "               tensor([0.8675, 1.1037, 0.9522, 0.9306, 0.9445, 0.8689, 0.7672, 0.8872, 1.1200,\n",
       "                       0.9330, 0.8082, 0.8825, 0.9027, 0.9956, 0.9557, 0.9589, 0.8920, 0.8401,\n",
       "                       0.9206, 0.8195, 0.9725, 1.0594, 0.8879, 0.9538, 1.1736, 0.8755, 1.1856,\n",
       "                       1.0226, 1.0088, 0.9859, 0.8524, 1.2627, 1.1747, 0.9020, 0.6713, 1.0430,\n",
       "                       0.8643, 0.9641, 1.0725, 1.0967, 0.9077, 0.9459, 0.9019, 0.9693, 1.1505,\n",
       "                       0.8954, 0.9223, 1.0194, 0.8529, 1.0170, 0.9480, 0.9381, 1.1692, 1.0504,\n",
       "                       1.0674, 0.9699, 1.0398, 0.8934, 1.0733, 0.9320, 0.8490, 0.9033, 0.8427,\n",
       "                       1.0026], device='cuda:0')),\n",
       "              ('r3.bn1.bias',\n",
       "               tensor([-0.2662, -0.2333, -0.0535, -0.4358,  0.0682,  0.0170, -0.3745,  0.2786,\n",
       "                       -0.2440, -0.0571, -0.0465, -0.3431, -0.3730, -0.1476,  0.0149, -0.1446,\n",
       "                       -0.2531, -0.3669, -0.3252, -0.2403, -0.1372, -0.1815, -0.2903, -0.2717,\n",
       "                        0.2066, -0.2996, -0.1470, -0.1489, -0.3126, -0.1604, -0.0735, -0.3925,\n",
       "                       -0.1646,  0.0055,  0.2818, -0.6912,  0.2276,  0.0037, -0.6257, -0.4526,\n",
       "                       -0.1148, -0.3409, -0.4709, -0.1257,  0.1799, -0.1143, -0.0772, -0.3048,\n",
       "                       -0.2364, -0.4744, -0.1978, -0.0333, -0.3402, -0.2392, -0.1509, -0.0460,\n",
       "                       -0.0852, -0.2489, -0.3411, -0.2189, -0.2353, -0.0146, -0.2850, -0.0566],\n",
       "                      device='cuda:0')),\n",
       "              ('r3.bn1.running_mean',\n",
       "               tensor([-1.9278e-01, -3.0006e-01, -3.5588e-01, -1.7291e-01,  2.9792e-01,\n",
       "                       -9.2507e-01, -3.6485e-01, -6.9372e-02,  1.8269e-02,  1.5059e-04,\n",
       "                       -4.4129e-02, -2.8460e-01,  2.8096e-02, -5.7909e-01,  5.3390e-01,\n",
       "                       -3.1744e-01,  7.2831e-01,  4.9455e-01,  2.0788e-01, -3.1023e-01,\n",
       "                        3.4711e-02, -5.7284e-01, -1.8480e-01,  4.4035e-02,  4.6192e-01,\n",
       "                       -5.5340e-02,  8.6344e-01,  3.3357e-01,  3.9993e-01,  3.3246e-01,\n",
       "                       -3.9504e-01, -7.5991e-01, -1.2372e-01,  1.8541e-01, -4.4322e-01,\n",
       "                        1.3805e-01, -3.2314e-01, -3.8771e-01,  2.8118e-01,  1.4584e-01,\n",
       "                       -2.6687e-01, -2.8074e-01,  3.6097e-01, -3.5862e-01,  9.5789e-02,\n",
       "                        4.7643e-02, -1.0904e-01,  1.0481e+00,  1.4060e-02,  3.2128e-01,\n",
       "                       -3.3750e-01,  2.2710e-01, -1.1007e+00, -1.1457e-01, -3.7070e-01,\n",
       "                        3.0313e-01, -8.2500e-01,  2.0000e-01,  2.9871e-01,  2.3117e-01,\n",
       "                       -2.6759e-01,  1.1660e-01, -1.7363e-01, -1.4035e-01], device='cuda:0')),\n",
       "              ('r3.bn1.running_var',\n",
       "               tensor([0.2732, 0.4619, 3.2887, 0.3535, 0.9381, 0.5442, 0.5027, 0.8602, 0.7382,\n",
       "                       0.4029, 0.3121, 0.3518, 0.5811, 0.7396, 0.8660, 1.1259, 0.6690, 0.3969,\n",
       "                       0.2708, 0.2987, 0.6092, 0.4901, 0.7074, 1.8027, 0.9969, 0.5442, 0.7808,\n",
       "                       0.4259, 0.5017, 0.6012, 0.4093, 0.3374, 0.6707, 0.7740, 0.6621, 0.4933,\n",
       "                       0.5789, 1.3388, 0.5578, 0.3771, 0.4015, 0.3916, 0.3664, 0.4285, 1.0126,\n",
       "                       0.8462, 0.5044, 0.6128, 4.7831, 0.7184, 0.4943, 0.6055, 0.6339, 0.8408,\n",
       "                       0.7086, 0.4657, 0.4173, 1.0200, 0.7241, 0.8948, 0.4947, 0.3910, 0.4523,\n",
       "                       0.3745], device='cuda:0')),\n",
       "              ('r3.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r3.c1.weights',\n",
       "               tensor([[[[ 0.0551,  0.0131, -0.0245],\n",
       "                         [-0.0128, -0.0179,  0.0193],\n",
       "                         [ 0.0146,  0.0272,  0.0315]],\n",
       "               \n",
       "                        [[-0.0356, -0.0054,  0.0248],\n",
       "                         [-0.0840, -0.0100, -0.0474],\n",
       "                         [ 0.0027, -0.0026,  0.0044]],\n",
       "               \n",
       "                        [[ 0.0772, -0.0016,  0.0435],\n",
       "                         [-0.0182,  0.0225,  0.0705],\n",
       "                         [ 0.0914,  0.0494,  0.0437]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0077, -0.0030, -0.0428],\n",
       "                         [-0.0245, -0.0318, -0.0215],\n",
       "                         [ 0.0146, -0.0432, -0.0483]],\n",
       "               \n",
       "                        [[-0.1062, -0.0296, -0.0107],\n",
       "                         [-0.0936, -0.0328, -0.0359],\n",
       "                         [-0.0782, -0.1392, -0.0534]],\n",
       "               \n",
       "                        [[-0.0209, -0.0337, -0.0185],\n",
       "                         [-0.0221,  0.0019,  0.0592],\n",
       "                         [ 0.0429,  0.0334,  0.0522]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0609,  0.0102,  0.0564],\n",
       "                         [-0.0113, -0.1284, -0.1174],\n",
       "                         [ 0.0011, -0.0524, -0.0018]],\n",
       "               \n",
       "                        [[ 0.1102, -0.0914, -0.0343],\n",
       "                         [-0.0050, -0.1209, -0.2090],\n",
       "                         [ 0.1210,  0.0602, -0.0073]],\n",
       "               \n",
       "                        [[-0.0447,  0.1479, -0.0114],\n",
       "                         [ 0.0419,  0.1829,  0.0874],\n",
       "                         [ 0.0091,  0.1422,  0.0508]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1110, -0.1172, -0.0497],\n",
       "                         [-0.0372,  0.0152, -0.0145],\n",
       "                         [-0.1037, -0.1963, -0.2507]],\n",
       "               \n",
       "                        [[-0.0484, -0.0380,  0.0453],\n",
       "                         [ 0.0797,  0.0437,  0.1171],\n",
       "                         [-0.0318,  0.0311, -0.0498]],\n",
       "               \n",
       "                        [[-0.0528, -0.0499,  0.0391],\n",
       "                         [-0.0705, -0.0118, -0.0163],\n",
       "                         [-0.1197,  0.0302, -0.0037]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0601, -0.0380, -0.0296],\n",
       "                         [-0.0476, -0.0465, -0.0018],\n",
       "                         [ 0.0316, -0.0670, -0.0027]],\n",
       "               \n",
       "                        [[-0.0214,  0.0180, -0.0458],\n",
       "                         [-0.0491, -0.0803,  0.0617],\n",
       "                         [-0.0095, -0.0641,  0.1014]],\n",
       "               \n",
       "                        [[ 0.0613,  0.0964,  0.0958],\n",
       "                         [ 0.0279,  0.0034,  0.0608],\n",
       "                         [-0.0585, -0.0192, -0.0123]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0506, -0.0010,  0.0655],\n",
       "                         [ 0.0265,  0.0258,  0.0825],\n",
       "                         [-0.1234, -0.0870, -0.0672]],\n",
       "               \n",
       "                        [[ 0.0044, -0.0340, -0.0599],\n",
       "                         [-0.0327, -0.0757, -0.0776],\n",
       "                         [-0.0419,  0.0191, -0.0488]],\n",
       "               \n",
       "                        [[-0.0843, -0.0377, -0.0645],\n",
       "                         [-0.0131,  0.0091,  0.0003],\n",
       "                         [ 0.0760,  0.0178,  0.0430]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0309, -0.0124,  0.0953],\n",
       "                         [ 0.0422,  0.0646,  0.0894],\n",
       "                         [-0.0137,  0.0224,  0.0991]],\n",
       "               \n",
       "                        [[-0.0534, -0.0472,  0.0064],\n",
       "                         [-0.0756, -0.0123, -0.0015],\n",
       "                         [-0.0327,  0.0148,  0.0889]],\n",
       "               \n",
       "                        [[-0.0173, -0.0431,  0.0185],\n",
       "                         [-0.0043, -0.0414, -0.0425],\n",
       "                         [-0.0318,  0.0438,  0.0075]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0267,  0.0335,  0.1193],\n",
       "                         [ 0.0324,  0.1177,  0.1191],\n",
       "                         [-0.0377,  0.0330,  0.0188]],\n",
       "               \n",
       "                        [[-0.0324, -0.0690, -0.0230],\n",
       "                         [ 0.0016, -0.0679, -0.0550],\n",
       "                         [-0.0406, -0.0862, -0.0231]],\n",
       "               \n",
       "                        [[ 0.0848,  0.0291,  0.1192],\n",
       "                         [ 0.0010,  0.0359,  0.0547],\n",
       "                         [-0.0254,  0.0098,  0.1046]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0360, -0.0098, -0.0083],\n",
       "                         [ 0.0375,  0.0481,  0.1102],\n",
       "                         [ 0.0415, -0.0345, -0.0298]],\n",
       "               \n",
       "                        [[-0.0683, -0.0547,  0.0047],\n",
       "                         [ 0.0295,  0.0335,  0.0505],\n",
       "                         [ 0.0018,  0.0254,  0.0788]],\n",
       "               \n",
       "                        [[ 0.0170, -0.0218, -0.0136],\n",
       "                         [ 0.0746,  0.0682,  0.0713],\n",
       "                         [ 0.0766,  0.1130,  0.0542]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0601,  0.0217, -0.0042],\n",
       "                         [-0.0349, -0.0313, -0.0858],\n",
       "                         [ 0.0639,  0.0147, -0.0050]],\n",
       "               \n",
       "                        [[ 0.0380,  0.0274, -0.0242],\n",
       "                         [-0.0430, -0.0359, -0.0013],\n",
       "                         [-0.0246, -0.0161,  0.0208]],\n",
       "               \n",
       "                        [[-0.0225,  0.0135,  0.0398],\n",
       "                         [-0.0061, -0.0185,  0.0243],\n",
       "                         [ 0.0363, -0.0323, -0.0386]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0083, -0.0070, -0.0671],\n",
       "                         [-0.0093, -0.0442, -0.0618],\n",
       "                         [ 0.0359, -0.0613,  0.0421]],\n",
       "               \n",
       "                        [[ 0.0755, -0.0287, -0.0171],\n",
       "                         [ 0.0178, -0.0107,  0.0133],\n",
       "                         [ 0.0439, -0.0084,  0.0031]],\n",
       "               \n",
       "                        [[ 0.0313,  0.0887,  0.0631],\n",
       "                         [ 0.0241,  0.0511,  0.0426],\n",
       "                         [ 0.0066, -0.0140,  0.0195]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0370, -0.0006, -0.0416],\n",
       "                         [ 0.0997,  0.0890,  0.0010],\n",
       "                         [-0.0328, -0.0548, -0.1015]],\n",
       "               \n",
       "                        [[ 0.0097, -0.0445,  0.0358],\n",
       "                         [-0.0480, -0.0070,  0.0017],\n",
       "                         [-0.0786,  0.0332,  0.0375]],\n",
       "               \n",
       "                        [[ 0.0174, -0.0372,  0.0469],\n",
       "                         [ 0.0180, -0.0305,  0.0529],\n",
       "                         [ 0.0668,  0.0530,  0.1327]]]], device='cuda:0')),\n",
       "              ('r3.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r3.bn2.weight',\n",
       "               tensor([0.4487, 0.4258, 0.5803, 0.6998, 0.6153, 0.4236, 0.7579, 0.7307, 0.1789,\n",
       "                       0.5426, 0.5001, 0.7042, 0.4525, 0.4858, 0.6262, 0.6021, 0.4419, 0.5116,\n",
       "                       0.4204, 0.6614, 0.6118, 0.5830, 0.6468, 0.6726, 0.4402, 0.8101, 0.3934,\n",
       "                       0.6904, 0.7501, 0.2186, 0.6409, 0.6166, 0.6403, 0.3955, 0.7385, 0.8176,\n",
       "                       0.6117, 0.2785, 0.5492, 0.7270, 0.6976, 0.6402, 0.6962, 0.7307, 0.7167,\n",
       "                       0.7760, 0.7648, 0.4862, 0.4478, 0.7070, 0.7135, 0.6840, 0.5928, 0.3573,\n",
       "                       0.6829, 0.4826, 0.6435, 0.6123, 0.2922, 0.5578, 0.5506, 0.4945, 0.6241,\n",
       "                       0.2732, 0.6195, 0.3508, 0.6224, 0.4964, 0.3233, 0.7802, 0.7149, 0.5912,\n",
       "                       0.7080, 0.5444, 0.6406, 0.6825, 0.4833, 0.7019, 0.5707, 0.6787, 0.7082,\n",
       "                       0.6148, 0.6921, 0.4517, 0.5230, 0.5685, 0.7438, 0.7842, 0.7112, 0.6246,\n",
       "                       0.7771, 0.4758, 0.5825, 0.5570, 0.3521, 0.6144, 0.3427, 0.7172, 0.6406,\n",
       "                       0.3590, 0.3907, 0.4088, 0.7131, 0.5282, 0.3515, 0.7785, 0.5572, 0.6210,\n",
       "                       0.2011, 0.2347, 0.5687, 0.6823, 0.7472, 0.5397, 0.6255, 0.3788, 0.5290,\n",
       "                       0.5697, 0.7265, 0.6797, 0.3179, 0.1323, 0.6518, 0.7540, 0.0740, 0.4572,\n",
       "                       0.4602, 0.6321], device='cuda:0')),\n",
       "              ('r3.bn2.bias',\n",
       "               tensor([-0.4394, -0.4241, -0.3006, -0.3280, -0.2808, -0.3645, -0.3504, -0.2431,\n",
       "                       -0.7859, -0.2631, -0.3897, -0.2412, -0.5532, -0.3734, -0.2459, -0.2647,\n",
       "                       -0.5680, -0.2931, -0.3287, -0.1089, -0.2456, -0.4378, -0.1008, -0.1044,\n",
       "                       -0.3676, -0.3186, -0.4558, -0.1282, -0.1826, -1.0134, -0.2535, -0.3056,\n",
       "                       -0.2014, -0.3168, -0.0952, -0.3118, -0.2343, -0.3463, -0.2291, -0.1570,\n",
       "                       -0.1267, -0.3167, -0.1658, -0.1393, -0.1924, -0.1705, -0.1919, -0.1296,\n",
       "                       -0.1017, -0.1291, -0.0373, -0.2174, -0.2170, -0.4969, -0.4717, -0.3334,\n",
       "                       -0.2799, -0.1676, -0.3478, -1.0837, -0.3963, -0.8964, -0.1079, -0.5654,\n",
       "                       -0.1991, -1.0823, -0.0737, -0.3055, -0.3719, -0.2401, -0.1267, -0.1717,\n",
       "                       -0.2092, -0.3569, -0.0716, -0.0681, -0.3326, -0.2366, -0.2620, -0.1338,\n",
       "                       -0.1735, -0.2948, -0.3080, -0.4609, -0.3546, -0.2668, -0.1824, -0.2911,\n",
       "                       -0.2406, -0.2955, -0.2663, -0.3908, -0.0217, -0.2624, -0.3134, -0.0455,\n",
       "                       -0.4261, -0.1587, -0.2275, -0.4147, -0.4226, -0.3622, -0.0860, -0.2857,\n",
       "                       -0.6655, -0.3222, -0.3975, -0.2620, -0.8615, -0.8571, -0.3426, -0.3121,\n",
       "                       -0.1856, -0.2998, -0.1512, -0.3638, -0.3208, -0.4521, -0.1659, -0.1042,\n",
       "                       -0.5309, -0.8179, -0.0771, -0.1609, -0.5330, -0.4084, -0.4178, -0.2001],\n",
       "                      device='cuda:0')),\n",
       "              ('r3.bn2.running_mean',\n",
       "               tensor([-1.4229, -1.2214, -1.6072, -0.5497, -2.9158, -1.3382, -1.6177, -2.6316,\n",
       "                        2.0688, -3.7313, -1.1994, -2.0367, -1.6080, -3.6217, -1.9497, -2.8026,\n",
       "                        1.8835, -2.1745, -0.9390, -4.3341, -2.3829, -2.1748, -3.9251, -1.4747,\n",
       "                       -1.0798, -1.1470,  0.8587, -1.2908, -2.4180,  2.0651, -2.5052, -1.4777,\n",
       "                       -4.2651, -0.7060, -2.2940, -2.2242, -5.1973, -1.8376, -2.1210, -3.3058,\n",
       "                       -3.0069, -2.2393, -2.5131, -3.0334, -1.7253, -1.6586, -2.2470, -0.4986,\n",
       "                       -1.1442, -0.6551, -3.3414, -1.6640, -3.5348, -0.2186, -0.8474, -1.2372,\n",
       "                       -1.8001, -1.2853,  1.8191,  2.7470, -0.6561,  1.5140, -2.2054,  1.9099,\n",
       "                       -1.1999,  2.5554, -3.3777, -0.9515, -0.4237, -1.5183, -3.3959, -4.2173,\n",
       "                       -1.6800, -3.4718, -3.6495, -2.5857, -1.1050, -0.7061, -1.3940, -2.4701,\n",
       "                       -2.3020, -2.9414, -2.2424,  0.0644, -2.1255, -1.2172, -3.1529, -0.5823,\n",
       "                       -1.4325, -1.4185, -2.1456, -1.9120, -3.8414, -2.4951,  0.1384, -3.6720,\n",
       "                        0.4042, -2.7264, -3.9914, -0.3188,  0.1165, -0.6583, -2.4770, -1.6104,\n",
       "                        1.9039, -1.0656, -1.9332, -2.0394,  1.4163,  2.0477, -2.1979, -3.0457,\n",
       "                       -2.2735, -3.0073, -4.1444, -1.6255, -1.0772, -2.1981, -3.1091, -3.1619,\n",
       "                        0.2437,  1.7166, -3.5788, -1.2899,  1.0910, -0.4010, -0.2438, -3.2033],\n",
       "                      device='cuda:0')),\n",
       "              ('r3.bn2.running_var',\n",
       "               tensor([ 20.6248,  10.3829,  19.3959,  30.7233,  29.8760,   4.8046,  17.8302,\n",
       "                        35.2218,   3.6984,  21.6000,   6.9558,  30.6562,  26.7864,  24.8933,\n",
       "                        35.5919,  41.9059,   8.4625,  22.9419,  20.5941,  59.0075,  40.7266,\n",
       "                        18.3103,  52.6914,  27.3787,  14.5865,   9.2299,   5.9455,  28.9556,\n",
       "                        58.8720,   3.7771,  20.2346,  21.0079,  38.4317,  19.1639,  79.9343,\n",
       "                        11.7834,  49.2036,   7.3241,  23.6135,  60.2148,  52.8322,  36.0019,\n",
       "                        57.7596,  57.3080,  37.4944,  41.7453,  54.4390,   7.9999,   7.5305,\n",
       "                        60.7387,  77.3516,  37.5160,  43.9071,  12.3770,   8.1787,  21.3965,\n",
       "                        18.7327,  12.1208,   9.4553,   5.2316,  14.6643,   2.8303,  44.3003,\n",
       "                         6.0943,  12.6909,   4.7829,  81.6555,  11.9265,   3.4809,  21.1887,\n",
       "                        56.9740,  42.4061,  57.0822,  31.2416, 107.7060,  70.0009,  15.7898,\n",
       "                        31.5419,  15.6442,  46.3966,  61.4154,  43.5911,  21.3872,  11.7950,\n",
       "                        21.8694,  15.3868,  48.6712,  20.4233,  27.5794,  19.1808,  26.6044,\n",
       "                        29.8940,  69.2093,  26.8491,  12.3252,  70.1996,  15.1197,  57.5469,\n",
       "                        55.2945,  11.6873,  14.7951,   9.7224,  63.2704,  14.0533,   8.6125,\n",
       "                         9.6000,  21.8863,  23.4461,   2.8249,   3.8557,  14.7867,  44.7444,\n",
       "                        44.2260,  24.4537,  52.3307,  12.3560,  28.0939,  25.5822,  51.2772,\n",
       "                        46.3443,   4.8609,   2.5286,  54.7209,  29.5268,   1.6435,   8.8145,\n",
       "                         5.8615,  49.4068], device='cuda:0')),\n",
       "              ('r3.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r3.c2.weights',\n",
       "               tensor([[[[-4.2241e-02,  6.0634e-03, -4.3331e-02],\n",
       "                         [ 2.8007e-03, -3.5940e-02,  5.8868e-03],\n",
       "                         [ 4.2950e-02,  1.8647e-02, -4.9648e-02]],\n",
       "               \n",
       "                        [[ 7.7930e-03,  5.2935e-02,  3.5154e-02],\n",
       "                         [ 4.6323e-02,  4.9953e-03,  2.6152e-04],\n",
       "                         [-7.1081e-03,  2.6285e-02,  2.3181e-03]],\n",
       "               \n",
       "                        [[ 5.1973e-02, -1.7457e-02, -6.2277e-02],\n",
       "                         [-7.6112e-02, -3.5111e-02, -1.8018e-02],\n",
       "                         [ 3.0006e-02, -5.4445e-03, -2.7126e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.4444e-02,  5.5301e-04, -5.6272e-02],\n",
       "                         [ 3.6641e-03,  9.3104e-02,  9.3451e-03],\n",
       "                         [ 3.2748e-02,  6.1822e-03, -4.5234e-02]],\n",
       "               \n",
       "                        [[ 1.0401e-02, -2.3212e-02, -1.8195e-02],\n",
       "                         [ 3.8380e-02,  6.1057e-02,  8.1524e-03],\n",
       "                         [ 3.3717e-02,  3.7650e-02,  4.8839e-02]],\n",
       "               \n",
       "                        [[ 4.7364e-03, -1.1083e-03, -6.9948e-02],\n",
       "                         [ 7.1775e-02, -3.3347e-02, -7.4051e-02],\n",
       "                         [-9.5769e-03,  1.9650e-02, -1.5570e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4579e-02, -4.0301e-02, -3.4657e-02],\n",
       "                         [-3.9114e-02,  4.5730e-03, -1.7341e-02],\n",
       "                         [-7.1029e-03,  3.4330e-02, -5.5338e-02]],\n",
       "               \n",
       "                        [[ 5.0473e-02,  3.1860e-02, -5.0916e-02],\n",
       "                         [ 2.5310e-02,  2.2656e-02,  3.9051e-02],\n",
       "                         [-2.9535e-02, -5.1721e-02, -1.8309e-04]],\n",
       "               \n",
       "                        [[-4.0483e-02, -9.2076e-02, -5.6337e-02],\n",
       "                         [-2.9253e-02, -6.7071e-02, -5.8313e-02],\n",
       "                         [-1.0540e-01, -8.5127e-02, -7.1102e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1038e-02, -3.5464e-03, -3.1545e-02],\n",
       "                         [-1.7624e-02,  2.0409e-02, -7.2493e-03],\n",
       "                         [-4.8978e-02,  1.1752e-02,  1.9105e-02]],\n",
       "               \n",
       "                        [[-1.7745e-02, -4.7549e-02,  2.0586e-03],\n",
       "                         [-2.2376e-02, -5.3136e-02, -4.0941e-03],\n",
       "                         [ 2.0235e-02, -3.2917e-02,  2.1693e-02]],\n",
       "               \n",
       "                        [[ 1.1746e-02, -1.5072e-02, -6.6297e-03],\n",
       "                         [ 1.9710e-03, -4.1630e-03, -3.3000e-02],\n",
       "                         [-4.7734e-02, -1.8569e-02, -1.2600e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.8994e-02, -8.0698e-02,  9.9588e-04],\n",
       "                         [ 2.4494e-02,  9.7804e-03, -2.4828e-02],\n",
       "                         [ 1.8874e-02, -7.4873e-03, -5.4045e-02]],\n",
       "               \n",
       "                        [[-1.2893e-02, -4.2495e-02, -3.5188e-02],\n",
       "                         [-1.5791e-01,  1.4672e-01,  6.4395e-02],\n",
       "                         [ 2.6638e-02,  6.3295e-02,  2.8504e-02]],\n",
       "               \n",
       "                        [[-4.8363e-02,  8.4942e-02, -1.6167e-02],\n",
       "                         [ 1.5117e-01, -8.9933e-02, -9.8458e-02],\n",
       "                         [ 4.2797e-02, -5.2784e-02,  4.1934e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.3389e-02, -4.8692e-02,  3.0780e-03],\n",
       "                         [-5.9504e-03, -8.7611e-02, -1.4129e-02],\n",
       "                         [-4.6840e-02, -5.5957e-02,  7.9926e-02]],\n",
       "               \n",
       "                        [[-3.0812e-02,  2.2404e-02, -6.5081e-02],\n",
       "                         [ 1.6202e-02, -6.0443e-02, -1.8899e-02],\n",
       "                         [-2.0808e-02,  1.1701e-02, -7.4778e-02]],\n",
       "               \n",
       "                        [[-4.9083e-02,  3.1608e-02, -5.1236e-02],\n",
       "                         [-1.8720e-02,  3.5814e-03, -5.4495e-02],\n",
       "                         [ 1.1478e-04,  4.5066e-02,  2.8880e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.0978e-03,  2.6683e-03,  1.0970e-02],\n",
       "                         [ 5.8734e-02, -6.3728e-03,  9.7145e-03],\n",
       "                         [-3.3218e-02,  3.1883e-03, -1.5158e-02]],\n",
       "               \n",
       "                        [[-3.5116e-02, -6.0445e-02, -1.0849e-02],\n",
       "                         [-3.4669e-02, -5.4951e-02, -1.0379e-02],\n",
       "                         [-3.4688e-02, -1.0798e-01, -1.0583e-01]],\n",
       "               \n",
       "                        [[ 6.3655e-02,  4.4585e-02, -1.2452e-02],\n",
       "                         [ 3.5036e-02,  8.6877e-03,  1.4152e-03],\n",
       "                         [ 3.7897e-02, -3.6749e-02, -4.2307e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5002e-02,  7.4937e-02,  5.1894e-02],\n",
       "                         [-2.4756e-02,  2.5818e-02,  7.5960e-02],\n",
       "                         [ 1.4612e-02, -4.2628e-02, -1.1269e-02]],\n",
       "               \n",
       "                        [[ 1.6010e-02,  3.6683e-02, -5.7157e-02],\n",
       "                         [-3.5667e-03, -1.9447e-02,  3.0003e-02],\n",
       "                         [-5.7815e-02,  4.9099e-02, -9.5429e-03]],\n",
       "               \n",
       "                        [[-3.7041e-02, -7.6815e-03, -7.5542e-02],\n",
       "                         [-1.7482e-03, -3.9869e-02, -2.9184e-02],\n",
       "                         [ 7.7490e-03, -5.8041e-02, -2.7788e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.7158e-03,  1.1087e-02, -1.5310e-02],\n",
       "                         [-1.7685e-02, -4.9878e-02,  1.3674e-02],\n",
       "                         [ 2.3838e-02,  6.2673e-02,  7.5673e-02]],\n",
       "               \n",
       "                        [[ 5.5794e-02,  1.1428e-02,  6.8418e-02],\n",
       "                         [-2.3456e-02,  5.7386e-02, -1.8555e-02],\n",
       "                         [ 9.7281e-02,  9.7079e-02,  2.7480e-02]],\n",
       "               \n",
       "                        [[-5.1792e-02, -1.7564e-02, -4.3937e-02],\n",
       "                         [ 2.8545e-03,  1.5756e-02,  1.9774e-02],\n",
       "                         [-3.6388e-02, -1.0959e-02, -2.5904e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2453e-02, -4.8331e-02, -3.6712e-02],\n",
       "                         [ 1.7493e-02,  3.4033e-02, -6.2692e-02],\n",
       "                         [ 1.7119e-02,  5.0712e-02,  4.4655e-02]],\n",
       "               \n",
       "                        [[ 2.1704e-02,  9.1125e-03, -2.3818e-02],\n",
       "                         [-8.7800e-03, -9.0027e-03,  3.1929e-02],\n",
       "                         [ 2.1412e-02,  1.5263e-02,  4.0311e-02]],\n",
       "               \n",
       "                        [[ 5.3241e-02,  5.1625e-02,  5.4779e-04],\n",
       "                         [ 7.4963e-03, -1.2384e-02, -3.9310e-03],\n",
       "                         [-8.3949e-03,  7.8017e-03,  1.6196e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1452e-02,  3.5427e-02,  6.8650e-03],\n",
       "                         [-2.2374e-02,  5.3086e-02, -1.7199e-02],\n",
       "                         [ 2.3440e-02, -3.9746e-02,  1.0775e-02]],\n",
       "               \n",
       "                        [[ 1.4481e-01,  5.6123e-02,  1.0310e-01],\n",
       "                         [ 4.4039e-02,  4.7696e-02,  6.7283e-02],\n",
       "                         [ 8.2448e-02,  7.2715e-02,  3.7615e-02]],\n",
       "               \n",
       "                        [[-1.1639e-01, -5.8458e-02, -3.1611e-02],\n",
       "                         [ 1.4560e-02, -6.3400e-02,  2.4541e-03],\n",
       "                         [ 4.3847e-03,  4.2422e-02,  7.3532e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2406e-02,  6.8514e-03,  6.3269e-02],\n",
       "                         [ 3.0760e-02,  6.7360e-02, -5.2397e-02],\n",
       "                         [-3.4711e-02, -1.0952e-02, -1.2569e-02]],\n",
       "               \n",
       "                        [[ 5.1711e-03, -4.6826e-02, -1.6512e-02],\n",
       "                         [-1.1610e-02, -6.0962e-02, -4.2392e-03],\n",
       "                         [-3.6009e-03,  9.5787e-03,  4.0561e-02]],\n",
       "               \n",
       "                        [[-5.0638e-02, -2.6063e-02, -7.6123e-04],\n",
       "                         [-6.3553e-02, -6.9383e-02, -4.5953e-02],\n",
       "                         [-2.3766e-02,  1.3966e-02,  3.7778e-02]]]], device='cuda:0')),\n",
       "              ('r3.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r3.c3.weights',\n",
       "               tensor([[[[-0.0457]],\n",
       "               \n",
       "                        [[ 0.0629]],\n",
       "               \n",
       "                        [[ 0.0052]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0307]],\n",
       "               \n",
       "                        [[-0.0269]],\n",
       "               \n",
       "                        [[-0.0951]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0570]],\n",
       "               \n",
       "                        [[-0.0313]],\n",
       "               \n",
       "                        [[ 0.1019]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0291]],\n",
       "               \n",
       "                        [[-0.0014]],\n",
       "               \n",
       "                        [[ 0.0061]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0440]],\n",
       "               \n",
       "                        [[-0.0117]],\n",
       "               \n",
       "                        [[ 0.0611]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0279]],\n",
       "               \n",
       "                        [[ 0.0377]],\n",
       "               \n",
       "                        [[ 0.0873]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0333]],\n",
       "               \n",
       "                        [[-0.0173]],\n",
       "               \n",
       "                        [[ 0.0918]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0479]],\n",
       "               \n",
       "                        [[-0.0959]],\n",
       "               \n",
       "                        [[ 0.0127]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0516]],\n",
       "               \n",
       "                        [[-0.0256]],\n",
       "               \n",
       "                        [[ 0.0434]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0193]],\n",
       "               \n",
       "                        [[-0.0982]],\n",
       "               \n",
       "                        [[ 0.0515]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0684]],\n",
       "               \n",
       "                        [[-0.0772]],\n",
       "               \n",
       "                        [[-0.0229]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1435]],\n",
       "               \n",
       "                        [[-0.0499]],\n",
       "               \n",
       "                        [[-0.0294]]]], device='cuda:0')),\n",
       "              ('r3.c3.mask',\n",
       "               tensor([[[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]]], device='cuda:0')),\n",
       "              ('r4.bn1.weight',\n",
       "               tensor([1.0197, 0.9384, 1.1390, 1.0016, 1.0644, 1.0353, 0.8476, 0.9879, 1.0328,\n",
       "                       1.1409, 1.0185, 0.9065, 0.8470, 0.9573, 0.8557, 0.9776, 0.9571, 1.0078,\n",
       "                       1.1119, 1.1658, 0.9138, 0.8903, 0.8223, 1.0211, 0.9232, 0.9098, 1.0201,\n",
       "                       0.9982, 0.9375, 1.0711, 0.7453, 1.0193, 0.8212, 1.1591, 0.9237, 0.9822,\n",
       "                       1.0374, 0.9086, 0.9025, 0.8993, 0.8913, 1.0272, 0.9556, 0.9480, 1.3681,\n",
       "                       0.8292, 1.1538, 0.8435, 0.9512, 1.0918, 0.9034, 1.1688, 1.0568, 0.9991,\n",
       "                       0.8825, 0.8894, 0.8459, 0.9313, 0.9799, 0.9058, 1.1368, 1.0518, 0.9293,\n",
       "                       0.9107, 0.8785, 0.9704, 0.9425, 1.0248, 1.0970, 0.8551, 0.9847, 1.1315,\n",
       "                       1.1429, 0.9076, 0.9340, 1.1795, 1.0598, 0.9507, 0.9982, 0.7873, 0.9815,\n",
       "                       0.9626, 0.8741, 0.9295, 1.0052, 0.9209, 0.8938, 0.8327, 0.8727, 0.9860,\n",
       "                       0.9132, 0.9707, 0.8844, 0.8107, 1.0065, 0.8770, 0.9258, 0.9691, 0.9221,\n",
       "                       0.9267, 0.8738, 0.8407, 0.9865, 0.8845, 0.8794, 1.0181, 0.8909, 0.8098,\n",
       "                       0.8611, 0.9658, 0.8461, 1.0831, 0.8537, 1.1062, 1.0883, 0.8191, 0.9297,\n",
       "                       1.0532, 0.9457, 1.1440, 0.9697, 0.8943, 1.0773, 1.0182, 0.9591, 0.8958,\n",
       "                       1.0009, 0.8846], device='cuda:0')),\n",
       "              ('r4.bn1.bias',\n",
       "               tensor([-0.1495,  0.0710, -0.3176, -0.3097,  0.0881,  0.1457, -0.0878, -0.2693,\n",
       "                       -0.1152, -0.1964, -0.2641, -0.1002, -0.0728, -0.0932,  0.1166,  0.0017,\n",
       "                       -0.1914,  0.1088,  0.1544,  0.2228, -0.4714, -0.0978,  0.3299, -0.2282,\n",
       "                       -0.1105, -0.0503,  0.0107,  0.1330, -0.1452, -0.7954,  0.0649, -0.3006,\n",
       "                       -0.1844, -0.2434,  0.0160, -0.6435, -0.1501, -0.3925,  0.1155,  0.0639,\n",
       "                        0.3235, -0.7231, -0.1008,  0.0586,  0.0485, -0.1537, -0.0081,  0.0610,\n",
       "                       -0.3036, -0.3119, -0.4247, -0.0258, -0.1253, -0.1726, -0.3303, -0.2425,\n",
       "                       -0.0849, -0.1354, -0.0437,  0.2559,  0.0027,  0.1178,  0.0910, -0.0199,\n",
       "                       -0.1832, -0.3501,  0.1508, -0.3482, -0.2499, -0.2692, -0.4178, -0.0730,\n",
       "                       -0.0465,  0.0705, -0.1474,  0.0148, -0.1660, -0.5048,  0.0710, -0.2304,\n",
       "                       -0.3387,  0.0081,  0.2084,  0.0242, -0.3399, -0.0860,  0.0292, -0.0398,\n",
       "                       -0.1183,  0.0574, -0.2974,  0.0228, -0.2065, -0.5449,  0.0432,  0.2192,\n",
       "                       -0.2693, -0.1550, -0.2631, -0.4075, -0.2129, -0.0919, -0.1590, -0.0994,\n",
       "                       -0.0927, -0.3896,  0.4985, -0.5364,  0.3756,  0.0957,  0.3017,  0.1020,\n",
       "                       -0.0712,  0.0935, -0.4944, -0.0170,  0.2488, -0.1015, -0.1148, -0.1279,\n",
       "                       -0.3274, -0.5941,  0.0013, -0.1453,  0.0585, -0.4417, -0.1097, -0.1308],\n",
       "                      device='cuda:0')),\n",
       "              ('r4.bn1.running_mean',\n",
       "               tensor([-6.2514e-02, -2.6815e-01, -1.0541e-01,  2.5349e-02, -1.2356e-01,\n",
       "                        4.3743e-02, -1.0278e-01,  7.4869e-02, -1.6589e-01, -1.7716e-01,\n",
       "                        4.3114e-02, -1.0786e-01, -3.0831e-03, -1.5592e-01,  1.1652e-01,\n",
       "                       -1.7361e-01, -4.2151e-01, -2.0162e-02,  4.7464e-02, -4.3966e-01,\n",
       "                       -1.8169e-01, -1.5107e-01,  2.1316e-01,  5.8468e-02,  9.6980e-02,\n",
       "                       -2.7273e-01,  2.0965e-01, -1.3590e-01, -1.6757e-01, -3.4665e-02,\n",
       "                       -1.7565e-01, -2.1252e-01, -3.0358e-01, -6.8125e-02,  8.0394e-02,\n",
       "                       -2.1039e-01, -1.5078e-01,  1.6007e-01, -4.0260e-01, -2.6693e-01,\n",
       "                        8.3553e-02,  1.0186e-01, -3.8698e-02,  5.9290e-02, -1.4225e-01,\n",
       "                        2.8897e-02,  1.1114e-01, -1.6512e-01, -6.9821e-02,  3.4514e-02,\n",
       "                       -1.7548e-01, -1.7690e-01,  9.1554e-02, -8.1324e-02, -1.2314e-01,\n",
       "                       -2.9844e-02,  7.8347e-03,  1.2181e-02,  1.4599e-01, -1.8155e-01,\n",
       "                       -2.3917e-01,  5.2749e-02,  2.0678e-04, -9.1368e-02, -1.5812e-01,\n",
       "                        1.1513e-01, -7.3212e-02,  9.2637e-02, -2.1074e-01, -4.9703e-02,\n",
       "                       -1.0741e-01, -1.4730e-01, -1.3549e-01,  8.7265e-03, -1.5970e-01,\n",
       "                       -2.1174e-01, -4.4298e-02, -1.0822e-01, -1.0848e-01, -1.0132e-01,\n",
       "                        1.1092e-02,  1.5929e-02, -3.0193e-01, -1.8679e-01, -9.8603e-02,\n",
       "                       -1.9708e-01,  1.2521e-01,  9.7301e-02,  1.0833e-01, -2.1375e-01,\n",
       "                       -1.3146e-01,  3.9126e-02, -2.8270e-03,  1.6629e-01,  2.4586e-01,\n",
       "                       -1.2599e-02, -1.5780e-01, -3.0062e-01, -2.0904e-01, -1.7642e-01,\n",
       "                       -1.0994e-01,  5.6743e-02, -1.9637e-01, -9.8438e-02,  1.9385e-01,\n",
       "                       -1.6889e-01, -5.1785e-02,  1.3709e-02,  1.5220e-01,  6.1015e-02,\n",
       "                        5.0794e-02, -3.4209e-01, -1.4882e-01, -6.1633e-02, -8.7556e-02,\n",
       "                       -7.0199e-02,  3.7528e-02, -1.9112e-01,  1.3705e-01, -2.7203e-01,\n",
       "                       -1.4989e-01, -5.5495e-02, -9.3656e-02,  2.5170e-02, -3.4810e-01,\n",
       "                        1.3438e-01, -1.7271e-01,  1.9649e-02], device='cuda:0')),\n",
       "              ('r4.bn1.running_var',\n",
       "               tensor([0.2124, 0.2966, 0.1614, 0.5274, 0.2715, 0.1848, 0.2535, 0.6423, 0.1966,\n",
       "                       0.2353, 0.2025, 0.1035, 0.2047, 0.2287, 0.2670, 0.2620, 0.2050, 0.3893,\n",
       "                       0.2055, 0.3120, 0.3807, 0.1466, 0.5568, 0.3355, 0.3643, 0.2213, 0.2826,\n",
       "                       0.2710, 0.2873, 0.6178, 0.3470, 0.5442, 0.2894, 0.2565, 0.1513, 0.4621,\n",
       "                       0.3118, 0.1926, 0.2497, 0.4562, 0.4828, 0.2067, 0.9285, 0.2423, 0.5538,\n",
       "                       0.4538, 0.3760, 0.6903, 0.4075, 0.2266, 0.1406, 0.3815, 0.3245, 0.2094,\n",
       "                       0.4205, 0.2256, 0.2388, 0.3055, 0.3850, 0.4159, 0.5176, 0.2791, 0.4501,\n",
       "                       0.5296, 0.2046, 0.4778, 0.2887, 0.2595, 0.2947, 0.2252, 0.2082, 0.3068,\n",
       "                       0.4570, 0.1524, 0.2862, 0.4245, 0.2629, 0.1512, 1.0909, 0.2771, 0.5448,\n",
       "                       0.2441, 0.6888, 0.8831, 0.6633, 0.2665, 0.4302, 0.2142, 0.3257, 0.8192,\n",
       "                       0.2339, 0.2420, 0.3218, 0.5501, 0.7704, 0.1908, 0.1649, 0.3933, 0.2890,\n",
       "                       0.4370, 0.1455, 0.5190, 0.6414, 0.1951, 0.2861, 0.2863, 0.3127, 0.1822,\n",
       "                       0.3091, 0.3961, 0.5112, 1.2833, 0.3048, 0.6915, 0.5101, 0.2723, 0.4943,\n",
       "                       0.2067, 0.1521, 0.1947, 0.4053, 0.1229, 0.1687, 0.1967, 0.3041, 0.3760,\n",
       "                       0.6296, 0.2945], device='cuda:0')),\n",
       "              ('r4.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r4.c1.weights',\n",
       "               tensor([[[[-2.0701e-02, -6.7124e-03, -8.3096e-02],\n",
       "                         [-4.2389e-02, -5.3368e-02, -1.0783e-01],\n",
       "                         [-2.0443e-03, -1.0593e-01, -9.7191e-02]],\n",
       "               \n",
       "                        [[ 8.7065e-02, -1.0189e-02,  3.6817e-02],\n",
       "                         [ 1.8459e-02,  5.8257e-02,  5.3128e-02],\n",
       "                         [-1.4945e-02,  6.0380e-02,  2.0462e-02]],\n",
       "               \n",
       "                        [[ 3.7297e-02, -1.5432e-02, -2.1116e-02],\n",
       "                         [ 1.3392e-03, -1.5028e-01, -8.3236e-05],\n",
       "                         [-1.5660e-02, -7.4512e-02, -4.3484e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4011e-03, -1.0289e-02, -2.8245e-02],\n",
       "                         [-4.7503e-02, -1.3015e-02, -6.2152e-04],\n",
       "                         [-2.9770e-02, -8.1689e-02, -2.5688e-02]],\n",
       "               \n",
       "                        [[-8.3011e-02, -5.6617e-02, -1.1814e-02],\n",
       "                         [-7.2711e-03,  5.1844e-03, -3.8252e-02],\n",
       "                         [-1.2486e-02, -1.9263e-03, -5.5109e-02]],\n",
       "               \n",
       "                        [[ 9.3657e-02,  4.6117e-02,  3.1390e-02],\n",
       "                         [ 8.7444e-02,  2.8517e-02,  2.0125e-02],\n",
       "                         [ 3.2281e-02,  1.1638e-02,  2.6405e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4350e-02, -6.7695e-02,  5.7502e-03],\n",
       "                         [-3.6249e-03, -2.2057e-02,  2.3060e-03],\n",
       "                         [-7.3621e-02, -7.5700e-02, -7.4602e-02]],\n",
       "               \n",
       "                        [[-2.4913e-02,  3.6492e-02, -5.6730e-02],\n",
       "                         [-4.5737e-03, -5.7642e-02,  3.2205e-02],\n",
       "                         [ 6.0124e-02, -5.0403e-02, -6.8068e-03]],\n",
       "               \n",
       "                        [[-1.7186e-02, -1.9775e-02, -1.4992e-01],\n",
       "                         [-9.3284e-02, -1.9182e-01,  2.0665e-02],\n",
       "                         [ 2.7030e-02, -1.0946e-01, -1.1137e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.8603e-03,  1.3439e-02, -3.8738e-02],\n",
       "                         [-1.8682e-02,  2.2263e-02,  8.3420e-03],\n",
       "                         [-9.3412e-02, -8.4533e-02, -6.9195e-02]],\n",
       "               \n",
       "                        [[ 4.4989e-02,  2.1456e-02, -3.0814e-02],\n",
       "                         [-4.4863e-02, -2.2473e-02, -1.5814e-02],\n",
       "                         [-8.6270e-03,  5.0136e-02,  4.6416e-02]],\n",
       "               \n",
       "                        [[ 8.5668e-02,  1.3737e-01,  7.4215e-02],\n",
       "                         [ 1.6296e-02, -4.7430e-02, -2.2730e-02],\n",
       "                         [ 1.3953e-02, -1.1080e-02,  4.0351e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4075e-01, -1.8304e-01, -1.1762e-01],\n",
       "                         [-1.0753e-01, -1.3083e-01, -3.7030e-02],\n",
       "                         [-3.0404e-02, -1.6685e-01, -4.7781e-02]],\n",
       "               \n",
       "                        [[-4.7818e-02,  1.6212e-03, -1.4781e-01],\n",
       "                         [-2.4018e-02, -2.7224e-03, -1.4936e-01],\n",
       "                         [-3.4087e-02, -1.4362e-02, -8.9598e-02]],\n",
       "               \n",
       "                        [[ 1.0315e-02,  1.8676e-02,  5.7153e-02],\n",
       "                         [ 4.4792e-02, -1.6442e-01,  1.8842e-02],\n",
       "                         [ 1.0126e-01, -1.7088e-02, -4.4026e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.5262e-02, -1.0397e-01, -7.7613e-02],\n",
       "                         [-8.7310e-02, -4.1479e-02, -1.1328e-01],\n",
       "                         [-5.3738e-02, -1.1851e-01, -1.0658e-01]],\n",
       "               \n",
       "                        [[-2.8483e-02, -1.2975e-02, -5.8719e-02],\n",
       "                         [ 2.1516e-02, -2.5276e-04,  7.1234e-03],\n",
       "                         [ 9.4386e-02,  7.3364e-02,  4.1800e-02]],\n",
       "               \n",
       "                        [[-8.4998e-03,  6.8525e-02, -2.9262e-02],\n",
       "                         [-4.3037e-04,  3.3313e-02,  2.8715e-02],\n",
       "                         [-1.9337e-02,  6.3064e-02,  8.7716e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.8326e-02, -5.7295e-02, -6.3381e-02],\n",
       "                         [-6.7238e-02, -9.2321e-02,  1.1447e-02],\n",
       "                         [-4.6883e-02, -7.7131e-02, -1.4115e-02]],\n",
       "               \n",
       "                        [[ 4.2320e-02,  2.5101e-02, -2.7233e-02],\n",
       "                         [-2.6296e-02,  5.0804e-03, -2.9953e-03],\n",
       "                         [ 4.1890e-02,  2.5621e-02,  2.5651e-02]],\n",
       "               \n",
       "                        [[-7.9106e-02, -1.9095e-01, -1.0232e-01],\n",
       "                         [-4.5479e-02, -8.3747e-02, -2.6148e-02],\n",
       "                         [-8.9248e-02, -3.2425e-02, -1.2641e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.2461e-02,  5.7031e-03, -2.6891e-02],\n",
       "                         [-6.7766e-03,  3.0134e-02, -4.9065e-02],\n",
       "                         [ 9.3087e-03, -2.0559e-02, -4.7624e-02]],\n",
       "               \n",
       "                        [[-5.8138e-02, -8.0674e-02, -3.4917e-02],\n",
       "                         [-1.0053e-02,  6.9456e-04,  4.7214e-02],\n",
       "                         [-8.1029e-02, -1.1630e-02,  2.4715e-02]],\n",
       "               \n",
       "                        [[ 2.2566e-02, -2.8033e-02, -9.0598e-02],\n",
       "                         [-3.6645e-02,  6.3887e-03, -3.5741e-02],\n",
       "                         [-8.1139e-02,  1.8818e-02, -4.7393e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.9630e-03, -9.6177e-03, -7.0942e-02],\n",
       "                         [-3.9061e-02, -8.5656e-03, -8.7356e-02],\n",
       "                         [-1.6355e-02,  6.3403e-02, -3.1198e-02]],\n",
       "               \n",
       "                        [[-3.5489e-02,  1.6512e-03, -1.7337e-02],\n",
       "                         [-1.9254e-02, -1.3197e-02, -3.7539e-02],\n",
       "                         [-8.1128e-02, -6.5050e-02, -1.5609e-02]],\n",
       "               \n",
       "                        [[-2.1383e-02,  2.7373e-02, -3.1080e-02],\n",
       "                         [-3.7807e-03,  1.8529e-02, -1.2296e-01],\n",
       "                         [ 1.6134e-02,  1.5258e-02, -5.9258e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.6049e-02, -2.6564e-02, -1.0038e-01],\n",
       "                         [ 1.8275e-02, -4.3866e-02, -7.0659e-02],\n",
       "                         [-1.4446e-02, -1.1328e-02, -9.8764e-02]],\n",
       "               \n",
       "                        [[-8.7356e-02, -5.5067e-02, -8.2183e-02],\n",
       "                         [-6.3952e-02, -1.0426e-01, -6.5356e-02],\n",
       "                         [ 8.4941e-03, -4.5303e-02, -2.9643e-02]],\n",
       "               \n",
       "                        [[-2.2568e-02, -8.1581e-02,  2.8747e-02],\n",
       "                         [-6.0030e-02, -6.1851e-02,  1.0629e-02],\n",
       "                         [-6.0040e-02, -8.1975e-02, -8.7878e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0196e-01, -1.0838e-01, -8.7481e-02],\n",
       "                         [-1.7778e-01, -1.2646e-01, -8.6558e-02],\n",
       "                         [-1.5047e-01, -1.3910e-01, -8.8632e-02]],\n",
       "               \n",
       "                        [[-4.1662e-03, -1.8109e-03,  5.6040e-02],\n",
       "                         [-2.7060e-02,  2.7150e-03,  2.4379e-02],\n",
       "                         [ 2.3785e-04,  7.6815e-02,  9.5915e-02]],\n",
       "               \n",
       "                        [[-5.4899e-02, -5.9670e-02, -1.2805e-01],\n",
       "                         [ 1.8772e-02, -1.5850e-02, -9.7900e-02],\n",
       "                         [ 3.3848e-02,  9.0799e-02, -4.5989e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.9531e-02, -8.8958e-02, -5.5845e-03],\n",
       "                         [-5.7516e-02, -9.2782e-02, -9.8022e-02],\n",
       "                         [-7.4768e-02, -9.9853e-02, -7.5665e-02]],\n",
       "               \n",
       "                        [[ 1.4258e-02, -5.5679e-02,  2.4317e-02],\n",
       "                         [-6.8598e-02, -7.4970e-02, -5.2604e-02],\n",
       "                         [-1.7993e-01, -1.5735e-01, -1.3615e-01]],\n",
       "               \n",
       "                        [[ 7.9182e-02,  8.3260e-02,  2.7045e-02],\n",
       "                         [ 1.0638e-01,  9.0367e-03, -3.3144e-03],\n",
       "                         [-2.5534e-03, -1.9954e-02, -4.8912e-02]]]], device='cuda:0')),\n",
       "              ('r4.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r4.bn2.weight',\n",
       "               tensor([0.5899, 0.7360, 0.9279, 0.5880, 0.5021, 0.5610, 0.5251, 0.8648, 0.8318,\n",
       "                       0.8333, 0.7503, 0.3464, 0.7808, 0.3635, 0.6267, 0.8098, 0.8610, 0.8442,\n",
       "                       0.8753, 0.8072, 0.8842, 0.6464, 0.8499, 0.6619, 0.8295, 0.8510, 0.4127,\n",
       "                       0.6762, 0.6337, 0.6980, 0.8533, 0.7866, 0.7115, 0.8483, 0.9284, 0.7929,\n",
       "                       0.6577, 0.4342, 0.8235, 0.3604, 0.7816, 0.7129, 0.8312, 0.8756, 0.8624,\n",
       "                       0.8713, 0.8741, 0.6486, 0.8390, 0.7126, 0.6719, 0.8752, 0.6208, 0.9094,\n",
       "                       0.6650, 0.7655, 0.7929, 0.7882, 0.7929, 0.7802, 0.6892, 0.3191, 0.4590,\n",
       "                       0.8049, 0.8541, 0.6607, 0.4546, 0.9102, 0.7610, 0.8430, 0.8968, 0.8624,\n",
       "                       0.8531, 0.7481, 0.8743, 0.8485, 0.5433, 0.8558, 0.8283, 0.8649, 0.8712,\n",
       "                       0.3517, 0.8871, 0.8561, 0.6703, 0.4712, 0.8628, 0.8122, 0.7997, 0.8256,\n",
       "                       0.8472, 0.8194, 0.8360, 0.8711, 0.8772, 0.8543, 0.7678, 0.4782, 0.7819,\n",
       "                       0.8410, 0.9191, 0.3705, 0.6508, 0.7571, 0.8716, 0.8927, 0.8460, 0.7413,\n",
       "                       0.8333, 0.8739, 0.8762, 0.6207, 0.8934, 0.6270, 0.8113, 0.7825, 0.8743,\n",
       "                       0.8480, 0.5125, 0.9120, 0.8750, 0.8738, 0.8754, 0.5649, 0.9106, 0.6210,\n",
       "                       0.8330, 0.7093], device='cuda:0')),\n",
       "              ('r4.bn2.bias',\n",
       "               tensor([-0.1641, -0.3835, -0.0658, -0.6119, -0.3839, -0.3109, -0.2260, -0.0553,\n",
       "                       -0.0770, -0.1156, -0.1032, -0.4661, -0.2371, -0.3048, -0.3082, -0.1675,\n",
       "                       -0.0874, -0.2069, -0.0664, -0.1906, -0.2471, -0.1746, -0.1472, -0.1637,\n",
       "                       -0.1407, -0.2167, -0.3526, -0.2092, -0.2213, -0.2826, -0.0624, -0.1390,\n",
       "                       -0.1496, -0.1330, -0.2678, -0.1197, -0.1728, -0.3726, -0.1293, -0.3142,\n",
       "                       -0.1416, -0.1942, -0.1670, -0.0250, -0.1938, -0.0481, -0.1690, -0.3145,\n",
       "                       -0.2891, -0.1267, -0.1693, -0.1830, -0.3034, -0.2708, -0.1784, -0.1900,\n",
       "                       -0.2284, -0.1625, -0.1253, -0.1657, -0.2510, -0.4356, -0.7191, -0.2125,\n",
       "                       -0.1347, -0.2296, -0.7036, -0.2443, -0.1584, -0.1423, -0.1389, -0.2246,\n",
       "                       -0.1631, -0.1300, -0.1509, -0.2477, -0.3056, -0.1298, -0.1305, -0.0839,\n",
       "                       -0.0417, -0.3795, -0.1988, -0.3522, -0.1761, -0.6544, -0.1271, -0.2300,\n",
       "                       -0.3298, -0.1674, -0.2266, -0.0187, -0.1632, -0.0919, -0.1438, -0.0718,\n",
       "                       -0.0179, -0.3635, -0.0718, -0.2498, -0.2651, -0.4105, -0.1975, -0.2040,\n",
       "                       -0.0779, -0.0995, -0.2125, -0.1546, -0.1719, -0.1395, -0.1997, -0.2966,\n",
       "                       -0.1821, -0.1948, -0.1190, -0.1945, -0.0971, -0.0954, -0.5515, -0.2247,\n",
       "                       -0.1374, -0.2022, -0.1697, -0.3383, -0.1905, -0.2734, -0.1935, -0.1696],\n",
       "                      device='cuda:0')),\n",
       "              ('r4.bn2.running_mean',\n",
       "               tensor([ -0.5519,  -2.9361,  -3.4711,   2.7192,  -1.8508,  -3.6288,   0.4428,\n",
       "                        -3.8559,  -3.4314,  -2.5350,  -1.2538,  -0.0755,  -7.2062,   0.0890,\n",
       "                        -1.8306,  -5.1902,  -2.1438,  -2.6898,  -1.3285,  -2.0223,  -4.3871,\n",
       "                        -1.3054,  -1.7624,  -5.3017,  -4.9624,  -4.1866,   0.3808,  -2.0366,\n",
       "                        -3.7416,  -2.4169,  -8.4011,  -6.0188,  -7.0096,  -5.3688,  -3.4369,\n",
       "                        -2.3541,  -2.5854,  -2.4196,  -3.0176,   5.2562,  -0.7296,  -1.6242,\n",
       "                        -1.6992,  -2.1969,  -2.6593, -10.4140,  -5.5639,  -2.1605,  -2.4790,\n",
       "                        -4.4262,  -2.3062,  -3.0585,  -6.9343,  -2.1645,  -1.7754,  -0.6304,\n",
       "                        -3.1817,  -1.6215,  -2.4833,  -2.6921,  -2.1680,  -2.2474,   2.8162,\n",
       "                        -2.6278,  -6.7151,  -3.7609,   2.9484,  -2.8691,  -2.5440,  -1.8778,\n",
       "                        -2.3301,  -1.7107,  -5.8433,  -2.7236,  -2.0750,  -3.4407,  -1.7838,\n",
       "                        -0.9766,  -6.2004,  -1.5282,  -8.2105,  -2.7113,  -3.3875,  -3.2955,\n",
       "                        -5.7438,   2.3478,  -2.4837,  -3.3218,  -1.1166,  -3.7548,  -4.1097,\n",
       "                        -7.2048,  -5.9346,  -5.4907,  -5.6436,  -4.3324,  -5.8304,  -2.2860,\n",
       "                        -6.2331,  -3.7409,  -5.0723,  -3.7776,  -1.3037,  -2.1585,  -8.3150,\n",
       "                        -7.8563,  -1.6652,   2.0258,  -2.4013,  -5.5839,  -1.8470,  -1.8287,\n",
       "                        -1.5125,  -2.1949,  -4.3853,  -2.6826,  -7.1592,  -4.1266,   2.8493,\n",
       "                        -6.1351,  -2.6646,  -3.8047,  -2.0367,  -1.5403,  -1.6450,  -3.1972,\n",
       "                        -7.3865,  -5.3912], device='cuda:0')),\n",
       "              ('r4.bn2.running_var',\n",
       "               tensor([ 74.6973,  14.7500, 176.4199,  26.5441,  39.5453,  66.5100,  36.4590,\n",
       "                       209.9697, 235.8192, 179.5257, 131.6800,  21.1018,  86.6611,  45.4783,\n",
       "                        20.5544, 190.5841, 144.5226,  69.8679, 133.8676,  90.6800,  64.2805,\n",
       "                        50.1606, 128.2945,  87.7885, 150.6814,  94.4226,  38.8046,  73.4731,\n",
       "                        66.9571,  56.6041, 236.4249, 131.1115,  74.2868, 198.0058,  54.7358,\n",
       "                        79.6584,  42.3792,  33.8196, 195.4823,  17.9369,  88.5322,  70.1156,\n",
       "                       108.5254, 174.6455,  82.6139, 287.1449, 122.6838,  15.8298,  73.3860,\n",
       "                       201.3462,  51.3403, 138.1251,  48.8632,  44.1395,  70.4481,  67.9944,\n",
       "                        62.3545,  44.8277, 111.5166,  84.1673,  52.9258,  30.0016,  29.7460,\n",
       "                        89.7146, 218.3987,  84.5291,  32.2418,  80.7082,  48.2369, 138.7767,\n",
       "                        95.1245,  93.0191, 101.0114,  94.8722, 101.7097,  65.6303,  24.7460,\n",
       "                        74.6671, 101.6891, 139.1264, 311.1356,  41.2017, 152.4571,  28.4577,\n",
       "                        64.1535,  32.4904, 175.7481,  52.8007,  46.8374,  83.4703,  53.1225,\n",
       "                       312.0416, 104.5450, 260.9738, 122.6493, 131.3488, 225.0193,  34.3231,\n",
       "                       156.9039,  51.4893,  73.6450,  37.6467,  42.0490,  98.8510, 251.4523,\n",
       "                       192.3532, 116.5023,  78.4739,  93.7732, 257.3903, 108.3223,  39.3067,\n",
       "                        47.0219,  40.6934,  94.0464,  88.5688, 299.5186, 164.2583,  32.3684,\n",
       "                        81.2384, 111.9084, 154.2418,  67.5137,  26.2529,  77.8959,  47.8247,\n",
       "                       111.6241, 162.4984], device='cuda:0')),\n",
       "              ('r4.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r4.c2.weights',\n",
       "               tensor([[[[-0.0129, -0.0143,  0.0793],\n",
       "                         [-0.0023, -0.0208,  0.0268],\n",
       "                         [ 0.0249,  0.0930,  0.0527]],\n",
       "               \n",
       "                        [[-0.0076, -0.0220, -0.0489],\n",
       "                         [-0.0045, -0.0011, -0.0332],\n",
       "                         [-0.0145,  0.0436,  0.0894]],\n",
       "               \n",
       "                        [[-0.0064, -0.0291,  0.0502],\n",
       "                         [-0.0492, -0.0480,  0.0737],\n",
       "                         [ 0.0280, -0.0026, -0.0082]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0560,  0.0438, -0.0014],\n",
       "                         [ 0.0096,  0.0300, -0.0433],\n",
       "                         [-0.0255, -0.0109, -0.0113]],\n",
       "               \n",
       "                        [[ 0.0109,  0.0371,  0.0176],\n",
       "                         [-0.0242,  0.0560, -0.0029],\n",
       "                         [-0.0037, -0.0475,  0.0043]],\n",
       "               \n",
       "                        [[-0.0325, -0.0498,  0.0655],\n",
       "                         [-0.0155,  0.0125,  0.0781],\n",
       "                         [ 0.0036, -0.0052,  0.0970]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0102, -0.0034, -0.0501],\n",
       "                         [ 0.0207, -0.0328, -0.0556],\n",
       "                         [ 0.0161, -0.0027, -0.0530]],\n",
       "               \n",
       "                        [[ 0.0190, -0.0051, -0.0279],\n",
       "                         [ 0.0170,  0.0653,  0.0140],\n",
       "                         [ 0.0453,  0.0336,  0.0191]],\n",
       "               \n",
       "                        [[ 0.0571,  0.0256, -0.0105],\n",
       "                         [ 0.1046,  0.0052, -0.0207],\n",
       "                         [ 0.1208,  0.0600,  0.0370]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0673,  0.0042, -0.0202],\n",
       "                         [-0.0088, -0.0309, -0.0021],\n",
       "                         [ 0.0493,  0.0589, -0.0095]],\n",
       "               \n",
       "                        [[ 0.1097,  0.0603,  0.0336],\n",
       "                         [ 0.0245,  0.0091, -0.0292],\n",
       "                         [ 0.0336, -0.0120, -0.0198]],\n",
       "               \n",
       "                        [[-0.0230, -0.1113, -0.0258],\n",
       "                         [-0.0432, -0.0825, -0.0593],\n",
       "                         [ 0.0523, -0.0209,  0.0403]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0036, -0.0363,  0.0295],\n",
       "                         [-0.0859,  0.0160, -0.0085],\n",
       "                         [ 0.0007, -0.0003,  0.0370]],\n",
       "               \n",
       "                        [[-0.0113,  0.0268,  0.0220],\n",
       "                         [-0.0509, -0.0637, -0.0071],\n",
       "                         [-0.0258, -0.0275,  0.0441]],\n",
       "               \n",
       "                        [[ 0.0045,  0.0382, -0.0200],\n",
       "                         [ 0.0105,  0.0332, -0.0370],\n",
       "                         [ 0.0079,  0.0049, -0.0446]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0680,  0.0758,  0.0373],\n",
       "                         [ 0.0019,  0.0039,  0.0165],\n",
       "                         [ 0.0048, -0.0229, -0.0185]],\n",
       "               \n",
       "                        [[-0.0716,  0.0248,  0.1126],\n",
       "                         [-0.0448,  0.0311,  0.1585],\n",
       "                         [-0.0557,  0.0321,  0.1847]],\n",
       "               \n",
       "                        [[-0.0037, -0.0243, -0.0477],\n",
       "                         [-0.0511, -0.0412, -0.0129],\n",
       "                         [ 0.0030,  0.0227, -0.0813]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0759, -0.0182,  0.0194],\n",
       "                         [-0.0175, -0.0322,  0.0163],\n",
       "                         [-0.0246, -0.0229, -0.0122]],\n",
       "               \n",
       "                        [[ 0.0405,  0.0299,  0.0420],\n",
       "                         [-0.0213,  0.0073, -0.0208],\n",
       "                         [ 0.0064,  0.0242,  0.0189]],\n",
       "               \n",
       "                        [[-0.0257,  0.0169,  0.0824],\n",
       "                         [ 0.0379,  0.0149,  0.0928],\n",
       "                         [ 0.0292,  0.0510,  0.1439]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0235, -0.0069, -0.0295],\n",
       "                         [-0.0279, -0.0530,  0.0159],\n",
       "                         [-0.0580, -0.0812, -0.0395]],\n",
       "               \n",
       "                        [[-0.0102,  0.0339,  0.0362],\n",
       "                         [ 0.0549,  0.0770, -0.0205],\n",
       "                         [ 0.0616,  0.0353,  0.0828]],\n",
       "               \n",
       "                        [[-0.0645, -0.1271, -0.0578],\n",
       "                         [-0.0568, -0.0927, -0.0186],\n",
       "                         [-0.0655, -0.0251, -0.0028]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0352,  0.0179, -0.0350],\n",
       "                         [ 0.0518,  0.0366, -0.0204],\n",
       "                         [-0.0340, -0.0042, -0.0225]],\n",
       "               \n",
       "                        [[ 0.0243,  0.0149, -0.0133],\n",
       "                         [-0.0524,  0.0108,  0.0103],\n",
       "                         [ 0.0090,  0.0336,  0.0653]],\n",
       "               \n",
       "                        [[ 0.0609,  0.0485, -0.0281],\n",
       "                         [ 0.0588,  0.0365, -0.0188],\n",
       "                         [ 0.0878,  0.0973,  0.0566]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0179, -0.0320,  0.0073],\n",
       "                         [-0.0143, -0.0398, -0.0641],\n",
       "                         [ 0.0045,  0.0550,  0.0289]],\n",
       "               \n",
       "                        [[ 0.1206,  0.0271,  0.0611],\n",
       "                         [-0.0054,  0.0709, -0.0165],\n",
       "                         [-0.0645, -0.0332, -0.0198]],\n",
       "               \n",
       "                        [[ 0.0393,  0.0375,  0.0611],\n",
       "                         [ 0.0589, -0.0504, -0.0010],\n",
       "                         [ 0.0137, -0.0142,  0.0153]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0352,  0.0186,  0.0338],\n",
       "                         [-0.0163, -0.0238, -0.0396],\n",
       "                         [-0.0518, -0.0614, -0.0164]],\n",
       "               \n",
       "                        [[-0.0286, -0.0522, -0.0501],\n",
       "                         [ 0.0140, -0.0510,  0.0138],\n",
       "                         [-0.0124,  0.1066,  0.0435]],\n",
       "               \n",
       "                        [[ 0.0508,  0.0053, -0.0416],\n",
       "                         [-0.0322, -0.0678, -0.0399],\n",
       "                         [-0.0488, -0.0895, -0.0968]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0460, -0.0785, -0.0507],\n",
       "                         [-0.0458, -0.0130, -0.0228],\n",
       "                         [-0.0972, -0.0670,  0.0275]],\n",
       "               \n",
       "                        [[-0.0193,  0.0061,  0.0081],\n",
       "                         [-0.0353, -0.0393,  0.0224],\n",
       "                         [-0.0991, -0.0716, -0.0258]],\n",
       "               \n",
       "                        [[-0.0265, -0.0629, -0.0387],\n",
       "                         [-0.0071, -0.0168, -0.0725],\n",
       "                         [ 0.0043,  0.0161,  0.0205]]]], device='cuda:0')),\n",
       "              ('r4.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r5.bn1.weight',\n",
       "               tensor([1.0851, 0.9129, 0.7819, 1.0151, 1.0220, 0.9854, 1.0129, 1.0943, 1.0123,\n",
       "                       0.8981, 1.1121, 1.0479, 1.1369, 1.0345, 0.8015, 0.9683, 0.9752, 0.9107,\n",
       "                       0.9080, 1.0179, 0.7341, 0.9774, 1.0322, 0.9373, 0.9802, 0.8647, 0.9560,\n",
       "                       1.0098, 0.9980, 0.9090, 0.8128, 0.9366, 0.8820, 0.9805, 0.9828, 0.9001,\n",
       "                       0.8450, 0.9796, 1.0044, 0.9313, 0.9417, 0.9668, 1.2026, 1.1870, 1.1055,\n",
       "                       0.8001, 1.1790, 0.9283, 1.0569, 0.9879, 0.8615, 1.0021, 1.1782, 1.0736,\n",
       "                       0.6804, 0.7648, 0.9181, 0.8486, 1.1668, 1.0122, 0.9388, 1.0863, 0.9836,\n",
       "                       0.9321, 0.9501, 1.0509, 1.2568, 0.8459, 0.8795, 0.6236, 0.7779, 0.9270,\n",
       "                       0.9775, 0.9397, 0.8980, 0.9639, 1.0859, 0.9374, 1.0456, 1.0380, 0.9742,\n",
       "                       0.7963, 0.9643, 0.9641, 0.9420, 0.8721, 0.9546, 0.9865, 0.8547, 1.1474,\n",
       "                       0.8615, 0.8107, 0.7914, 0.8408, 0.8855, 0.8496, 1.0721, 1.1338, 0.8821,\n",
       "                       0.8127, 0.8534, 0.8689, 0.9416, 1.0475, 0.9053, 1.0120, 0.8368, 0.7223,\n",
       "                       0.9778, 0.9364, 1.1224, 1.0848, 0.9548, 1.1633, 1.1477, 1.1260, 1.0994,\n",
       "                       1.0238, 1.0566, 0.9110, 1.0496, 0.9668, 1.0210, 0.8591, 0.8908, 0.8179,\n",
       "                       1.0005, 0.8779], device='cuda:0')),\n",
       "              ('r5.bn1.bias',\n",
       "               tensor([-0.1001, -0.0606, -0.2262, -0.2365, -0.1994, -0.0663,  0.0874, -0.2470,\n",
       "                       -0.0691, -0.1721, -0.3984, -0.0163, -0.0725, -0.2225, -0.0985, -0.1228,\n",
       "                       -0.2460,  0.0276, -0.0522, -0.1631, -0.0440,  0.0318,  0.1935, -0.1147,\n",
       "                       -0.0289, -0.0944,  0.0565, -0.0012, -0.2707, -0.4129, -0.0766, -0.2306,\n",
       "                       -0.0879, -0.3669,  0.0248, -0.1474, -0.3955, -0.1930, -0.3420, -0.1134,\n",
       "                        0.2500, -0.2987, -0.1132, -0.3091, -0.1347, -0.3279, -0.0182,  0.0123,\n",
       "                       -0.2576, -0.2414, -0.3213, -0.1800,  0.1369, -0.1875, -0.2491, -0.2743,\n",
       "                        0.0284, -0.2637, -0.1059, -0.0493,  0.0877,  0.1195, -0.2970, -0.1349,\n",
       "                       -0.1500, -0.3176,  0.0257, -0.4123, -0.2979, -0.1376, -0.1496, -0.3828,\n",
       "                       -0.4216, -0.0356, -0.3119, -0.1879, -0.1306, -0.4794,  0.1954, -0.3678,\n",
       "                       -0.0892,  0.0456, -0.0520, -0.0425, -0.3174, -0.2575, -0.0242,  0.0867,\n",
       "                       -0.0728, -0.0982, -0.2465, -0.0204, -0.2889, -0.1438, -0.1371, -0.1143,\n",
       "                       -0.4091, -0.2491,  0.0110, -0.2557, -0.2041, -0.0936, -0.1491, -0.1363,\n",
       "                        0.0033, -0.1345,  0.0217, -0.2852,  0.1562, -0.2523, -0.1429,  0.1451,\n",
       "                       -0.0475, -0.0607, -0.5239, -0.1565,  0.0107, -0.3718,  0.0070, -0.3270,\n",
       "                       -0.0163, -0.3714, -0.0970, -0.2870, -0.1510, -0.2478, -0.1249, -0.1741],\n",
       "                      device='cuda:0')),\n",
       "              ('r5.bn1.running_mean',\n",
       "               tensor([-0.0184, -0.1015, -0.1328, -0.0539, -0.0920, -0.1816,  0.0951,  0.1132,\n",
       "                       -0.2000, -0.1190,  0.0673, -0.1088,  0.0388, -0.1383,  0.1776, -0.1046,\n",
       "                       -0.1553, -0.1626,  0.1186, -0.4968,  0.0367, -0.1437, -0.0571,  0.0036,\n",
       "                        0.0276, -0.0620,  0.0835,  0.0358, -0.0924, -0.0146,  0.0500, -0.2105,\n",
       "                       -0.2850, -0.1470, -0.0720, -0.0737, -0.1308,  0.1360, -0.0067, -0.1391,\n",
       "                        0.0710,  0.2211, -0.0055,  0.0613, -0.1755, -0.1101, -0.0741, -0.0006,\n",
       "                       -0.0289,  0.0284, -0.2098, -0.1839, -0.2831, -0.0973,  0.0678,  0.0320,\n",
       "                        0.0765, -0.0280,  0.1078, -0.0902, -0.2316, -0.0476,  0.0309, -0.0863,\n",
       "                       -0.1304,  0.1884, -0.3061,  0.0562, -0.2091, -0.0030, -0.1751, -0.2371,\n",
       "                       -0.1921,  0.1833, -0.1276, -0.1738,  0.0306, -0.0697, -0.0804,  0.0094,\n",
       "                       -0.0016,  0.0566, -0.1851, -0.2447, -0.1508, -0.1872,  0.0034, -0.0344,\n",
       "                        0.0683,  0.0188, -0.1218,  0.1139,  0.0072,  0.0185,  0.2362,  0.1080,\n",
       "                       -0.0026, -0.2773, -0.1338, -0.1388, -0.0020,  0.0507, -0.1479,  0.0508,\n",
       "                        0.1088,  0.1029, -0.0452,  0.0826, -0.1378,  0.0666,  0.0880, -0.2421,\n",
       "                       -0.0767, -0.1534, -0.1029, -0.0866, -0.0167,  0.0661,  0.0556, -0.1538,\n",
       "                       -0.1007,  0.0309,  0.0007, -0.1394, -0.1544,  0.1089,  0.0177, -0.0460],\n",
       "                      device='cuda:0')),\n",
       "              ('r5.bn1.running_var',\n",
       "               tensor([0.5825, 0.2774, 0.1773, 0.4054, 0.3583, 0.3596, 0.5860, 0.5547, 0.3742,\n",
       "                       0.2391, 0.6224, 0.6539, 0.8385, 0.4306, 0.3590, 0.4812, 0.5095, 0.3461,\n",
       "                       0.2580, 0.3530, 0.2573, 0.4971, 0.5510, 0.3772, 0.5971, 0.3615, 0.3396,\n",
       "                       0.3911, 0.4495, 0.3384, 0.3169, 0.3925, 0.4780, 0.3379, 0.4464, 0.2799,\n",
       "                       0.2888, 0.4646, 0.3797, 0.2725, 0.5092, 0.7112, 1.0296, 0.7161, 0.5582,\n",
       "                       0.2875, 0.6074, 1.0470, 0.3851, 0.4652, 0.2623, 0.5038, 0.6330, 0.3584,\n",
       "                       0.2406, 0.3517, 0.5892, 0.3318, 0.8709, 0.5755, 0.4720, 0.5748, 0.4835,\n",
       "                       0.4812, 0.5000, 0.4913, 0.7347, 0.2581, 0.3203, 0.2006, 0.3553, 0.2374,\n",
       "                       0.3579, 0.2882, 0.3249, 0.4275, 0.7076, 0.3586, 1.1109, 0.3925, 0.3093,\n",
       "                       0.2990, 0.4735, 0.9119, 0.3135, 0.3374, 0.5098, 0.3917, 0.4593, 1.0519,\n",
       "                       0.2375, 0.2424, 0.2344, 0.2479, 0.8124, 0.2449, 0.6980, 0.5553, 0.5229,\n",
       "                       0.3163, 0.2780, 0.3656, 0.3733, 0.6643, 0.4179, 0.2856, 0.2780, 0.2184,\n",
       "                       0.5293, 0.3926, 0.6249, 1.4611, 0.3905, 0.7243, 0.3612, 0.6771, 0.8000,\n",
       "                       0.6623, 0.4966, 0.2692, 0.4720, 0.5832, 0.6409, 0.3045, 0.2482, 0.3159,\n",
       "                       0.7236, 0.5165], device='cuda:0')),\n",
       "              ('r5.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r5.c1.weights',\n",
       "               tensor([[[[ 1.9241e-02,  4.9350e-02,  6.6133e-02],\n",
       "                         [-6.3402e-03, -3.1675e-02, -5.6105e-03],\n",
       "                         [-7.4265e-02, -1.2926e-01, -1.2055e-01]],\n",
       "               \n",
       "                        [[ 1.9226e-02, -7.0685e-02, -2.6467e-02],\n",
       "                         [ 1.2502e-02, -3.8253e-02, -1.3713e-02],\n",
       "                         [ 6.5242e-03, -1.0071e-02,  5.7060e-03]],\n",
       "               \n",
       "                        [[-1.2974e-02,  1.8972e-02, -1.4650e-02],\n",
       "                         [ 1.5587e-02,  1.0252e-02, -8.8409e-02],\n",
       "                         [ 3.8305e-03, -4.4339e-03, -1.5224e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0120e-01, -1.2035e-01, -1.7067e-01],\n",
       "                         [-6.4071e-04, -1.0248e-01, -1.0454e-01],\n",
       "                         [ 6.6513e-03, -2.0156e-02, -9.4324e-02]],\n",
       "               \n",
       "                        [[ 9.4938e-02,  6.8195e-02,  7.7338e-02],\n",
       "                         [-1.0417e-01, -1.0234e-01, -8.0847e-02],\n",
       "                         [-2.2272e-01, -2.5229e-01, -1.7893e-01]],\n",
       "               \n",
       "                        [[ 1.0636e-01,  1.3294e-01,  1.6063e-01],\n",
       "                         [-3.0108e-03, -2.3134e-03,  9.9866e-02],\n",
       "                         [-3.8285e-02, -5.1281e-02, -1.6056e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.6761e-02, -9.2073e-02, -4.7402e-02],\n",
       "                         [-5.2900e-02, -6.3638e-02, -8.9701e-02],\n",
       "                         [-1.6111e-02, -2.9705e-02, -1.8814e-02]],\n",
       "               \n",
       "                        [[-7.4934e-02, -5.3465e-02, -2.1726e-02],\n",
       "                         [-3.4522e-02, -5.7913e-02, -3.0957e-02],\n",
       "                         [-1.3635e-01, -1.8456e-01, -7.5479e-02]],\n",
       "               \n",
       "                        [[-7.9025e-02, -5.9491e-02, -6.7273e-02],\n",
       "                         [-1.0978e-01, -4.5679e-02, -2.8750e-03],\n",
       "                         [-1.1070e-01, -3.2520e-02,  3.0847e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.5646e-02, -2.7543e-02, -1.8290e-02],\n",
       "                         [-3.8544e-02, -1.3208e-02, -3.5008e-02],\n",
       "                         [ 5.4373e-02,  1.8624e-02,  7.1973e-02]],\n",
       "               \n",
       "                        [[-1.2780e-01, -1.4990e-01, -1.1144e-01],\n",
       "                         [-2.4488e-02,  2.0351e-02, -2.2712e-02],\n",
       "                         [ 1.1404e-01,  1.1856e-01,  8.7317e-02]],\n",
       "               \n",
       "                        [[-1.5984e-01, -4.5158e-02, -4.8831e-02],\n",
       "                         [-5.7677e-02, -2.0796e-02, -6.7902e-02],\n",
       "                         [-8.6590e-02, -2.2693e-02, -1.9693e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.6072e-02, -1.2738e-02, -1.8386e-02],\n",
       "                         [-8.7660e-02, -2.8021e-02, -4.6714e-02],\n",
       "                         [-8.4275e-02, -2.4202e-02, -2.0433e-02]],\n",
       "               \n",
       "                        [[-2.2185e-02, -3.1349e-02, -6.4816e-02],\n",
       "                         [ 6.2002e-04,  1.7257e-02, -3.1948e-02],\n",
       "                         [-1.9388e-02,  3.4332e-02, -2.0376e-02]],\n",
       "               \n",
       "                        [[-4.9431e-02,  4.4673e-02, -3.7346e-02],\n",
       "                         [-2.1045e-03,  5.4496e-02, -4.3712e-03],\n",
       "                         [-3.6761e-02,  1.4031e-02, -3.3110e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.6366e-03, -3.4109e-04,  1.7214e-03],\n",
       "                         [ 8.9217e-03, -3.0947e-02,  2.2919e-03],\n",
       "                         [-4.9184e-02,  1.2427e-02,  6.2853e-03]],\n",
       "               \n",
       "                        [[-9.6863e-03, -1.3901e-04, -3.2500e-02],\n",
       "                         [-1.3244e-02, -3.4299e-02, -9.6499e-02],\n",
       "                         [-5.2844e-02, -8.3878e-02, -1.3509e-01]],\n",
       "               \n",
       "                        [[ 2.2550e-02,  9.8573e-03,  7.4773e-02],\n",
       "                         [-6.3315e-03, -1.8078e-02,  5.3015e-02],\n",
       "                         [ 4.5641e-02,  4.1886e-02,  4.5693e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.0503e-02, -6.6620e-02, -4.5231e-02],\n",
       "                         [-1.3073e-02, -2.9709e-02, -1.0249e-01],\n",
       "                         [-5.1564e-02, -2.8101e-02, -2.8771e-02]],\n",
       "               \n",
       "                        [[ 6.7559e-02, -3.9693e-02, -1.6254e-02],\n",
       "                         [ 7.3690e-02, -2.2900e-03, -2.3995e-02],\n",
       "                         [-3.2495e-02,  1.8904e-02,  2.9496e-02]],\n",
       "               \n",
       "                        [[-5.4342e-02, -3.4341e-02, -4.9037e-02],\n",
       "                         [-4.6075e-02, -4.6773e-02, -7.9933e-02],\n",
       "                         [-2.4499e-02, -1.0879e-01, -8.6778e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1693e-01, -9.9836e-02, -4.5112e-02],\n",
       "                         [-4.4480e-02, -6.3714e-02,  4.1881e-03],\n",
       "                         [-2.2427e-02,  5.4961e-02,  1.6103e-02]],\n",
       "               \n",
       "                        [[-6.0468e-02, -2.5995e-03, -5.3512e-03],\n",
       "                         [-5.3952e-03, -1.8078e-02,  2.6742e-03],\n",
       "                         [-9.1014e-03, -2.4572e-02, -6.4074e-02]],\n",
       "               \n",
       "                        [[-3.8843e-03,  5.6895e-02, -9.7605e-02],\n",
       "                         [ 3.4599e-02,  3.6161e-03, -7.1734e-02],\n",
       "                         [-3.5134e-02, -5.2084e-02, -1.3685e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4379e-01, -1.5091e-01, -7.9223e-02],\n",
       "                         [-7.2447e-02, -7.4824e-02, -2.9174e-02],\n",
       "                         [ 2.6216e-02, -1.3797e-03,  2.7362e-03]],\n",
       "               \n",
       "                        [[-4.6291e-02, -1.2367e-02,  1.9262e-02],\n",
       "                         [ 5.1477e-02,  5.4796e-02,  8.9764e-02],\n",
       "                         [-1.5134e-02,  1.8013e-02,  9.5139e-02]],\n",
       "               \n",
       "                        [[ 6.4132e-02,  6.8071e-02, -1.8351e-02],\n",
       "                         [ 9.6640e-02,  1.7139e-01,  4.9671e-02],\n",
       "                         [ 4.1079e-02,  1.3749e-01,  8.8912e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.3081e-02,  3.6914e-02,  1.0826e-01],\n",
       "                         [-1.1103e-02, -1.6310e-02,  9.2133e-02],\n",
       "                         [-6.4347e-03,  2.6567e-02,  1.6354e-02]],\n",
       "               \n",
       "                        [[-9.9040e-02, -2.7540e-02, -6.8304e-02],\n",
       "                         [-1.1647e-01, -1.4104e-02,  1.3951e-01],\n",
       "                         [-6.7012e-02, -5.7076e-02,  5.3950e-02]],\n",
       "               \n",
       "                        [[-3.1281e-03, -2.9581e-04,  2.9059e-02],\n",
       "                         [ 6.9021e-02,  1.0030e-02, -1.1128e-01],\n",
       "                         [ 7.0771e-02,  5.2497e-03, -1.6555e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.7740e-02, -1.3188e-01, -1.3414e-01],\n",
       "                         [-1.6810e-01, -1.4694e-01, -8.9223e-02],\n",
       "                         [-8.1372e-02, -1.2141e-01, -6.5201e-02]],\n",
       "               \n",
       "                        [[-5.0778e-02, -7.4459e-02, -2.6302e-04],\n",
       "                         [ 2.4958e-02, -2.3893e-02,  2.0280e-02],\n",
       "                         [ 4.6680e-03, -3.7143e-03,  2.1981e-02]],\n",
       "               \n",
       "                        [[ 2.3462e-02, -1.2353e-02, -3.2129e-02],\n",
       "                         [-3.6792e-02,  1.3063e-02, -3.4040e-02],\n",
       "                         [-1.1450e-02, -1.6029e-02, -2.9029e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8649e-02, -2.2227e-02, -3.8868e-02],\n",
       "                         [-7.6610e-02, -8.3604e-02, -1.5929e-02],\n",
       "                         [-2.2344e-02,  4.4593e-02,  4.8106e-03]],\n",
       "               \n",
       "                        [[-8.1312e-02, -1.2993e-01, -1.1580e-01],\n",
       "                         [-1.6280e-02, -5.0141e-02, -6.6088e-02],\n",
       "                         [ 3.0181e-02,  3.6010e-02,  7.6204e-02]],\n",
       "               \n",
       "                        [[-4.6931e-02, -4.5561e-02, -6.0898e-02],\n",
       "                         [-3.6711e-02, -4.0537e-02, -2.3634e-02],\n",
       "                         [ 8.3786e-02,  4.6857e-02,  5.4552e-02]]]], device='cuda:0')),\n",
       "              ('r5.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r5.bn2.weight',\n",
       "               tensor([0.8124, 0.8528, 0.6115, 0.9037, 0.7129, 0.9340, 0.8906, 0.8197, 0.5503,\n",
       "                       0.8163, 0.8438, 0.7107, 0.8664, 0.8572, 0.8522, 0.8478, 0.8142, 0.7797,\n",
       "                       0.9075, 0.8499, 0.8599, 0.4345, 0.7755, 0.7402, 0.8360, 0.8466, 0.9365,\n",
       "                       0.8419, 0.8365, 0.8971, 0.8267, 0.8747, 0.8770, 0.9139, 0.7901, 0.6043,\n",
       "                       0.8953, 0.9148, 0.8795, 0.7781, 0.8061, 0.7102, 0.8171, 0.5543, 0.8695,\n",
       "                       0.8509, 0.8811, 0.8731, 0.8655, 0.8284, 0.8670, 0.9378, 0.8416, 0.9008,\n",
       "                       0.8974, 0.7717, 0.9294, 0.9322, 0.8526, 0.8540, 0.8405, 0.9286, 0.8766,\n",
       "                       0.5828, 0.8454, 0.7486, 0.8829, 0.8894, 0.7531, 0.7446, 0.7874, 0.8338,\n",
       "                       0.8936, 0.8619, 0.9177, 0.8566, 0.8571, 0.8449, 0.8314, 0.8685, 0.6991,\n",
       "                       0.8727, 0.7859, 0.8929, 0.6932, 0.6873, 0.9140, 0.8384, 0.8798, 0.8130,\n",
       "                       0.9825, 0.5489, 0.9411, 0.8956, 0.8861, 0.9274, 0.8983, 0.8615, 0.9297,\n",
       "                       0.7621, 0.8479, 0.8015, 0.8267, 0.8954, 0.8891, 1.0545, 0.8473, 0.8067,\n",
       "                       0.8911, 0.8845, 0.8592, 0.8542, 0.8748, 0.8669, 0.8308, 0.9465, 0.9137,\n",
       "                       0.8597, 0.8936, 0.9043, 0.8741, 0.9021, 0.8574, 0.8736, 0.4829, 0.9126,\n",
       "                       0.9169, 0.8889, 0.8710, 0.7694, 0.9228, 0.8774, 0.5578, 0.8774, 0.8957,\n",
       "                       0.6153, 0.7944, 0.8715, 0.9181, 0.9004, 0.8548, 0.8441, 0.9067, 0.8532,\n",
       "                       0.9057, 0.8119, 0.9134, 0.8654, 0.8886, 0.8895, 0.8752, 0.8647, 0.8851,\n",
       "                       0.8093, 0.6652, 0.8012, 0.8903, 0.8439, 0.6194, 0.9196, 0.8554, 0.8706,\n",
       "                       0.8829, 1.0482, 0.8647, 0.8737, 0.8869, 0.8600, 0.8131, 0.9111, 0.8667,\n",
       "                       0.8533, 0.8963, 0.9300, 0.8746, 0.8436, 0.8545, 0.8032, 0.9044, 0.8210,\n",
       "                       0.9042, 0.8806, 0.7994, 0.8814, 0.8632, 0.8475, 0.7321, 0.8450, 0.9185,\n",
       "                       0.9054, 0.8836, 0.8315, 0.8862, 0.9423, 0.8833, 0.8877, 0.8513, 0.9076,\n",
       "                       0.8672, 0.9323, 0.8821, 0.8767, 0.4808, 0.8636, 0.8706, 0.9236, 0.8976,\n",
       "                       0.7502, 0.8522, 0.9110, 0.8613, 0.9046, 0.9115, 0.7995, 0.8145, 0.7161,\n",
       "                       0.9720, 0.9231, 0.8383, 0.7822, 0.8512, 0.8353, 0.3956, 0.8645, 0.8205,\n",
       "                       0.8251, 0.8618, 0.9395, 0.9507, 0.7096, 0.8335, 0.7772, 0.9071, 0.8314,\n",
       "                       0.8039, 0.7603, 0.8703, 0.8386, 0.6836, 0.8897, 0.8447, 0.8917, 0.8899,\n",
       "                       0.8640, 0.3818, 0.8511, 0.8681, 0.5184, 0.8481, 0.7816, 0.8378, 0.9102,\n",
       "                       0.9565, 0.8692, 0.9068, 0.8240], device='cuda:0')),\n",
       "              ('r5.bn2.bias',\n",
       "               tensor([-0.1087, -0.1718, -0.2932, -0.1413, -0.2535, -0.2102, -0.1198, -0.1956,\n",
       "                       -0.4890, -0.0848, -0.0675, -0.2085, -0.0960, -0.1072, -0.0736, -0.2135,\n",
       "                       -0.2569, -0.1928, -0.2053, -0.1006, -0.1070, -0.2343, -0.2144, -0.3207,\n",
       "                       -0.0905, -0.0625, -0.1237, -0.1084, -0.1052, -0.1092, -0.1584, -0.1918,\n",
       "                       -0.2146, -0.1917, -0.1653, -0.2397, -0.1740, -0.1986, -0.0881, -0.2011,\n",
       "                       -0.0446, -0.2323, -0.1391, -0.5653, -0.0407, -0.1763, -0.1727, -0.1409,\n",
       "                       -0.1038, -0.2277, -0.2713, -0.1921, -0.2500, -0.1238, -0.1294, -0.1485,\n",
       "                       -0.1415, -0.1411, -0.1445, -0.0944, -0.1127, -0.1302, -0.1268, -0.2856,\n",
       "                       -0.2298, -0.0804, -0.0354, -0.1054, -0.1870, -0.2311, -0.2007, -0.2156,\n",
       "                       -0.1464, -0.1991, -0.2072, -0.1389, -0.1380, -0.0311, -0.1093, -0.1625,\n",
       "                       -0.2441, -0.1282, -0.1900, -0.1680, -0.2649, -0.2573, -0.1799, -0.0687,\n",
       "                       -0.1391, -0.2888, -0.3281, -0.5316, -0.2790, -0.1374, -0.2225, -0.1847,\n",
       "                       -0.1828, -0.2284, -0.0903, -0.1585, -0.0545, -0.1673, -0.2578, -0.1467,\n",
       "                       -0.1467, -0.3644, -0.1498, -0.0464, -0.0847, -0.0990, -0.2186, -0.1075,\n",
       "                       -0.1633, -0.2097, -0.1604, -0.2005, -0.0869, -0.1280, -0.1890, -0.1166,\n",
       "                       -0.1719, -0.1561, -0.0843, -0.1381, -0.3339, -0.1397, -0.1848, -0.1418,\n",
       "                       -0.0659, -0.1808, -0.1158, -0.1797, -0.3033, -0.2289, -0.1214, -0.2795,\n",
       "                       -0.2168, -0.2035, -0.1277, -0.2137, -0.2025, -0.2404, -0.2245, -0.1992,\n",
       "                       -0.1877, -0.0502, -0.1770, -0.2275, -0.1269, -0.1429, -0.1636, -0.1719,\n",
       "                       -0.1340, -0.1470, -0.2929, -0.2598, -0.1209, -0.1049, -0.2662, -0.1818,\n",
       "                       -0.2650, -0.3930, -0.1180, -0.3600, -0.2210, -0.0655, -0.1813, -0.1517,\n",
       "                       -0.1922, -0.1929, -0.1643, -0.1134, -0.1666, -0.2279, -0.1181, -0.1349,\n",
       "                       -0.2275, -0.1870, -0.1445, -0.3099, -0.1388, -0.1766, -0.0408, -0.1012,\n",
       "                       -0.1098, -0.1703, -0.3064,  0.0161, -0.1487, -0.0691, -0.1532, -0.0524,\n",
       "                       -0.1971, -0.3736, -0.1217, -0.1337, -0.0703, -0.1023, -0.1129, -0.1810,\n",
       "                       -0.2262, -0.1201, -0.4205, -0.1641, -0.1243, -0.1564, -0.0902, -0.2507,\n",
       "                       -0.0863, -0.0841, -0.0922, -0.1211, -0.0868, -0.1101, -0.1075, -0.2026,\n",
       "                       -0.2580, -0.1915, -0.1933, -0.0924, -0.2182, -0.1096, -0.4430, -0.1337,\n",
       "                       -0.1532, -0.0885, -0.0976, -0.1788, -0.1860, -0.2742, -0.2349, -0.2161,\n",
       "                       -0.1951, -0.1063,  0.0487, -0.1058, -0.1673, -0.1978, -0.2296, -0.0958,\n",
       "                       -0.1161, -0.1722, -0.2438, -0.0160, -0.2115, -0.1193, -0.1941, -0.1719,\n",
       "                       -0.1694, -0.0982, -0.0223, -0.2431, -0.1338, -0.1755, -0.3262, -0.2498],\n",
       "                      device='cuda:0')),\n",
       "              ('r5.bn2.running_mean',\n",
       "               tensor([ -1.6410,  -5.7166,  -2.7925,  -4.6946,  -2.1758,  -3.1039,  -4.3322,\n",
       "                        -4.4089,   0.6661,  -6.7063,  -6.7021,  -2.7513,  -6.8583,  -4.2865,\n",
       "                        -6.5511,  -3.4179,  -2.8764,  -3.3586,  -4.5377,  -4.2777,  -6.3026,\n",
       "                        -2.5387,  -3.5470,  -2.8316,  -6.0174,  -4.7305,  -3.9742,  -3.2154,\n",
       "                        -4.0052,  -4.3123,  -4.7818,  -5.1039,  -4.4857,  -1.8403,  -3.4836,\n",
       "                         0.2073,  -4.9869,  -2.3312,  -7.1023,  -2.2251,  -1.4020,  -4.0414,\n",
       "                        -6.9805,   0.1996,  -6.3293,  -5.4241,  -4.9346,  -5.7441,  -5.7096,\n",
       "                        -3.6704,  -3.6544,  -2.7518,  -3.8981,  -6.2408,  -3.3739,  -4.9672,\n",
       "                        -5.7170,  -3.5717,  -3.7275,  -6.7858,  -3.4180,  -5.1413,  -2.6178,\n",
       "                        -3.0690,  -4.3052,  -5.4088,  -5.2397,  -5.8478,  -3.9961,  -3.1128,\n",
       "                        -2.6354,  -3.5560,  -6.4190,  -3.8299,  -4.0505,  -4.8578,  -2.7046,\n",
       "                        -5.9442,  -5.7725,  -4.7280,  -3.3688,  -6.4042,  -3.7942,  -5.7115,\n",
       "                        -4.7199,  -4.4340,  -3.6702,  -4.5446,  -4.4094,  -3.1208,  -1.1672,\n",
       "                         0.0499,  -2.3753,  -4.8590,  -2.9515,  -4.5630,  -2.9957,  -1.0014,\n",
       "                        -4.1244,  -4.5099,  -6.1860,  -4.3229,  -2.6736,  -4.4610,  -4.7007,\n",
       "                        -2.7322,  -4.7970,  -8.3271,  -7.7842,  -7.5806,  -3.6605,  -6.6415,\n",
       "                        -3.7264,  -3.8802,  -4.7977,  -3.5455,  -5.9653,  -4.5606,  -4.0311,\n",
       "                        -4.6121,  -5.7127,  -3.2948,  -4.8933,  -5.1384,  -7.2415,  -6.9716,\n",
       "                        -5.7297,  -3.6039,  -9.3292,  -4.1375,  -6.0180,  -4.6278,  -7.6504,\n",
       "                        -4.2250,  -8.0991,  -3.0289,  -4.0610,  -3.0637,  -3.1977,  -4.4607,\n",
       "                        -4.7931,  -3.0308,  -4.1632,  -4.3764,  -3.6310,  -8.4153,  -2.2514,\n",
       "                        -3.4011,  -5.8469,  -3.7491,  -4.7102,  -3.3045,  -4.0246,  -6.0031,\n",
       "                        -3.9228,  -2.3837,  -7.7821,  -6.3422,  -3.7318,  -4.0869,  -0.7502,\n",
       "                        -2.3224,  -3.7505,  -2.4929,  -3.4678,  -8.3122,  -4.5693,  -4.5460,\n",
       "                        -4.2828,  -3.0783,  -2.5182,  -6.2158,  -5.1365,  -2.8705,  -3.7193,\n",
       "                        -3.6395,  -2.9379,  -4.7944,  -5.5993,  -3.8387,  -4.3802,  -4.4983,\n",
       "                        -7.7024,  -7.0909,  -3.9074,  -3.8469,  -3.1007, -10.0695,  -4.0575,\n",
       "                        -5.5344,  -3.4667,  -7.3399,  -4.6744,  -3.3783,  -3.0100,  -3.5684,\n",
       "                        -7.8642,  -7.1897,  -4.5643,  -3.1354,  -4.9906,  -4.9248,  -6.7239,\n",
       "                        -4.0638,  -4.4427,  -5.1126,  -6.3432,  -3.3070,  -6.7186,  -5.8136,\n",
       "                        -7.5467,  -4.9393,  -3.9306,  -4.7773,  -5.5156,  -2.2270,  -2.7161,\n",
       "                        -3.4601,  -3.2133,  -7.3753,  -4.8500,  -7.2938,   0.1917,  -5.2034,\n",
       "                        -5.5720,  -2.6855,  -5.6789,  -5.2565,  -3.3401,  -1.9139,  -4.6679,\n",
       "                        -3.7744,  -3.5612,  -5.2710,  -2.5583,  -5.7785,  -3.4987,  -3.8204,\n",
       "                        -2.8600,  -3.7540,  -3.4494,  -4.7599,  -3.0831,  -9.2005,   3.3009,\n",
       "                        -2.8154,  -4.8879,  -1.9607,  -5.1966,  -5.8837,  -8.2258,  -0.4450,\n",
       "                        -5.5958,  -2.8984,  -3.3131,  -3.3853], device='cuda:0')),\n",
       "              ('r5.bn2.running_var',\n",
       "               tensor([139.1606, 235.0838,  94.5292, 223.9192,  82.0526, 122.2438, 185.5779,\n",
       "                        96.5032, 106.5975, 411.8985, 344.4930, 103.1693, 377.9881, 215.9586,\n",
       "                       235.0508, 287.0559,  78.1926, 122.8979, 165.5036, 166.7719, 266.4005,\n",
       "                        35.1851, 168.1385, 149.6965, 339.9692, 264.5004, 182.9233, 200.2643,\n",
       "                       240.3201, 262.4221, 153.2070, 163.2708, 199.3616, 120.0319, 192.3974,\n",
       "                        74.2914, 263.5033, 117.1364, 367.9451, 108.3399, 175.0126,  57.5794,\n",
       "                       259.4260, 120.1805, 398.7644, 117.8187, 184.2178, 238.2347, 297.0647,\n",
       "                       154.2704, 103.8168, 185.8207, 116.9485, 265.2585, 256.8615, 152.4089,\n",
       "                       250.2626, 310.2973, 237.3101, 369.8929, 233.4547, 198.7551, 157.1932,\n",
       "                       142.1030, 146.9476, 221.9128, 336.5078, 278.1583, 122.0806, 142.2253,\n",
       "                        96.1750, 118.6303, 218.7113, 133.7775, 238.2658, 267.2337, 190.6922,\n",
       "                       355.7857, 294.3595, 179.7773,  98.2347, 262.7547, 120.7041, 168.9100,\n",
       "                        85.8422, 100.7097, 159.5218, 283.0657, 269.1427, 113.9842,  91.2921,\n",
       "                        91.1949, 137.5588, 269.3021, 208.0438, 211.8407, 113.3870, 186.9201,\n",
       "                       177.6295, 217.2156, 358.7813, 196.8474, 104.3372, 200.5457, 225.3311,\n",
       "                        71.2346, 207.9304, 475.9666, 377.7920, 297.2275, 114.8318, 276.4864,\n",
       "                       201.2160, 102.3360, 230.3304, 173.5097, 195.6345, 230.5778,  96.4220,\n",
       "                       265.2159, 200.2873, 175.7034, 201.8877, 241.7458,  72.2083, 276.6307,\n",
       "                       222.9388, 220.4594, 569.8853, 122.0493, 288.8819, 186.4050, 123.4312,\n",
       "                       200.1790, 360.5443,  70.4820, 120.1431, 148.1976, 183.7572, 145.3664,\n",
       "                       203.6977, 104.7282, 207.0846, 163.4845, 248.6130, 535.4202, 172.9923,\n",
       "                        95.8226, 261.8928, 165.5207, 250.9501, 194.6848, 237.7530, 225.1488,\n",
       "                        92.6904,  64.6479, 319.4946, 352.5181,  45.4882, 161.4795, 152.6521,\n",
       "                        46.0149, 225.9846,  68.5977, 138.7223, 383.9947, 165.9110, 203.9048,\n",
       "                       252.5952, 120.7472, 178.3377, 293.3408, 208.0274,  85.3330, 112.1503,\n",
       "                       220.5272, 152.9024, 140.7241, 187.3298,  61.7124, 204.6738, 202.0353,\n",
       "                       505.2320, 289.4446, 257.8363, 162.8435,  61.7139, 549.7590, 225.1181,\n",
       "                       320.9179, 183.5623, 396.2048, 140.5248,  42.6199, 125.5713, 194.9008,\n",
       "                       481.7126, 356.1484, 267.5809, 185.7073, 241.2847, 255.1598,  62.5762,\n",
       "                       225.4106, 196.5826, 148.9570, 305.1255,  79.3602, 408.3774, 384.2642,\n",
       "                       273.4417, 272.0061, 290.7096, 271.2717, 312.3063,  96.8279, 139.3713,\n",
       "                       117.8425, 116.6714, 337.4749, 236.9303, 249.8207,  80.3326, 305.2073,\n",
       "                       246.1678, 141.0779, 246.4717, 206.6938, 121.7852,  66.1235, 110.1062,\n",
       "                       136.3030, 161.1149, 262.4165, 136.9484, 280.5846, 188.5862, 158.6324,\n",
       "                        99.7348, 213.4742, 275.7535, 163.1822, 215.6294, 539.4337,  72.5479,\n",
       "                       178.3271, 208.5315,  55.4805, 229.1241, 275.7431, 498.6949, 179.6231,\n",
       "                       270.4618, 223.3792, 122.3681, 119.1717], device='cuda:0')),\n",
       "              ('r5.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r5.c2.weights',\n",
       "               tensor([[[[ 0.0055, -0.0229,  0.0092],\n",
       "                         [ 0.0542,  0.0185,  0.0350],\n",
       "                         [-0.0228,  0.0227, -0.0176]],\n",
       "               \n",
       "                        [[ 0.0419,  0.0383,  0.0601],\n",
       "                         [ 0.0419,  0.0302,  0.0414],\n",
       "                         [-0.0681, -0.0371, -0.0642]],\n",
       "               \n",
       "                        [[ 0.0635,  0.0247, -0.0445],\n",
       "                         [ 0.0033,  0.0010, -0.0282],\n",
       "                         [-0.0065, -0.0165, -0.0304]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0322,  0.0037, -0.0016],\n",
       "                         [ 0.0382,  0.0743,  0.0157],\n",
       "                         [ 0.0420, -0.0050, -0.0286]],\n",
       "               \n",
       "                        [[-0.0051, -0.0615, -0.0131],\n",
       "                         [ 0.0066, -0.0112,  0.0661],\n",
       "                         [-0.0156, -0.0620, -0.0008]],\n",
       "               \n",
       "                        [[-0.0644, -0.0330, -0.0559],\n",
       "                         [-0.0501, -0.0116, -0.0300],\n",
       "                         [-0.0258, -0.0742, -0.0021]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0795, -0.0722, -0.0923],\n",
       "                         [-0.0729, -0.0679, -0.1165],\n",
       "                         [-0.1206, -0.0916, -0.0292]],\n",
       "               \n",
       "                        [[ 0.0335,  0.0185, -0.0285],\n",
       "                         [-0.0211, -0.0486, -0.0861],\n",
       "                         [ 0.0608,  0.0198,  0.0095]],\n",
       "               \n",
       "                        [[-0.0558, -0.0620, -0.0283],\n",
       "                         [ 0.0086, -0.0307, -0.0575],\n",
       "                         [-0.0231,  0.0144,  0.0042]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0164, -0.0377,  0.0200],\n",
       "                         [-0.0534, -0.0631, -0.0524],\n",
       "                         [-0.0219, -0.0418, -0.0661]],\n",
       "               \n",
       "                        [[ 0.0575,  0.1362,  0.1012],\n",
       "                         [ 0.0152,  0.0036, -0.0203],\n",
       "                         [-0.0179, -0.0182, -0.0574]],\n",
       "               \n",
       "                        [[ 0.0596, -0.0219, -0.0092],\n",
       "                         [-0.0624, -0.0580, -0.0066],\n",
       "                         [-0.0407,  0.0239,  0.0736]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0328,  0.0108,  0.0109],\n",
       "                         [ 0.0316, -0.0156, -0.0562],\n",
       "                         [-0.0058, -0.0495, -0.1049]],\n",
       "               \n",
       "                        [[-0.0088,  0.0365,  0.0380],\n",
       "                         [-0.0553, -0.0281, -0.0134],\n",
       "                         [-0.0259, -0.0083,  0.0036]],\n",
       "               \n",
       "                        [[ 0.0298,  0.0105, -0.0078],\n",
       "                         [-0.0099,  0.0490, -0.0015],\n",
       "                         [-0.0197,  0.0045, -0.0274]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0262,  0.0378,  0.0207],\n",
       "                         [ 0.0111, -0.0054,  0.0508],\n",
       "                         [-0.0711, -0.0622, -0.0408]],\n",
       "               \n",
       "                        [[ 0.0364, -0.0118,  0.0066],\n",
       "                         [ 0.0852,  0.0726,  0.0682],\n",
       "                         [-0.0009,  0.0393,  0.0444]],\n",
       "               \n",
       "                        [[ 0.0176,  0.0049,  0.0225],\n",
       "                         [-0.0069,  0.0014, -0.0278],\n",
       "                         [ 0.0160, -0.0113,  0.0458]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0316, -0.0462,  0.0095],\n",
       "                         [-0.0484, -0.0516, -0.0116],\n",
       "                         [-0.1063, -0.1127, -0.0466]],\n",
       "               \n",
       "                        [[-0.0546,  0.0011,  0.0306],\n",
       "                         [ 0.0437,  0.0433,  0.0281],\n",
       "                         [ 0.0440,  0.0207,  0.0445]],\n",
       "               \n",
       "                        [[-0.0220, -0.0264, -0.0549],\n",
       "                         [ 0.0281,  0.0126,  0.0288],\n",
       "                         [ 0.0024,  0.0151, -0.0526]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0206, -0.0463, -0.0113],\n",
       "                         [-0.0589, -0.0360, -0.0592],\n",
       "                         [-0.0450, -0.0093, -0.0243]],\n",
       "               \n",
       "                        [[ 0.0312,  0.0422, -0.0247],\n",
       "                         [-0.0541,  0.0169,  0.0788],\n",
       "                         [-0.0882,  0.0093,  0.0595]],\n",
       "               \n",
       "                        [[-0.0119,  0.0295,  0.0158],\n",
       "                         [-0.0251, -0.0179, -0.0344],\n",
       "                         [ 0.0049, -0.0164, -0.0243]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1407,  0.1359,  0.1435],\n",
       "                         [ 0.0869,  0.0351,  0.0858],\n",
       "                         [ 0.0170,  0.0055,  0.0490]],\n",
       "               \n",
       "                        [[-0.0223, -0.0442, -0.1136],\n",
       "                         [-0.0286, -0.0187, -0.0426],\n",
       "                         [ 0.0184,  0.0032, -0.0734]],\n",
       "               \n",
       "                        [[ 0.0879,  0.0018, -0.0789],\n",
       "                         [ 0.0393, -0.0240, -0.0566],\n",
       "                         [ 0.0394, -0.0313, -0.0421]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0899,  0.0818, -0.0144],\n",
       "                         [ 0.1076, -0.0101, -0.0615],\n",
       "                         [ 0.0453, -0.0174, -0.0139]],\n",
       "               \n",
       "                        [[ 0.0219, -0.0283, -0.0534],\n",
       "                         [ 0.0344,  0.0201,  0.0513],\n",
       "                         [-0.0428, -0.0765,  0.0948]],\n",
       "               \n",
       "                        [[-0.0193, -0.0049,  0.0085],\n",
       "                         [-0.0452, -0.0471, -0.0920],\n",
       "                         [-0.0367, -0.0129, -0.0501]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0133, -0.0042, -0.0547],\n",
       "                         [-0.0576, -0.0071, -0.0277],\n",
       "                         [-0.0417, -0.0633, -0.0127]],\n",
       "               \n",
       "                        [[-0.0304,  0.0376,  0.0284],\n",
       "                         [ 0.0507,  0.0232,  0.0125],\n",
       "                         [-0.0139, -0.0241, -0.0339]],\n",
       "               \n",
       "                        [[ 0.0072, -0.0052, -0.0402],\n",
       "                         [ 0.0080, -0.0493, -0.0206],\n",
       "                         [-0.0279, -0.0268, -0.0632]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0620,  0.0193,  0.0053],\n",
       "                         [ 0.0231, -0.0361,  0.0385],\n",
       "                         [-0.0162,  0.0149,  0.0459]],\n",
       "               \n",
       "                        [[ 0.0087, -0.0800,  0.0084],\n",
       "                         [ 0.0192, -0.0418, -0.0409],\n",
       "                         [-0.0142,  0.0096, -0.0240]],\n",
       "               \n",
       "                        [[ 0.0497,  0.0278,  0.0525],\n",
       "                         [-0.0026,  0.0171,  0.0307],\n",
       "                         [ 0.0015, -0.0067, -0.0877]]]], device='cuda:0')),\n",
       "              ('r5.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r5.c3.weights',\n",
       "               tensor([[[[ 0.0302]],\n",
       "               \n",
       "                        [[ 0.0089]],\n",
       "               \n",
       "                        [[ 0.0198]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0146]],\n",
       "               \n",
       "                        [[ 0.0100]],\n",
       "               \n",
       "                        [[-0.0319]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0271]],\n",
       "               \n",
       "                        [[-0.0365]],\n",
       "               \n",
       "                        [[ 0.0232]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0368]],\n",
       "               \n",
       "                        [[-0.0069]],\n",
       "               \n",
       "                        [[-0.0097]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0162]],\n",
       "               \n",
       "                        [[ 0.0435]],\n",
       "               \n",
       "                        [[ 0.0370]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0081]],\n",
       "               \n",
       "                        [[ 0.0297]],\n",
       "               \n",
       "                        [[ 0.0021]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0597]],\n",
       "               \n",
       "                        [[-0.0573]],\n",
       "               \n",
       "                        [[-0.0456]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0185]],\n",
       "               \n",
       "                        [[-0.0033]],\n",
       "               \n",
       "                        [[ 0.0136]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0083]],\n",
       "               \n",
       "                        [[ 0.0350]],\n",
       "               \n",
       "                        [[-0.0079]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0544]],\n",
       "               \n",
       "                        [[-0.0600]],\n",
       "               \n",
       "                        [[ 0.0255]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0106]],\n",
       "               \n",
       "                        [[ 0.0162]],\n",
       "               \n",
       "                        [[-0.0025]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0305]],\n",
       "               \n",
       "                        [[-0.0605]],\n",
       "               \n",
       "                        [[-0.0431]]]], device='cuda:0')),\n",
       "              ('r5.c3.mask',\n",
       "               tensor([[[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]]], device='cuda:0')),\n",
       "              ('r6.bn1.weight',\n",
       "               tensor([0.9721, 0.8862, 0.9226, 1.0288, 0.9075, 0.8888, 0.9551, 1.1127, 0.8849,\n",
       "                       1.0272, 0.8743, 1.2074, 0.8594, 0.8555, 0.9391, 0.9497, 1.0553, 0.8797,\n",
       "                       0.9234, 0.9783, 0.9214, 0.9686, 0.9439, 0.8866, 1.0465, 0.8812, 1.2356,\n",
       "                       0.8807, 1.2013, 0.9002, 1.0727, 0.9294, 0.8079, 0.9011, 0.9045, 0.9028,\n",
       "                       1.2134, 1.0457, 1.2291, 1.1318, 0.9926, 1.2870, 0.9142, 0.9508, 1.0768,\n",
       "                       0.9893, 0.8398, 0.9043, 1.0143, 1.2695, 1.0492, 0.9783, 0.9470, 1.0369,\n",
       "                       1.0600, 0.7715, 0.9581, 1.0148, 0.9337, 0.8897, 0.9809, 0.8920, 1.0752,\n",
       "                       0.8276, 0.9271, 1.0923, 1.0592, 1.0681, 0.8489, 1.0537, 0.9471, 0.8319,\n",
       "                       0.7711, 1.0690, 0.9096, 0.8721, 0.9008, 0.8357, 0.9394, 1.0269, 0.9890,\n",
       "                       0.9368, 0.8305, 0.9928, 0.8524, 0.9198, 0.8406, 0.9386, 0.9465, 0.8932,\n",
       "                       0.8450, 1.0237, 0.8978, 1.1767, 0.9851, 1.0001, 1.0203, 1.2071, 0.9423,\n",
       "                       0.8643, 1.1125, 0.9820, 0.9643, 0.9667, 0.9064, 0.8932, 0.9726, 0.9329,\n",
       "                       0.7726, 0.8383, 0.8483, 0.9366, 0.9673, 0.8897, 0.9766, 1.0146, 0.9961,\n",
       "                       0.8978, 1.0206, 1.1349, 1.0105, 0.9017, 0.7673, 0.9630, 0.9619, 0.8993,\n",
       "                       0.9532, 1.1196, 0.8425, 0.8507, 0.8537, 0.9150, 0.8641, 0.9116, 0.8975,\n",
       "                       1.0099, 0.9215, 1.0082, 1.0355, 0.9193, 0.8745, 0.9604, 0.9353, 0.9287,\n",
       "                       1.0688, 0.9193, 0.8967, 1.0276, 0.8833, 0.9930, 0.9701, 0.8910, 0.9457,\n",
       "                       1.0072, 0.9019, 0.9479, 0.9946, 0.9532, 1.0741, 0.9283, 1.0658, 1.4718,\n",
       "                       1.1969, 1.0372, 0.9284, 0.8060, 0.8393, 1.2287, 0.8629, 1.0066, 1.1509,\n",
       "                       1.0669, 0.9381, 0.8742, 0.8595, 0.9653, 0.9735, 0.8739, 1.0902, 0.9136,\n",
       "                       0.9133, 1.0174, 0.9925, 1.1760, 1.2338, 1.0014, 1.2209, 1.3118, 0.9498,\n",
       "                       0.9673, 1.0796, 1.0457, 1.0825, 0.7729, 0.8514, 1.0665, 0.8642, 1.1000,\n",
       "                       0.9411, 1.1034, 0.8514, 0.8477, 0.8743, 1.0963, 0.9838, 0.9306, 1.0665,\n",
       "                       0.8699, 0.9256, 1.0091, 0.8928, 0.9231, 0.8285, 0.8632, 0.8744, 0.8779,\n",
       "                       1.0407, 1.0315, 0.9427, 0.8556, 0.9362, 1.0154, 0.8878, 0.8749, 1.0922,\n",
       "                       1.0458, 0.7838, 1.0870, 1.0072, 0.5955, 0.9299, 0.8623, 0.9750, 0.9126,\n",
       "                       0.9169, 1.0856, 0.9325, 0.9145, 0.9284, 1.0078, 1.1055, 0.8839, 0.9339,\n",
       "                       0.9220, 0.9973, 0.9064, 0.9710, 0.9645, 0.8850, 0.9390, 1.1100, 0.9218,\n",
       "                       0.9155, 0.8562, 0.8934, 0.8440], device='cuda:0')),\n",
       "              ('r6.bn1.bias',\n",
       "               tensor([-0.1868, -0.1514, -0.1444,  0.2459,  0.0154,  0.1132, -0.1887, -0.3310,\n",
       "                       -0.1251,  0.1364, -0.0705, -0.1353, -0.2225, -0.2341, -0.1864, -0.1826,\n",
       "                       -0.2319, -0.0619, -0.2290, -0.0254, -0.1680, -0.2322, -0.2081, -0.0698,\n",
       "                       -0.4254, -0.0923, -0.2204, -0.1498, -0.3931, -0.0757, -0.1299, -0.0989,\n",
       "                       -0.0816, -0.2188, -0.1429, -0.1638,  0.0475, -0.2516, -0.0128, -0.2176,\n",
       "                       -0.1040,  0.0199, -0.2601, -0.0587, -0.1668, -0.1710, -0.2111, -0.0900,\n",
       "                       -0.2834, -0.3871, -0.0674, -0.1291, -0.0368, -0.0806,  0.1655, -0.1762,\n",
       "                       -0.2652, -0.1580, -0.1789, -0.1289,  0.1282, -0.1139, -0.1011, -0.1819,\n",
       "                       -0.1224, -0.2373, -0.1963,  0.1496, -0.1829, -0.1331, -0.1261, -0.2402,\n",
       "                        0.0267, -0.0242, -0.1262, -0.1959, -0.1044, -0.1030, -0.3090, -0.1076,\n",
       "                       -0.2193, -0.2273, -0.1275, -0.0448, -0.1428, -0.1586, -0.1609, -0.2616,\n",
       "                       -0.0006, -0.2562, -0.0276, -0.0958, -0.2199, -0.2047, -0.2434,  0.0614,\n",
       "                        0.0637, -0.2006, -0.1586, -0.2297, -0.0987, -0.1140, -0.3458, -0.1121,\n",
       "                       -0.2102, -0.3021, -0.2342, -0.0968, -0.1717, -0.2795, -0.2060, -0.1728,\n",
       "                       -0.0693, -0.2015, -0.0874, -0.1393,  0.1621, -0.2155,  0.0918, -0.0906,\n",
       "                       -0.3833, -0.1707, -0.0787,  0.0661, -0.4203, -0.2564, -0.0976, -0.0101,\n",
       "                       -0.2046, -0.2805, -0.1348, -0.2253, -0.1684, -0.0891, -0.0242, -0.4422,\n",
       "                       -0.1313, -0.2384, -0.3198, -0.1361, -0.2587, -0.1015, -0.2460, -0.2012,\n",
       "                        0.0488, -0.2243, -0.1765, -0.2180, -0.1713, -0.1662, -0.1364, -0.2524,\n",
       "                       -0.2083, -0.1025, -0.1199, -0.0521, -0.2973, -0.0287,  0.1428, -0.0165,\n",
       "                       -0.0458,  0.1491, -0.0592, -0.1320, -0.1108, -0.2138, -0.1314,  0.0408,\n",
       "                       -0.1401, -0.2277, -0.2460, -0.1771, -0.0167, -0.2961, -0.2574, -0.1630,\n",
       "                       -0.1107,  0.0118, -0.2056, -0.0845, -0.1966, -0.1519, -0.2041,  0.0192,\n",
       "                       -0.1015, -0.2170, -0.0706,  0.2203, -0.1067, -0.0883,  0.1700, -0.0469,\n",
       "                       -0.0211, -0.3511, -0.2378,  0.0034, -0.1355,  0.1845,  0.0249, -0.2205,\n",
       "                       -0.2467,  0.0433, -0.0968, -0.2208, -0.3187, -0.1550, -0.3058, -0.1682,\n",
       "                       -0.2706, -0.3333, -0.1790, -0.2297,  0.0513, -0.2406, -0.2619, -0.3059,\n",
       "                       -0.0355, -0.2188, -0.1565,  0.0383, -0.2879, -0.3706, -0.2451, -0.1735,\n",
       "                        0.1109, -0.0410, -0.4120, -0.1477,  0.1344, -0.3105, -0.2481, -0.1167,\n",
       "                       -0.1562,  0.0064, -0.1384, -0.1411, -0.2098, -0.1114, -0.2950, -0.0980,\n",
       "                       -0.1525, -0.2992, -0.1054, -0.3367, -0.1046, -0.1039, -0.0231, -0.0366,\n",
       "                       -0.2162, -0.0916,  0.1713,  0.0821, -0.1147, -0.1051, -0.2332, -0.2693],\n",
       "                      device='cuda:0')),\n",
       "              ('r6.bn1.running_mean',\n",
       "               tensor([-7.4765e-02, -1.1293e-01, -3.8165e-02, -3.4779e-01,  3.2624e-02,\n",
       "                       -1.8097e-02, -4.4325e-02, -1.1157e-01, -1.0955e-01, -2.9937e-01,\n",
       "                       -1.3971e-01, -7.1871e-02, -1.2657e-01,  2.7139e-01, -1.3194e-01,\n",
       "                       -1.0501e-01,  4.7723e-02,  6.5898e-02, -1.2348e-02,  1.0176e-01,\n",
       "                        1.9443e-02, -1.1430e-01, -9.6292e-02, -1.7499e-01,  1.6965e-02,\n",
       "                       -1.4311e-01, -2.5881e-01, -9.0025e-02, -2.9690e-02,  1.6661e-02,\n",
       "                       -4.5053e-02, -4.5856e-02, -2.5631e-01, -6.0630e-02, -1.3427e-02,\n",
       "                       -1.4680e-01, -6.7513e-02, -1.1959e-01, -1.9404e-01, -1.9866e-01,\n",
       "                       -2.5346e-01,  1.3900e-01, -1.8874e-01, -1.5119e-02, -3.0365e-01,\n",
       "                       -1.4183e-01, -9.7598e-02,  1.5199e-02,  1.5911e-02, -1.8481e-01,\n",
       "                       -1.3223e-02, -2.1253e-01,  1.9828e-01, -2.2798e-01, -4.4411e-02,\n",
       "                       -1.5350e-01,  2.4000e-03, -7.2790e-02, -1.3697e-02, -2.1359e-01,\n",
       "                       -1.8497e-01, -7.3975e-02, -2.0928e-01,  2.3603e-02, -1.5060e-01,\n",
       "                       -1.2349e-01, -3.3315e-01, -1.7346e-01,  7.4802e-02,  1.9553e-02,\n",
       "                        1.9238e-01, -1.2136e-01,  2.6729e-01, -3.7240e-01, -1.0356e-01,\n",
       "                        8.3896e-02, -1.6876e-02, -1.8959e-01,  8.8328e-02, -4.9637e-02,\n",
       "                       -2.9571e-02,  3.2830e-02, -2.1286e-02, -3.2777e-01,  3.6251e-02,\n",
       "                        1.7400e-02,  1.8648e-01, -1.4137e-01, -1.2091e-01, -2.4001e-02,\n",
       "                       -6.8620e-02, -1.6023e-01, -5.3485e-02,  1.5195e-02,  1.0383e-02,\n",
       "                       -1.1946e-01, -1.6940e-02, -2.3279e-01, -1.6176e-01, -5.1066e-02,\n",
       "                       -7.9025e-02,  4.1312e-02,  9.7985e-02, -6.8782e-02, -2.3937e-01,\n",
       "                        6.0804e-02, -9.7284e-03,  4.9523e-02, -1.8360e-02, -1.6427e-02,\n",
       "                       -9.1342e-02, -7.2503e-04,  8.5301e-02, -1.5502e-01, -7.5552e-02,\n",
       "                       -1.3381e-01, -1.1465e-01,  1.6807e-01, -5.8505e-02, -3.0746e-01,\n",
       "                       -5.7443e-02, -1.3420e-01, -4.6863e-02, -1.3080e-01, -1.9191e-01,\n",
       "                       -2.3519e-02, -3.9705e-04, -1.4315e-01,  1.2238e-02,  1.4532e-01,\n",
       "                       -8.5322e-02, -1.2317e-01, -1.6890e-01, -9.3524e-02, -9.4775e-02,\n",
       "                        1.2093e-01, -1.2710e-03,  1.8009e-01,  1.1318e-01,  1.9031e-01,\n",
       "                        1.6969e-01, -1.6163e-01, -1.6715e-01,  2.7066e-03, -3.3100e-01,\n",
       "                        1.0944e-02, -1.1406e-01, -1.9798e-01, -8.3170e-02, -2.0596e-02,\n",
       "                       -3.4529e-03, -3.9816e-03, -2.1146e-02, -1.0495e-01, -1.1788e-01,\n",
       "                        2.2906e-02, -2.8085e-02,  3.7487e-02, -1.5929e-01, -3.2256e-01,\n",
       "                        9.7127e-03, -1.1973e-01, -4.1373e-03,  1.3461e-01, -1.9102e-01,\n",
       "                       -4.6246e-02, -6.6111e-02, -2.1049e-01, -9.1914e-02, -2.3476e-01,\n",
       "                       -9.9510e-02, -9.9801e-02,  2.7411e-02, -3.2167e-02, -1.5358e-01,\n",
       "                       -7.7926e-02, -1.3063e-02, -7.5206e-02, -2.7389e-02, -6.5421e-02,\n",
       "                        6.3075e-03,  2.5146e-03, -2.0453e-01, -1.0100e-01, -2.4951e-01,\n",
       "                        1.3269e-01,  5.2798e-02, -1.0554e-01, -8.2822e-03, -1.3354e-01,\n",
       "                       -1.3089e-01, -1.9977e-01, -4.5556e-02,  1.5111e-01, -1.5673e-01,\n",
       "                       -7.3420e-02,  1.6492e-01, -4.0334e-01, -2.2666e-01,  2.6396e-02,\n",
       "                       -1.5551e-01,  1.8651e-01, -1.3069e-01, -1.0162e-01,  7.5423e-02,\n",
       "                       -8.9056e-02, -5.8067e-02, -2.3751e-02, -4.1120e-02,  8.1674e-02,\n",
       "                       -1.5741e-01, -4.5179e-02, -1.6563e-01,  2.4669e-02, -1.2564e-01,\n",
       "                        2.2422e-02, -3.7524e-01, -1.0869e-01,  4.9972e-02, -1.9303e-01,\n",
       "                        1.0131e-01,  5.9674e-02,  1.9630e-02, -3.4015e-02, -8.0166e-02,\n",
       "                       -2.9393e-01,  4.7596e-02, -1.6031e-01,  1.4383e-01,  1.7013e-02,\n",
       "                        1.3031e-01,  1.5280e-01,  3.3890e-02, -5.8254e-02,  6.9411e-02,\n",
       "                       -9.4832e-02, -1.3414e-01,  5.7576e-02, -2.8870e-01, -5.4388e-02,\n",
       "                        3.6327e-02,  1.4250e-02, -2.3070e-02, -1.4268e-01, -1.3035e-01,\n",
       "                       -6.5518e-02, -2.2509e-01, -8.7117e-03, -9.9546e-02, -1.0586e-01,\n",
       "                       -5.4774e-02,  1.8182e-01, -1.2075e-02, -3.9844e-02, -2.2783e-03,\n",
       "                        7.0738e-03], device='cuda:0')),\n",
       "              ('r6.bn1.running_var',\n",
       "               tensor([0.9665, 0.3015, 0.5076, 0.6050, 0.5323, 0.3166, 0.4330, 0.2686, 0.4673,\n",
       "                       0.4818, 0.3851, 0.6433, 0.3565, 0.3012, 0.3239, 0.5665, 0.3145, 0.3723,\n",
       "                       0.6097, 0.5034, 0.5097, 0.3403, 0.3202, 0.4062, 0.7826, 0.3261, 0.7389,\n",
       "                       0.5774, 0.2102, 0.4045, 0.4430, 0.4391, 0.4393, 0.3297, 0.2818, 0.3927,\n",
       "                       0.4680, 0.5341, 0.5105, 0.3774, 0.3223, 0.7528, 0.4960, 0.4758, 0.3386,\n",
       "                       0.3847, 0.3750, 0.5831, 0.4009, 0.5206, 0.3760, 0.4586, 0.5430, 0.7279,\n",
       "                       0.5088, 0.4967, 0.4322, 0.3035, 0.4812, 0.2909, 0.6961, 0.5420, 0.3155,\n",
       "                       0.4087, 0.3507, 0.9616, 0.3139, 0.4052, 0.5584, 0.3507, 0.3979, 0.3595,\n",
       "                       0.4588, 0.5770, 0.5384, 0.6329, 0.5492, 0.6674, 0.3937, 0.2704, 0.7672,\n",
       "                       0.2869, 0.2458, 0.7505, 0.3117, 0.3691, 0.3590, 0.4097, 1.1162, 0.3379,\n",
       "                       0.4488, 0.3807, 0.3085, 0.3844, 0.3987, 0.7289, 0.3847, 0.4723, 0.2885,\n",
       "                       0.3522, 0.5368, 0.4347, 0.5123, 0.5096, 0.4101, 0.4639, 0.6328, 0.3026,\n",
       "                       0.5479, 0.2575, 0.3275, 0.2338, 0.3318, 0.3448, 0.5722, 0.5141, 0.6964,\n",
       "                       0.5190, 0.4000, 0.7330, 0.3729, 0.3441, 0.5355, 0.4762, 0.4752, 0.4065,\n",
       "                       0.4262, 0.4063, 0.3325, 0.3635, 0.3961, 0.3827, 0.3708, 0.3973, 0.3564,\n",
       "                       0.2541, 0.8306, 0.7025, 0.3210, 0.3551, 0.3322, 0.3174, 0.3177, 0.7575,\n",
       "                       0.6227, 0.4446, 0.7854, 0.3075, 0.4655, 0.3149, 0.4524, 0.3861, 0.4226,\n",
       "                       0.3802, 0.3634, 0.6307, 0.2543, 0.3909, 0.8277, 0.3434, 0.3269, 0.6689,\n",
       "                       0.5352, 0.8673, 0.3440, 0.4486, 0.3427, 0.7172, 0.4069, 0.4946, 0.3472,\n",
       "                       0.2830, 0.3882, 0.4315, 0.2757, 0.6658, 0.2431, 0.2959, 0.6224, 0.3682,\n",
       "                       0.3153, 1.0349, 0.6066, 0.4411, 0.4833, 0.8001, 0.6041, 0.7006, 0.5957,\n",
       "                       0.6981, 0.6086, 0.4077, 0.6461, 0.4017, 0.5285, 0.5282, 0.7098, 0.6560,\n",
       "                       0.5108, 1.1172, 0.2719, 0.3087, 0.3080, 0.4778, 0.8820, 0.5168, 0.5304,\n",
       "                       0.4098, 0.2350, 1.0430, 0.3841, 0.5113, 0.4606, 0.6228, 0.2698, 0.2925,\n",
       "                       0.4171, 0.3139, 0.3244, 0.2826, 0.4580, 0.9067, 0.3107, 0.2490, 0.5760,\n",
       "                       0.3829, 0.3257, 0.3650, 0.5247, 0.1748, 0.3300, 0.6610, 0.5662, 0.6066,\n",
       "                       0.3787, 0.6946, 0.3400, 0.2789, 0.3504, 0.2915, 0.3257, 0.5031, 0.4388,\n",
       "                       0.2208, 0.4541, 0.5049, 0.3729, 0.5817, 0.7543, 0.7120, 0.5351, 0.5671,\n",
       "                       0.6000, 0.5058, 0.4509, 0.5071], device='cuda:0')),\n",
       "              ('r6.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r6.c1.weights',\n",
       "               tensor([[[[-0.0489, -0.0068, -0.0213],\n",
       "                         [-0.1280, -0.1133, -0.0780],\n",
       "                         [-0.1166, -0.1206, -0.0231]],\n",
       "               \n",
       "                        [[ 0.0061, -0.0549,  0.0209],\n",
       "                         [-0.0717, -0.0632, -0.0813],\n",
       "                         [-0.0150, -0.0190, -0.0708]],\n",
       "               \n",
       "                        [[ 0.0389,  0.0115, -0.0457],\n",
       "                         [-0.0929, -0.0601, -0.1570],\n",
       "                         [-0.0569, -0.0243, -0.0214]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0736,  0.0959,  0.0518],\n",
       "                         [ 0.0640,  0.0032,  0.0030],\n",
       "                         [ 0.0179, -0.0223, -0.0282]],\n",
       "               \n",
       "                        [[-0.0922, -0.1305, -0.0821],\n",
       "                         [-0.0883, -0.0778, -0.0599],\n",
       "                         [ 0.0009,  0.0050,  0.1031]],\n",
       "               \n",
       "                        [[-0.0781, -0.0921, -0.0970],\n",
       "                         [-0.0285, -0.0665, -0.1115],\n",
       "                         [-0.0594, -0.0357, -0.0276]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0005,  0.0722,  0.0145],\n",
       "                         [ 0.0253,  0.0551,  0.0362],\n",
       "                         [-0.0194, -0.0258, -0.0588]],\n",
       "               \n",
       "                        [[-0.0041, -0.0155, -0.0299],\n",
       "                         [-0.0750, -0.0301, -0.0689],\n",
       "                         [ 0.0045, -0.0508, -0.0678]],\n",
       "               \n",
       "                        [[-0.0983, -0.0480, -0.0370],\n",
       "                         [-0.0221,  0.0216, -0.0787],\n",
       "                         [-0.0289, -0.0653, -0.0724]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0381,  0.0678, -0.0301],\n",
       "                         [ 0.0291,  0.0151, -0.0407],\n",
       "                         [-0.0038, -0.0484, -0.0407]],\n",
       "               \n",
       "                        [[-0.0664,  0.0123,  0.0040],\n",
       "                         [-0.0797, -0.0170, -0.0229],\n",
       "                         [-0.1087, -0.0560, -0.1065]],\n",
       "               \n",
       "                        [[-0.0698,  0.0050, -0.0642],\n",
       "                         [-0.0573, -0.0196, -0.0412],\n",
       "                         [-0.0286, -0.0078, -0.0328]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0785, -0.0705, -0.0556],\n",
       "                         [-0.0421, -0.0485, -0.0915],\n",
       "                         [-0.0291, -0.0501, -0.0982]],\n",
       "               \n",
       "                        [[-0.2828, -0.1429, -0.1373],\n",
       "                         [-0.0620, -0.0405, -0.0930],\n",
       "                         [-0.0474, -0.1586, -0.2316]],\n",
       "               \n",
       "                        [[-0.0587,  0.0193,  0.0122],\n",
       "                         [-0.0285, -0.0410, -0.0064],\n",
       "                         [-0.0896, -0.1205, -0.1315]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0058, -0.0512, -0.0327],\n",
       "                         [-0.0286,  0.0406, -0.0092],\n",
       "                         [-0.0987, -0.0534, -0.0073]],\n",
       "               \n",
       "                        [[ 0.0504,  0.0634, -0.0348],\n",
       "                         [ 0.0749,  0.0824, -0.0499],\n",
       "                         [ 0.0144, -0.0386, -0.0749]],\n",
       "               \n",
       "                        [[-0.1226, -0.0267, -0.0387],\n",
       "                         [-0.0093, -0.0446,  0.0246],\n",
       "                         [-0.0672, -0.0132,  0.0411]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0031,  0.0641,  0.0438],\n",
       "                         [-0.0915, -0.0203, -0.0242],\n",
       "                         [-0.0617, -0.0906, -0.0471]],\n",
       "               \n",
       "                        [[ 0.0068,  0.0711,  0.1052],\n",
       "                         [-0.0174,  0.0057,  0.0219],\n",
       "                         [-0.0031, -0.0315, -0.1080]],\n",
       "               \n",
       "                        [[ 0.0628,  0.0080, -0.0164],\n",
       "                         [ 0.0264,  0.1092,  0.1264],\n",
       "                         [-0.0955, -0.1417, -0.1264]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0972,  0.0369,  0.0480],\n",
       "                         [-0.0115, -0.0245, -0.0227],\n",
       "                         [-0.1521, -0.1955, -0.1500]],\n",
       "               \n",
       "                        [[-0.0117, -0.0073, -0.0400],\n",
       "                         [-0.1341, -0.0755, -0.0704],\n",
       "                         [-0.0268, -0.1210, -0.0849]],\n",
       "               \n",
       "                        [[-0.0038,  0.0287, -0.0076],\n",
       "                         [-0.0824, -0.0494, -0.0018],\n",
       "                         [-0.1012, -0.0940, -0.1187]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0611,  0.0508,  0.0375],\n",
       "                         [ 0.0961,  0.0412, -0.0611],\n",
       "                         [ 0.0429,  0.0101, -0.0725]],\n",
       "               \n",
       "                        [[ 0.0434,  0.1034, -0.0053],\n",
       "                         [ 0.0027, -0.0326, -0.1325],\n",
       "                         [-0.0544, -0.1625, -0.2300]],\n",
       "               \n",
       "                        [[ 0.0307,  0.0314,  0.0233],\n",
       "                         [ 0.0754,  0.0144,  0.0030],\n",
       "                         [ 0.0316, -0.0285, -0.0930]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0118,  0.0233, -0.0199],\n",
       "                         [-0.0341, -0.0957, -0.1396],\n",
       "                         [-0.1127, -0.0819, -0.0642]],\n",
       "               \n",
       "                        [[ 0.0194,  0.0239, -0.0471],\n",
       "                         [ 0.1347,  0.0400, -0.0835],\n",
       "                         [ 0.0947,  0.0211, -0.1307]],\n",
       "               \n",
       "                        [[-0.0271, -0.0602,  0.0258],\n",
       "                         [-0.0274, -0.0489, -0.1176],\n",
       "                         [ 0.0031, -0.0832, -0.0594]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0032,  0.0791,  0.0185],\n",
       "                         [ 0.0502,  0.0129, -0.0765],\n",
       "                         [ 0.0472, -0.0265, -0.0551]],\n",
       "               \n",
       "                        [[-0.1876, -0.0086, -0.1380],\n",
       "                         [-0.0601, -0.1294, -0.2139],\n",
       "                         [ 0.0698, -0.0897, -0.0521]],\n",
       "               \n",
       "                        [[ 0.0626,  0.0335, -0.0584],\n",
       "                         [ 0.0309, -0.0354, -0.1365],\n",
       "                         [ 0.0311,  0.0083, -0.0688]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0316, -0.0375, -0.1196],\n",
       "                         [ 0.0712, -0.0530, -0.1013],\n",
       "                         [ 0.0500, -0.0453, -0.0645]],\n",
       "               \n",
       "                        [[ 0.0346,  0.0768,  0.0128],\n",
       "                         [ 0.1027,  0.0829, -0.0568],\n",
       "                         [-0.0413,  0.0159, -0.1571]],\n",
       "               \n",
       "                        [[ 0.0735,  0.0469, -0.0161],\n",
       "                         [ 0.0570, -0.0184,  0.0291],\n",
       "                         [ 0.0241, -0.0660, -0.0195]]]], device='cuda:0')),\n",
       "              ('r6.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r6.bn2.weight',\n",
       "               tensor([0.8978, 0.6805, 0.9136, 0.9091, 0.8287, 0.8714, 0.9218, 0.9574, 0.8685,\n",
       "                       0.9169, 0.9464, 0.9221, 0.6245, 0.9317, 0.9325, 0.9523, 0.9194, 0.9073,\n",
       "                       0.8286, 0.9525, 0.9182, 0.9595, 0.7171, 0.9171, 0.8962, 0.8776, 0.9301,\n",
       "                       0.9161, 0.9433, 0.9015, 0.8837, 0.9212, 0.8776, 0.9144, 0.9001, 0.9443,\n",
       "                       0.9187, 0.8765, 0.9465, 0.8944, 0.6597, 0.9197, 0.9133, 0.9318, 0.9160,\n",
       "                       0.9549, 0.6889, 0.9245, 0.9228, 0.7445, 0.8431, 0.6877, 0.9208, 0.9262,\n",
       "                       0.9479, 0.8970, 0.9447, 0.9532, 0.8835, 0.7280, 0.9129, 0.9637, 0.9346,\n",
       "                       0.9297, 0.8139, 0.8914, 0.9211, 0.8954, 0.9466, 0.8984, 0.8779, 0.9404,\n",
       "                       0.8879, 0.8232, 0.7839, 0.9035, 0.6777, 0.7139, 0.8224, 0.8912, 0.8331,\n",
       "                       0.9734, 0.9572, 0.6811, 0.9383, 0.8719, 0.7285, 0.9184, 0.9090, 0.9186,\n",
       "                       0.9393, 0.9549, 0.9576, 0.7331, 0.9439, 0.9493, 0.8278, 0.9184, 0.9238,\n",
       "                       0.7894, 0.8729, 0.6303, 0.8694, 0.8995, 0.9325, 0.9748, 0.8713, 0.8434,\n",
       "                       0.8817, 0.8898, 0.8713, 0.9835, 0.9329, 0.8691, 0.8788, 0.9203, 0.9506,\n",
       "                       0.8315, 0.9029, 0.9007, 0.7829, 0.9176, 0.9112, 0.8983, 0.9249, 0.8682,\n",
       "                       0.9351, 0.9389, 0.7669, 0.9264, 0.8933, 0.7470, 0.9313, 0.9454, 0.8880,\n",
       "                       0.9024, 0.8230, 0.9526, 0.9205, 0.8889, 0.9295, 0.7978, 0.8517, 0.9231,\n",
       "                       0.9229, 0.9122, 0.9336, 0.9061, 0.8776, 0.8591, 0.8506, 0.8087, 0.8938,\n",
       "                       0.9605, 0.9518, 0.9078, 0.9074, 0.8797, 0.9373, 0.8632, 0.9264, 0.6924,\n",
       "                       0.8370, 0.9092, 0.9184, 0.8578, 0.8085, 0.6757, 0.6982, 0.9008, 0.9930,\n",
       "                       0.9003, 0.8168, 0.8808, 0.6842, 0.9076, 0.7712, 0.9087, 0.9210, 0.9011,\n",
       "                       0.7949, 0.8830, 0.9063, 0.8320, 0.8879, 0.9211, 0.9318, 0.9125, 0.8286,\n",
       "                       0.8105, 0.9113, 0.9080, 0.9591, 0.8809, 0.8872, 0.8599, 0.8785, 0.9133,\n",
       "                       0.8801, 0.9171, 0.9275, 0.8818, 0.7994, 1.0095, 0.9123, 0.8216, 0.9635,\n",
       "                       0.8938, 0.9337, 0.9356, 0.8311, 0.7232, 0.7914, 0.9324, 0.9321, 0.8846,\n",
       "                       0.9686, 0.8421, 0.7947, 0.8794, 0.8954, 0.9074, 0.9327, 0.8961, 0.9147,\n",
       "                       0.8108, 0.8440, 0.9246, 0.7395, 0.9092, 0.9418, 0.8703, 0.9117, 0.8857,\n",
       "                       0.8024, 0.9011, 0.9450, 0.8585, 0.8910, 0.8187, 0.9917, 0.9051, 0.9248,\n",
       "                       0.9393, 0.9308, 0.9193, 0.9014, 0.8516, 0.9719, 0.7611, 1.0207, 0.8893,\n",
       "                       0.8249, 0.9079, 0.9431, 0.9087], device='cuda:0')),\n",
       "              ('r6.bn2.bias',\n",
       "               tensor([-0.1924, -0.2447, -0.2023, -0.2364, -0.2617, -0.3023, -0.2195, -0.2142,\n",
       "                       -0.2043, -0.2239, -0.2246, -0.2861, -0.3202, -0.1467, -0.2298, -0.2608,\n",
       "                       -0.2529, -0.1974, -0.1895, -0.2314, -0.2301, -0.2056, -0.1723, -0.1993,\n",
       "                       -0.1648, -0.2016, -0.2449, -0.2406, -0.2034, -0.1793, -0.1662, -0.1832,\n",
       "                       -0.2179, -0.1418, -0.1541, -0.1331, -0.2322, -0.2289, -0.2065, -0.1085,\n",
       "                       -0.2490, -0.1975, -0.1766, -0.2097, -0.1660, -0.1720, -0.2449, -0.1965,\n",
       "                       -0.1634, -0.4031, -0.1886, -0.3202, -0.1702, -0.1895, -0.1720, -0.2369,\n",
       "                       -0.1691, -0.2696, -0.2261, -0.4079, -0.1783, -0.2289, -0.1796, -0.1477,\n",
       "                       -0.2323, -0.1359, -0.1829, -0.1324, -0.1968, -0.2887, -0.2888, -0.2720,\n",
       "                       -0.1574, -0.2262, -0.2449, -0.2432, -0.2977, -0.2942, -0.2034, -0.2445,\n",
       "                       -0.2582, -0.2129, -0.2081, -0.3539, -0.1736, -0.1766, -0.3954, -0.2340,\n",
       "                       -0.1730, -0.2359, -0.1706, -0.1853, -0.2415, -0.3122, -0.2465, -0.2000,\n",
       "                       -0.2251, -0.1610, -0.1161, -0.2722, -0.2755, -0.3143, -0.1542, -0.1735,\n",
       "                       -0.2103, -0.2715, -0.1965, -0.2651, -0.1984, -0.3372, -0.3880, -0.1355,\n",
       "                       -0.2070, -0.1547, -0.2719, -0.1681, -0.1920, -0.1883, -0.2415, -0.2007,\n",
       "                       -0.2534, -0.1547, -0.2280, -0.2796, -0.2557, -0.3283, -0.2171, -0.2205,\n",
       "                       -0.2813, -0.2280, -0.1417, -0.3782, -0.2228, -0.1342, -0.2103, -0.2156,\n",
       "                       -0.3000, -0.1712, -0.2088, -0.2380, -0.2297, -0.2035, -0.3166, -0.2243,\n",
       "                       -0.2038, -0.1457, -0.2474, -0.1816, -0.2252, -0.2375, -0.1600, -0.1959,\n",
       "                       -0.1954, -0.2990, -0.1456, -0.1887, -0.1383, -0.2202, -0.2594, -0.1764,\n",
       "                       -0.1810, -0.3495, -0.2118, -0.2102, -0.1850, -0.3085, -0.2469, -0.3009,\n",
       "                       -0.2932, -0.1491, -0.2057, -0.1335, -0.2796, -0.2115, -0.3096, -0.3510,\n",
       "                       -0.2552, -0.1580, -0.1349, -0.2110, -0.2759, -0.2482, -0.1214, -0.2297,\n",
       "                       -0.2101, -0.1785, -0.1609, -0.1999, -0.2330, -0.3288, -0.2592, -0.1394,\n",
       "                       -0.2713, -0.2784, -0.2531, -0.2437, -0.1295, -0.3228, -0.2703, -0.2291,\n",
       "                       -0.2359, -0.1627, -0.1748, -0.2122, -0.2538, -0.1912, -0.1600, -0.2016,\n",
       "                       -0.2402, -0.1537, -0.2299, -0.2351, -0.2385, -0.2108, -0.1404, -0.1727,\n",
       "                       -0.1092, -0.1665, -0.2588, -0.1268, -0.2224, -0.1524, -0.2261, -0.2433,\n",
       "                       -0.1368, -0.1779, -0.2604, -0.2212, -0.2996, -0.2144, -0.2537, -0.2289,\n",
       "                       -0.2298, -0.2234, -0.2771, -0.1776, -0.3262, -0.2006, -0.2842, -0.2254,\n",
       "                       -0.2994, -0.2805, -0.1543, -0.1831, -0.1397, -0.2538, -0.1134, -0.3026,\n",
       "                       -0.2402, -0.2505, -0.4314, -0.1245, -0.2612, -0.2535, -0.2461, -0.2534],\n",
       "                      device='cuda:0')),\n",
       "              ('r6.bn2.running_mean',\n",
       "               tensor([ -9.4912, -10.8792,  -4.6189,  -4.8334,  -5.8038,  -7.2912,  -6.1766,\n",
       "                        -7.9012,  -2.1838,  -6.8200,  -7.6414,  -2.8890, -10.7816, -12.7148,\n",
       "                        -8.2160,  -5.0018,  -5.4287,  -9.9145,  -1.6094,  -6.1909,  -7.0356,\n",
       "                        -4.1868,  -6.0034,  -6.7607,  -6.5018,  -5.5168,  -5.9463,  -8.5640,\n",
       "                        -6.3523,  -6.1659,  -8.5646,  -5.2303,  -5.3216,  -8.8771,  -5.6068,\n",
       "                        -9.7993,  -5.5358,  -6.2956,  -5.2702,  -7.2483,  -3.7404,  -8.4534,\n",
       "                        -8.2858,  -5.9118,  -7.4312,  -5.6474,  -9.6826,  -3.4912,  -8.5499,\n",
       "                        -6.2102,  -8.8106, -15.5987, -11.5849,  -5.0837,  -9.7286,  -8.3193,\n",
       "                        -5.1871,  -5.5283,  -4.4491,  -7.0782,  -8.4390,  -6.0551,  -6.2342,\n",
       "                        -9.4877,  -9.3473, -11.7590,  -7.0546,  -9.5609,  -9.1730,  -4.0426,\n",
       "                        -5.0608,  -8.1565,  -9.6923,  -7.7600,  -5.1466,  -7.2529, -10.4802,\n",
       "                        -4.8470, -11.6651,  -4.9653,  -6.0014,  -7.1198,  -8.5390, -15.7229,\n",
       "                        -5.7228,  -4.4529,  -4.9800,  -6.1629,  -7.8603,  -2.8966,  -9.2494,\n",
       "                        -9.6986,  -6.2028,  -8.4153,  -3.8144,  -5.9021,  -3.3810,  -8.5234,\n",
       "                        -8.9487,  -7.8414,  -2.5889,  -6.0674,  -6.6545,  -7.5518,  -4.8287,\n",
       "                        -5.7399,  -5.0298,  -7.6334,  -4.5504,  -4.5326,  -4.5459, -11.3489,\n",
       "                        -9.4813,  -8.1003,  -5.7726, -10.3773, -10.0066, -11.2829,  -9.0277,\n",
       "                        -3.6566,  -4.5723,  -6.1607,  -6.4944,  -1.5747,  -3.1703,  -7.0102,\n",
       "                        -7.7101,  -6.8596,  -5.1961,  -5.2327,  -6.2355,  -2.7425,  -8.9725,\n",
       "                        -8.6015,  -4.0936,  -7.1354,  -4.1598,  -6.4549,  -3.4946,  -4.0683,\n",
       "                        -6.5398,  -5.0825,  -8.2192,  -3.2706,  -7.6867,  -8.9802,  -7.7172,\n",
       "                       -11.3140,  -9.5493,  -8.9966,  -7.7290,  -9.4039,  -5.5833,  -6.7026,\n",
       "                        -6.4047,  -9.9816,  -8.3617,  -5.4533,  -6.1649, -15.8915,  -6.9066,\n",
       "                       -10.9437,  -5.7655,  -6.3899,  -7.5720,  -5.0923,  -5.1807,  -3.2808,\n",
       "                        -2.3271,  -8.1939,  -5.9578,  -8.0394,  -6.4713,  -4.8702,  -4.4462,\n",
       "                        -4.9508,  -5.5639,  -6.7215, -14.9305,  -5.3340,  -8.6029,  -3.6683,\n",
       "                       -11.9010,  -9.6403, -11.3784,  -5.3555, -10.2574,  -8.1958,  -9.5277,\n",
       "                       -14.9949,  -2.1593, -11.9624,  -6.4462,  -6.4141,   1.3108,  -8.6074,\n",
       "                        -5.1369,  -5.7021,  -5.8870,  -7.7004,  -1.7030,  -4.4559,  -4.1073,\n",
       "                        -6.1563,  -3.7604,  -9.2367, -10.4723,  -6.0606,  -6.1731,  -7.6315,\n",
       "                        -6.8293,  -3.0130,  -7.6179,  -8.1405, -10.7566,  -5.3891,  -5.7103,\n",
       "                        -4.5203,  -8.2201, -12.2589,  -3.3498, -10.9601,  -7.1192,  -9.5910,\n",
       "                        -7.3220,  -9.2538,  -4.8708,  -4.1412,  -4.7019,  -6.5713,  -4.8133,\n",
       "                        -4.3207,  -4.5731,  -4.8764,  -3.1211,  -9.1112,  -5.1200,  -0.2847,\n",
       "                        -7.4242, -10.7260,  -5.9324,  -6.9224,  -6.3074,  -8.9432,  -9.7702,\n",
       "                        -7.9091, -10.9162,  -4.7551,  -6.7614,  -4.3351,  -3.6768, -10.6937,\n",
       "                        -9.8064,  -5.2994,  -7.1515,  -3.0798], device='cuda:0')),\n",
       "              ('r6.bn2.running_var',\n",
       "               tensor([553.1585, 178.4396, 300.6385, 346.5598, 258.3226, 172.1668, 242.0635,\n",
       "                       240.3674, 210.6842, 205.6129, 225.6115, 216.4830, 129.6687, 490.0221,\n",
       "                       323.2346, 198.4011, 197.1270, 319.9908, 208.2039, 206.5413, 154.5148,\n",
       "                       201.6512, 290.8278, 339.8303, 468.9858, 157.2867, 246.3215, 248.0274,\n",
       "                       206.5683, 259.6046, 549.8019, 275.6976, 143.1450, 287.7416, 370.6895,\n",
       "                       489.7972, 230.7373, 259.5675, 166.6467, 572.4450,  78.0905, 366.2521,\n",
       "                       190.3169, 221.3141, 326.5474, 171.9791, 256.8936, 216.7473, 532.1648,\n",
       "                        76.8206, 252.3796, 234.5956, 434.5347, 272.6800, 425.0520, 204.2776,\n",
       "                       241.6066, 171.8449, 208.0530,  67.8901, 368.5014, 208.9276, 458.7100,\n",
       "                       330.1405, 179.7825, 586.2976, 254.4697, 400.7226, 332.8967, 151.3241,\n",
       "                       158.8029, 175.9760, 378.2014, 234.8173, 267.2762, 315.5914, 184.2840,\n",
       "                        63.1229, 310.4147, 240.6050, 122.1444, 198.2958, 360.5956, 291.8809,\n",
       "                       153.7040, 298.3407,  66.0821, 232.9986, 320.1956, 175.6391, 410.1960,\n",
       "                       348.6873, 194.9552, 207.5589, 226.9763, 281.7661,  75.0534, 371.5368,\n",
       "                       405.4265, 221.5366, 249.5275, 129.3185, 583.3104, 370.4863, 250.8972,\n",
       "                       185.9479, 349.3740, 185.8570, 470.3515, 303.1758,  77.4006, 293.5469,\n",
       "                       207.7616, 226.9837, 138.3691, 473.2134, 376.8140, 299.0420, 265.3178,\n",
       "                       314.3236,  88.6131, 322.3785, 235.4041, 281.4589, 223.3939, 197.5174,\n",
       "                       279.7705, 190.8320, 113.0665, 225.7162, 250.1623,  63.7255, 344.4167,\n",
       "                       515.4155, 265.9317, 153.0041,  87.1797, 296.0431, 312.3525, 191.0731,\n",
       "                       197.4766, 286.9911, 202.4996, 323.9379, 373.1860, 303.8165, 235.4860,\n",
       "                       577.9083, 334.9980, 214.3207, 353.6921, 390.7761, 238.5382, 168.0219,\n",
       "                       367.7724, 488.8183, 399.2727, 278.6250, 196.5096, 541.4457, 335.0272,\n",
       "                       156.8605, 168.8838, 470.8729, 447.0924, 158.4581,  79.4184, 233.0155,\n",
       "                       226.3413, 396.9499, 227.4507, 356.4537, 153.9010, 241.9465,  41.5944,\n",
       "                       110.0401,  82.9118, 445.3719, 750.6807, 228.8665, 221.3761, 178.4514,\n",
       "                       407.4458, 154.7910, 300.7809, 305.3842, 438.4135, 399.9998, 165.2704,\n",
       "                       205.4499, 251.2821, 367.3454, 146.0255, 152.1246, 379.9993, 148.8697,\n",
       "                       170.2129, 125.1833, 159.5103, 361.9698, 215.5898, 220.5007, 200.4817,\n",
       "                       143.2185, 181.4450, 242.1470, 323.7773, 337.9390, 170.2265, 424.9121,\n",
       "                       148.2678, 103.1801, 168.2546, 223.5795, 410.4672, 368.9226, 235.7752,\n",
       "                       247.3945, 166.7399, 613.7322, 444.4831, 467.7536, 242.8273, 208.0220,\n",
       "                       457.9845, 333.4984, 101.1771, 315.4497,  76.4946, 226.3956, 267.6721,\n",
       "                       117.4037, 256.1129, 246.3232,  62.2907, 285.1340, 136.9192, 194.5550,\n",
       "                       135.8734, 178.0033, 104.8703, 374.7382, 318.3897, 359.8919, 485.4179,\n",
       "                       199.7391, 315.0176, 208.5765, 201.8120, 146.6300,  99.0749, 542.0485,\n",
       "                       214.1458, 137.2931, 213.3618, 228.6363], device='cuda:0')),\n",
       "              ('r6.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r6.c2.weights',\n",
       "               tensor([[[[-0.0553, -0.0214,  0.0463],\n",
       "                         [ 0.0334,  0.0011,  0.0388],\n",
       "                         [ 0.0125,  0.0029,  0.0638]],\n",
       "               \n",
       "                        [[-0.0112, -0.0490, -0.0546],\n",
       "                         [ 0.0341,  0.0234, -0.0140],\n",
       "                         [-0.0168, -0.0078, -0.0695]],\n",
       "               \n",
       "                        [[ 0.0401,  0.0122,  0.0418],\n",
       "                         [ 0.0703, -0.0181,  0.0578],\n",
       "                         [-0.0007, -0.0458,  0.0457]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0215,  0.0241,  0.0187],\n",
       "                         [ 0.0128, -0.0222,  0.0188],\n",
       "                         [-0.0333, -0.0053, -0.0204]],\n",
       "               \n",
       "                        [[-0.0724, -0.0473,  0.0468],\n",
       "                         [-0.0368, -0.0354, -0.0273],\n",
       "                         [-0.0187,  0.0243,  0.0153]],\n",
       "               \n",
       "                        [[-0.0061,  0.0358,  0.0125],\n",
       "                         [ 0.0515,  0.0870,  0.0185],\n",
       "                         [ 0.0414,  0.1006, -0.0159]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0415, -0.0029, -0.0390],\n",
       "                         [ 0.0431,  0.0066,  0.0024],\n",
       "                         [ 0.1080,  0.1129,  0.0433]],\n",
       "               \n",
       "                        [[ 0.0096, -0.0490, -0.0922],\n",
       "                         [-0.0379,  0.0080, -0.0547],\n",
       "                         [ 0.0017, -0.0531, -0.0026]],\n",
       "               \n",
       "                        [[ 0.0619,  0.0684, -0.0051],\n",
       "                         [-0.0146, -0.0206, -0.0378],\n",
       "                         [-0.0205, -0.0493, -0.0851]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0532, -0.0162,  0.0177],\n",
       "                         [-0.1402, -0.0695, -0.1182],\n",
       "                         [-0.0651, -0.0469, -0.0533]],\n",
       "               \n",
       "                        [[ 0.0571,  0.0994,  0.0303],\n",
       "                         [ 0.0435,  0.0116, -0.0054],\n",
       "                         [ 0.0959,  0.0083, -0.0261]],\n",
       "               \n",
       "                        [[-0.0332, -0.0668, -0.0799],\n",
       "                         [-0.0113,  0.0594, -0.0382],\n",
       "                         [-0.0773,  0.0416,  0.0020]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0331,  0.0458,  0.0970],\n",
       "                         [ 0.0183,  0.0496,  0.0898],\n",
       "                         [ 0.0973,  0.0558,  0.0840]],\n",
       "               \n",
       "                        [[-0.0550, -0.0512, -0.0135],\n",
       "                         [-0.0366, -0.0010,  0.0576],\n",
       "                         [-0.0143,  0.0066,  0.0564]],\n",
       "               \n",
       "                        [[ 0.0214,  0.0346,  0.0620],\n",
       "                         [-0.0041, -0.0185, -0.0044],\n",
       "                         [-0.0283, -0.0449, -0.0277]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0371,  0.0188,  0.0522],\n",
       "                         [-0.0071, -0.0093, -0.0065],\n",
       "                         [ 0.0510, -0.0093,  0.0141]],\n",
       "               \n",
       "                        [[ 0.0413,  0.0649,  0.0587],\n",
       "                         [ 0.0745,  0.0136, -0.0040],\n",
       "                         [ 0.0352,  0.0524,  0.0252]],\n",
       "               \n",
       "                        [[-0.0669, -0.0976, -0.0048],\n",
       "                         [-0.0856, -0.0552,  0.0028],\n",
       "                         [ 0.0034, -0.0166, -0.0861]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0065, -0.0146, -0.0485],\n",
       "                         [ 0.0049,  0.0479,  0.0602],\n",
       "                         [ 0.0798,  0.0694,  0.0443]],\n",
       "               \n",
       "                        [[-0.0019, -0.0118, -0.0150],\n",
       "                         [-0.0259, -0.0276,  0.0429],\n",
       "                         [-0.0458, -0.0334,  0.0095]],\n",
       "               \n",
       "                        [[ 0.0705,  0.0514,  0.0335],\n",
       "                         [ 0.0372,  0.0141,  0.0166],\n",
       "                         [ 0.0352,  0.0249,  0.0440]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0139,  0.1164,  0.1212],\n",
       "                         [ 0.0259,  0.0082,  0.0608],\n",
       "                         [-0.0521,  0.0096,  0.0349]],\n",
       "               \n",
       "                        [[-0.0567, -0.0736,  0.1078],\n",
       "                         [ 0.0294,  0.1132,  0.1383],\n",
       "                         [ 0.0771,  0.1427,  0.0127]],\n",
       "               \n",
       "                        [[-0.0092, -0.0590, -0.1330],\n",
       "                         [ 0.0050, -0.0658, -0.0641],\n",
       "                         [ 0.1144, -0.0411, -0.0863]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0058, -0.0170, -0.0110],\n",
       "                         [ 0.0296,  0.0586, -0.0202],\n",
       "                         [ 0.0671,  0.0308,  0.0175]],\n",
       "               \n",
       "                        [[-0.0564, -0.0751, -0.0293],\n",
       "                         [-0.0152,  0.0062, -0.0196],\n",
       "                         [-0.0499, -0.0347,  0.0072]],\n",
       "               \n",
       "                        [[-0.0129, -0.0251,  0.0326],\n",
       "                         [ 0.0748, -0.0140,  0.0383],\n",
       "                         [ 0.0198,  0.0431,  0.0268]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0629,  0.0891,  0.1012],\n",
       "                         [ 0.0706,  0.0674,  0.0582],\n",
       "                         [-0.0298,  0.0740,  0.0764]],\n",
       "               \n",
       "                        [[ 0.0253, -0.0814, -0.0969],\n",
       "                         [ 0.0855, -0.0153, -0.0649],\n",
       "                         [ 0.0614,  0.0018, -0.0043]],\n",
       "               \n",
       "                        [[ 0.0851,  0.0032,  0.0757],\n",
       "                         [ 0.1133,  0.0059,  0.0191],\n",
       "                         [ 0.0428,  0.0364,  0.0134]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0799,  0.0304,  0.0098],\n",
       "                         [-0.1003,  0.0267, -0.0309],\n",
       "                         [-0.0483, -0.0281, -0.0254]],\n",
       "               \n",
       "                        [[-0.0056,  0.0254, -0.0244],\n",
       "                         [ 0.0197,  0.0240,  0.0330],\n",
       "                         [ 0.0129,  0.0277,  0.0677]],\n",
       "               \n",
       "                        [[-0.1002, -0.0845, -0.0003],\n",
       "                         [-0.0928, -0.0564, -0.0281],\n",
       "                         [-0.0625, -0.0347, -0.0155]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0218, -0.0329, -0.0522],\n",
       "                         [ 0.0117,  0.0188, -0.0246],\n",
       "                         [-0.0276,  0.0424, -0.0258]],\n",
       "               \n",
       "                        [[ 0.1072,  0.0859,  0.0848],\n",
       "                         [ 0.0117,  0.0322,  0.0400],\n",
       "                         [-0.0505, -0.0576, -0.0514]],\n",
       "               \n",
       "                        [[ 0.0298,  0.0281, -0.0174],\n",
       "                         [ 0.0609,  0.0640, -0.0337],\n",
       "                         [-0.0018,  0.0437, -0.0306]]]], device='cuda:0')),\n",
       "              ('r6.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r7.bn1.weight',\n",
       "               tensor([0.8736, 0.9557, 1.0783, 0.8535, 1.0502, 1.0732, 0.9943, 0.7968, 0.9661,\n",
       "                       0.9022, 0.9421, 1.0220, 1.0128, 0.9282, 0.8945, 1.0103, 0.8513, 0.9471,\n",
       "                       1.1143, 1.0560, 0.9032, 0.8982, 0.9140, 1.1445, 0.8473, 0.8711, 0.9514,\n",
       "                       0.9571, 1.0790, 1.0238, 1.0416, 1.0475, 0.8673, 0.9228, 0.9433, 0.9454,\n",
       "                       1.0569, 0.9397, 1.0240, 0.9413, 0.8019, 1.0991, 0.9703, 0.9326, 0.9014,\n",
       "                       1.0650, 0.9105, 0.9326, 0.9552, 0.9433, 1.0388, 0.8653, 1.0535, 0.9929,\n",
       "                       1.0700, 0.9426, 0.9474, 0.9605, 0.9931, 0.9351, 1.0333, 1.0203, 0.9653,\n",
       "                       0.8630, 0.9700, 0.9179, 0.9602, 0.8817, 0.9726, 0.9106, 1.0151, 0.8923,\n",
       "                       0.8615, 1.0405, 1.0838, 0.8401, 1.0350, 0.8829, 1.0003, 0.9135, 0.8970,\n",
       "                       0.8821, 0.9006, 0.9656, 1.0561, 1.0728, 0.8260, 0.8981, 0.9643, 0.9439,\n",
       "                       0.9611, 0.8528, 0.8678, 0.9677, 0.8582, 0.9521, 1.0431, 0.9818, 0.9161,\n",
       "                       0.9094, 0.9262, 0.9214, 0.9641, 1.0981, 0.9360, 0.8515, 1.0158, 0.8489,\n",
       "                       0.9324, 0.8489, 0.8801, 0.9598, 0.9090, 0.9909, 0.8235, 0.9596, 0.8793,\n",
       "                       0.9572, 1.1126, 1.0214, 0.9522, 0.9506, 0.9389, 1.1373, 0.8892, 1.0486,\n",
       "                       1.0303, 1.0671, 0.9112, 0.8082, 0.9955, 0.8662, 0.9420, 1.0839, 0.9283,\n",
       "                       0.9205, 0.8724, 0.7671, 1.1310, 1.0575, 0.9777, 0.8877, 0.9663, 0.7706,\n",
       "                       1.0858, 0.9015, 0.9195, 0.8728, 0.9741, 1.0406, 1.0840, 0.9350, 0.9205,\n",
       "                       0.9865, 0.9594, 0.8314, 0.8259, 1.0513, 0.9115, 0.8986, 1.0008, 0.9808,\n",
       "                       0.9625, 0.8717, 1.0231, 0.8051, 0.9999, 1.0602, 0.9734, 1.0792, 0.9401,\n",
       "                       0.9544, 0.9741, 0.9892, 0.9081, 0.9131, 1.0106, 0.8192, 0.8957, 1.1812,\n",
       "                       1.0067, 1.0684, 0.8851, 1.1241, 1.0027, 1.0639, 1.0270, 1.1480, 0.9691,\n",
       "                       0.8027, 0.9957, 1.0137, 0.9996, 0.9227, 0.9007, 1.1005, 0.9330, 0.9632,\n",
       "                       0.9997, 0.8713, 0.8687, 0.9287, 0.9689, 0.9607, 0.8145, 0.8628, 1.0310,\n",
       "                       0.8147, 0.9219, 0.8059, 0.9323, 0.9767, 0.8439, 0.9975, 0.8346, 0.8326,\n",
       "                       0.9943, 0.9850, 0.9939, 0.9599, 0.9821, 0.7936, 0.9248, 0.8203, 0.9981,\n",
       "                       1.0195, 0.6988, 1.0076, 1.1985, 0.7435, 0.9825, 0.9226, 0.8312, 0.9365,\n",
       "                       0.9688, 1.0109, 0.8414, 0.9651, 0.9633, 1.0387, 0.9967, 0.9485, 1.0375,\n",
       "                       0.9126, 1.0349, 0.9497, 0.9702, 1.1730, 0.8844, 1.0254, 0.9964, 1.0880,\n",
       "                       0.7817, 0.9172, 1.0009, 0.9182], device='cuda:0')),\n",
       "              ('r7.bn1.bias',\n",
       "               tensor([-0.1421, -0.1903, -0.1054, -0.1118, -0.1525,  0.0372, -0.2276, -0.2994,\n",
       "                       -0.3404, -0.0067, -0.0495, -0.2653, -0.1895, -0.2707, -0.2113, -0.3284,\n",
       "                       -0.2132, -0.0422, -0.2876, -0.2184, -0.1868, -0.2178, -0.2452, -0.2058,\n",
       "                       -0.3308, -0.0736, -0.1320, -0.1803, -0.4528, -0.3561, -0.3642, -0.0922,\n",
       "                       -0.1014, -0.3865, -0.2970, -0.3074, -0.2570, -0.1258, -0.1868, -0.2551,\n",
       "                       -0.2388, -0.1419, -0.2352, -0.1946, -0.2095, -0.1481, -0.1128, -0.1495,\n",
       "                       -0.2956, -0.3428, -0.2000, -0.3217, -0.1096, -0.0642, -0.1919, -0.1719,\n",
       "                       -0.2340, -0.1612, -0.2208, -0.2357, -0.0394, -0.1775, -0.3187, -0.1548,\n",
       "                       -0.2519, -0.2275, -0.3790, -0.0914, -0.2080, -0.1931, -0.1183, -0.2130,\n",
       "                        0.0503, -0.1190, -0.2426, -0.2765, -0.3844, -0.0605, -0.2558, -0.2156,\n",
       "                       -0.3910, -0.2358, -0.0611, -0.0308, -0.1218, -0.2249, -0.1957, -0.2169,\n",
       "                       -0.0681, -0.3885, -0.0463, -0.3026, -0.3374, -0.2636, -0.2448,  0.0373,\n",
       "                       -0.1708, -0.3066, -0.1943, -0.3396, -0.1098, -0.1203, -0.4016, -0.2907,\n",
       "                       -0.2311, -0.3341, -0.2597, -0.2670, -0.3409, -0.2788, -0.2803, -0.3697,\n",
       "                       -0.0222, -0.2280, -0.2272, -0.2715, -0.2238, -0.2141, -0.1999, -0.1149,\n",
       "                       -0.3498, -0.1117,  0.0053, -0.0585, -0.3908, -0.3707, -0.1470, -0.1870,\n",
       "                       -0.1681, -0.2800, -0.1049, -0.2326, -0.2555, -0.2264, -0.0651, -0.2802,\n",
       "                       -0.0547, -0.2026, -0.4736, -0.1525, -0.2506, -0.1312, -0.2796, -0.2202,\n",
       "                       -0.1382, -0.2250, -0.2685, -0.2996, -0.1260, -0.1410, -0.2388, -0.1727,\n",
       "                       -0.2453, -0.2677, -0.3682, -0.0521, -0.2129, -0.0685, -0.0732, -0.1245,\n",
       "                       -0.1347, -0.3307, -0.1631, -0.1300, -0.1326, -0.3476, -0.1155, -0.2192,\n",
       "                       -0.1930, -0.4053, -0.2130, -0.3701, -0.1820, -0.1590, -0.3210, -0.0651,\n",
       "                       -0.2468, -0.1118, -0.2108, -0.1358, -0.0361, -0.1444, -0.3948, -0.2662,\n",
       "                       -0.1445, -0.2574, -0.2572,  0.0381, -0.0608, -0.2989, -0.0332, -0.0751,\n",
       "                       -0.1675, -0.2384, -0.2729, -0.3812, -0.1204, -0.1089, -0.1380, -0.3258,\n",
       "                       -0.3107, -0.3204, -0.2921, -0.2628, -0.3142, -0.1562, -0.2699, -0.2658,\n",
       "                       -0.3421, -0.2655, -0.2125, -0.3613, -0.0893, -0.3751, -0.2389, -0.2972,\n",
       "                       -0.1634, -0.1228, -0.2458, -0.1123, -0.3047, -0.3771, -0.2491, -0.1848,\n",
       "                       -0.0461, -0.1373, -0.3674, -0.2038,  0.0339, -0.4272, -0.2237, -0.2116,\n",
       "                       -0.2842, -0.1164, -0.1158, -0.2141, -0.3340, -0.1644, -0.1355, -0.2756,\n",
       "                       -0.3766, -0.2633, -0.1269, -0.3439, -0.1062, -0.3468, -0.0469,  0.0086,\n",
       "                       -0.1130, -0.1626, -0.0522, -0.0553, -0.3976, -0.2524, -0.3849, -0.1814],\n",
       "                      device='cuda:0')),\n",
       "              ('r7.bn1.running_mean',\n",
       "               tensor([ 1.5484e-01, -1.4886e-01, -1.7681e-01, -6.8466e-02, -1.2886e-01,\n",
       "                       -1.8104e-01, -7.8182e-03, -4.1509e-02, -1.1725e-01, -8.1706e-02,\n",
       "                       -4.1748e-02, -5.8520e-02, -6.3004e-02,  1.1168e-01,  4.3099e-02,\n",
       "                       -7.0741e-02, -1.6278e-02, -1.2330e-01,  1.3174e-01, -8.4279e-02,\n",
       "                       -4.8705e-03, -1.0713e-01,  1.2740e-01, -1.4875e-01, -9.3688e-03,\n",
       "                       -1.7504e-01, -3.3129e-02, -6.3455e-02, -1.8949e-01,  7.4440e-02,\n",
       "                       -9.7600e-02, -2.3777e-02, -1.3983e-01, -6.4402e-02, -9.0745e-02,\n",
       "                       -7.4397e-02, -1.8879e-01, -3.9264e-02, -1.4556e-02, -8.3765e-02,\n",
       "                       -1.7127e-01, -9.3258e-05, -4.8927e-02, -1.5870e-01, -9.0035e-02,\n",
       "                       -1.3101e-01, -6.3606e-02,  3.2669e-02, -1.1987e-01, -3.4320e-02,\n",
       "                       -1.1416e-01, -7.5669e-02,  6.2057e-02, -1.8558e-01, -9.5123e-02,\n",
       "                       -2.0225e-01, -1.3168e-01, -1.2092e-01, -9.0581e-02, -2.5968e-01,\n",
       "                       -1.4590e-01, -9.9549e-02, -1.4671e-01,  1.4456e-02, -1.4954e-01,\n",
       "                        1.0541e-02, -2.2541e-01, -1.0593e-01,  4.2202e-02,  1.0975e-01,\n",
       "                        2.2306e-01, -2.5754e-02,  1.1690e-01, -5.4345e-02, -2.9477e-02,\n",
       "                        1.5064e-01, -3.1925e-02,  1.1434e-02, -2.8311e-02, -8.2709e-03,\n",
       "                       -7.9089e-02, -2.9969e-02, -3.1011e-02, -4.2814e-01, -1.2368e-01,\n",
       "                       -6.7486e-02,  2.3268e-01, -8.4086e-02,  5.0130e-02, -1.4884e-01,\n",
       "                       -9.0042e-02, -1.2434e-01,  5.6937e-02, -7.7769e-02, -7.6323e-02,\n",
       "                       -7.0597e-02, -1.1976e-01, -4.4688e-02, -1.3654e-01, -1.4964e-01,\n",
       "                       -1.4041e-01,  3.1975e-02,  7.8235e-03, -1.0252e-01, -1.6594e-01,\n",
       "                        7.9137e-02, -1.5816e-01, -1.1089e-01, -8.9056e-02,  4.9963e-02,\n",
       "                        5.3567e-02, -2.4240e-02, -6.6110e-02, -3.0254e-02,  1.1704e-01,\n",
       "                       -1.5977e-01, -6.7058e-02,  3.8171e-02, -1.2620e-02,  2.4816e-02,\n",
       "                       -1.8630e-01, -1.8114e-01,  3.3375e-02, -1.5468e-01, -1.9457e-02,\n",
       "                        2.0085e-02, -4.9400e-02, -7.1297e-02,  2.0485e-02,  9.8507e-02,\n",
       "                       -2.0170e-01, -8.5884e-02, -2.4761e-01, -1.6650e-01, -1.1997e-01,\n",
       "                        6.5061e-02, -2.3870e-01, -8.7835e-02, -8.2807e-02,  5.9203e-02,\n",
       "                        1.6612e-01, -2.0728e-01, -1.1658e-01,  1.5119e-02, -1.3601e-01,\n",
       "                        2.1104e-01, -2.3401e-02, -1.3756e-01, -1.6646e-01, -7.0792e-02,\n",
       "                       -8.8415e-02, -2.5920e-02,  6.6856e-02, -9.9176e-02, -3.8293e-02,\n",
       "                       -6.0170e-02, -2.5345e-02, -1.7378e-02, -1.2119e-01, -8.2109e-02,\n",
       "                       -2.7854e-01, -1.8380e-01, -2.2667e-02,  1.2091e-01, -1.5557e-01,\n",
       "                        1.2338e-01, -5.7807e-02, -1.0961e-01,  1.3214e-02, -2.0210e-01,\n",
       "                       -1.0260e-01, -2.3880e-02,  3.2585e-01, -1.0637e-01,  4.3458e-03,\n",
       "                        6.1582e-03, -1.8041e-01, -1.0592e-01,  1.5196e-03, -1.9961e-01,\n",
       "                       -7.3441e-02, -1.6969e-01, -5.0327e-02, -1.4987e-01, -2.4526e-02,\n",
       "                        6.0633e-02,  1.5601e-02, -7.7755e-02, -5.4634e-02,  7.9738e-02,\n",
       "                       -1.8899e-01, -1.9093e-02,  4.4152e-02,  1.5892e-01, -1.9860e-01,\n",
       "                       -1.5225e-01,  6.3695e-02, -2.3390e-01, -3.2332e-01, -7.0895e-02,\n",
       "                        6.3498e-02,  1.0624e-01, -4.3993e-02, -1.4105e-01,  1.2376e-01,\n",
       "                       -2.0998e-02, -1.6199e-03,  4.6063e-02, -1.7997e-01,  1.0764e-01,\n",
       "                       -1.1506e-01, -3.1713e-02, -2.6577e-01,  8.3188e-02, -4.7410e-03,\n",
       "                        5.0233e-02, -2.5585e-01, -1.7424e-01, -2.3288e-01, -5.5581e-02,\n",
       "                        1.0256e-01, -3.8956e-02,  8.8159e-02, -4.4504e-02, -1.4899e-01,\n",
       "                       -6.7743e-02,  2.7097e-02, -1.3858e-01, -1.0783e-01,  6.7794e-02,\n",
       "                        8.6602e-02,  2.8257e-02, -7.4770e-02,  1.1724e-01, -6.0155e-03,\n",
       "                        1.9586e-02,  2.9664e-02,  3.4033e-02, -1.5808e-01, -1.2644e-01,\n",
       "                       -9.9900e-02, -9.1256e-02, -2.5381e-02, -1.0759e-01, -2.2098e-01,\n",
       "                       -1.3600e-01, -4.8969e-02, -1.6794e-01,  8.5130e-02, -7.6779e-02,\n",
       "                       -7.9230e-02,  3.6298e-02,  4.7326e-02,  2.8904e-03, -2.0495e-02,\n",
       "                       -1.7125e-01], device='cuda:0')),\n",
       "              ('r7.bn1.running_var',\n",
       "               tensor([0.9610, 0.7432, 1.4707, 0.6079, 0.6297, 0.7661, 1.1330, 0.3979, 0.4994,\n",
       "                       0.5837, 0.6866, 1.2384, 0.9310, 0.4732, 0.6662, 0.6557, 0.4234, 0.7527,\n",
       "                       1.9879, 0.7348, 0.9282, 0.7001, 0.9278, 1.5977, 0.5484, 0.8013, 0.8434,\n",
       "                       1.2139, 0.4485, 0.4930, 0.6206, 1.1421, 0.5476, 0.4447, 0.4776, 0.5308,\n",
       "                       0.7182, 0.5286, 0.7227, 0.5096, 0.4094, 0.7088, 0.8286, 0.6303, 0.5669,\n",
       "                       1.0727, 0.7937, 0.7812, 0.5699, 0.5932, 0.8715, 0.4505, 1.2761, 0.9066,\n",
       "                       0.5361, 0.8821, 0.9245, 0.6968, 0.5199, 0.5616, 0.9368, 0.9771, 0.3988,\n",
       "                       0.6974, 1.0092, 1.0072, 0.4803, 0.5169, 1.1429, 0.5464, 0.7543, 0.6121,\n",
       "                       1.0161, 0.5903, 1.2675, 0.4820, 1.1844, 1.2581, 1.0292, 0.5389, 0.7646,\n",
       "                       0.7255, 0.7444, 1.1050, 0.8813, 1.2821, 0.6151, 0.7339, 1.1030, 0.5020,\n",
       "                       1.2495, 0.4439, 0.5338, 0.7627, 0.8568, 1.1827, 0.6243, 0.7440, 0.4688,\n",
       "                       0.4406, 0.6243, 0.7439, 0.8954, 1.2744, 0.8049, 0.6294, 0.9334, 0.5053,\n",
       "                       0.6383, 0.5501, 0.5821, 0.7181, 0.8800, 0.8541, 0.4604, 0.9187, 0.7299,\n",
       "                       0.7257, 0.6041, 0.8668, 0.5879, 0.9092, 0.7872, 1.5861, 0.8739, 1.0667,\n",
       "                       0.8345, 0.6924, 0.7478, 0.4895, 0.9315, 0.5975, 0.4920, 1.2774, 0.7559,\n",
       "                       0.4485, 1.1197, 0.7662, 0.5417, 0.9000, 0.7094, 0.5408, 0.5756, 0.3761,\n",
       "                       0.8636, 1.0267, 0.9847, 0.5738, 1.4789, 0.5481, 0.7635, 0.9445, 0.5363,\n",
       "                       0.8474, 0.7359, 0.5947, 0.5195, 1.2719, 1.0316, 0.5050, 0.5748, 0.5923,\n",
       "                       0.6052, 0.6301, 0.7510, 0.7365, 1.0111, 0.6590, 0.8595, 1.4903, 0.7867,\n",
       "                       0.4776, 0.8689, 0.9278, 0.7766, 0.6947, 0.4666, 0.5041, 0.5916, 1.4314,\n",
       "                       1.0667, 1.8535, 0.9230, 0.6753, 0.6351, 1.0245, 0.7365, 0.7774, 0.9047,\n",
       "                       0.5164, 0.7723, 0.9689, 0.7988, 1.2437, 0.8705, 1.3848, 0.5472, 0.8322,\n",
       "                       0.6348, 0.4800, 0.5452, 0.4405, 0.8978, 0.6446, 0.7025, 0.5911, 0.9011,\n",
       "                       0.4581, 0.4489, 0.4397, 0.9512, 1.1822, 0.5805, 0.9716, 0.6489, 0.6372,\n",
       "                       0.8717, 0.7195, 0.5586, 0.4722, 0.7176, 0.3334, 0.7991, 0.4738, 0.9721,\n",
       "                       1.0682, 0.4272, 0.7150, 1.6052, 0.3440, 0.7435, 0.7235, 0.6543, 0.6606,\n",
       "                       0.8899, 0.9797, 0.6944, 0.5114, 0.5453, 0.5701, 0.4755, 0.7574, 1.1868,\n",
       "                       0.3932, 0.7118, 0.5441, 0.7719, 1.2989, 1.0758, 1.6117, 0.8006, 1.2763,\n",
       "                       0.4634, 0.5114, 0.9656, 0.9660], device='cuda:0')),\n",
       "              ('r7.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r7.c1.weights',\n",
       "               tensor([[[[-1.0646e-01, -1.0980e-01, -8.3954e-02],\n",
       "                         [-8.5130e-02, -1.2763e-01, -6.3853e-02],\n",
       "                         [ 2.1760e-02, -3.6780e-02, -1.6602e-02]],\n",
       "               \n",
       "                        [[-3.3403e-02,  2.7338e-03,  6.8606e-02],\n",
       "                         [-9.0707e-03, -3.9045e-03, -1.1322e-02],\n",
       "                         [ 8.0954e-02,  3.4147e-02, -3.6774e-03]],\n",
       "               \n",
       "                        [[ 1.0573e-02,  1.9244e-02,  4.1885e-02],\n",
       "                         [-4.1562e-02, -3.4435e-02,  2.2556e-02],\n",
       "                         [-4.5936e-02, -7.3494e-02, -7.8129e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.0337e-03,  5.5800e-02,  1.6604e-02],\n",
       "                         [ 9.6532e-02,  1.1596e-01,  7.3521e-02],\n",
       "                         [ 6.1501e-02,  3.1694e-02,  2.7376e-02]],\n",
       "               \n",
       "                        [[ 1.0853e-01, -1.0980e-02, -9.3508e-02],\n",
       "                         [ 5.1080e-02, -6.0887e-02, -6.5291e-02],\n",
       "                         [-2.1888e-03, -7.4682e-02, -1.0373e-01]],\n",
       "               \n",
       "                        [[-8.4399e-02, -3.7601e-02, -1.1353e-02],\n",
       "                         [-2.7275e-02, -1.1043e-02, -3.4709e-03],\n",
       "                         [-2.1820e-02, -9.0111e-02, -5.2062e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.5137e-02, -1.5004e-02,  6.4871e-02],\n",
       "                         [-9.4157e-02, -5.4899e-02, -1.5414e-02],\n",
       "                         [-7.0286e-02, -8.9352e-03, -4.0003e-02]],\n",
       "               \n",
       "                        [[ 1.2048e-01,  3.6655e-02, -5.9407e-02],\n",
       "                         [ 5.5280e-02,  5.5961e-03, -3.4234e-02],\n",
       "                         [-1.0643e-02, -4.9575e-02, -9.5570e-02]],\n",
       "               \n",
       "                        [[-5.2828e-02, -7.0339e-02, -6.3884e-04],\n",
       "                         [-3.6540e-02, -7.0598e-02, -5.4258e-03],\n",
       "                         [ 5.6506e-02,  9.3739e-03,  7.6255e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.0303e-02,  3.5309e-02,  2.3180e-01],\n",
       "                         [-1.3529e-02,  3.9001e-02,  1.3837e-01],\n",
       "                         [-5.0928e-02,  1.2677e-02,  1.1682e-01]],\n",
       "               \n",
       "                        [[-4.4919e-02, -3.0801e-02,  6.0669e-02],\n",
       "                         [-6.5710e-02, -8.5855e-03,  1.8723e-02],\n",
       "                         [-1.9126e-02, -5.0326e-02,  4.3639e-02]],\n",
       "               \n",
       "                        [[-6.3415e-02, -6.9907e-02, -8.5296e-02],\n",
       "                         [ 3.6581e-02,  3.7016e-02,  1.9493e-02],\n",
       "                         [ 2.4748e-02,  2.4084e-02, -5.7905e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.4272e-02, -8.4149e-02, -4.8802e-02],\n",
       "                         [-1.0136e-01,  1.6818e-02,  2.6242e-02],\n",
       "                         [-1.4491e-01,  4.2067e-02, -2.0762e-02]],\n",
       "               \n",
       "                        [[-1.5916e-02, -7.0833e-02, -4.1228e-02],\n",
       "                         [ 1.1223e-02, -9.2079e-02, -3.3461e-02],\n",
       "                         [ 5.1563e-02,  3.5343e-02, -6.6008e-02]],\n",
       "               \n",
       "                        [[-4.2727e-03,  2.0576e-02, -3.7406e-02],\n",
       "                         [-2.3832e-03,  2.2307e-02, -7.5418e-02],\n",
       "                         [-6.0222e-02, -5.7618e-02, -7.7291e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.4074e-02, -3.8197e-02,  7.6361e-02],\n",
       "                         [ 2.1119e-02, -5.6175e-02,  5.5374e-02],\n",
       "                         [ 4.9527e-02,  3.2575e-02, -4.6306e-02]],\n",
       "               \n",
       "                        [[ 2.4399e-02,  4.2862e-02, -4.4833e-02],\n",
       "                         [ 5.4366e-02,  2.1043e-02, -6.7567e-02],\n",
       "                         [ 5.4615e-02,  2.0211e-02, -2.5135e-02]],\n",
       "               \n",
       "                        [[ 6.1288e-03,  5.4554e-02,  4.9614e-02],\n",
       "                         [-1.7275e-02, -8.2860e-03, -3.0524e-02],\n",
       "                         [ 4.8949e-02, -2.9642e-02, -8.9584e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-9.3923e-02, -3.2440e-02,  2.5420e-02],\n",
       "                         [-8.8360e-02, -6.1938e-03,  8.9252e-03],\n",
       "                         [-4.7224e-02, -1.5947e-02, -8.4049e-02]],\n",
       "               \n",
       "                        [[ 1.8525e-02,  3.3038e-02, -1.0091e-02],\n",
       "                         [ 9.7640e-02,  2.9119e-02, -3.8354e-02],\n",
       "                         [ 1.3483e-01,  7.2798e-02,  7.0084e-02]],\n",
       "               \n",
       "                        [[-2.0162e-02,  1.1032e-02,  9.9612e-02],\n",
       "                         [-7.9979e-02, -7.9945e-02, -1.3693e-02],\n",
       "                         [-5.8210e-02, -9.8925e-02, -4.6971e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.1370e-03, -1.1010e-02,  3.1087e-02],\n",
       "                         [-1.7180e-02, -3.9733e-02, -7.8851e-02],\n",
       "                         [-6.8712e-02, -1.2015e-01, -9.3198e-02]],\n",
       "               \n",
       "                        [[ 1.0463e-01,  1.3478e-01,  1.2678e-01],\n",
       "                         [-2.1363e-02,  5.9374e-02, -8.4611e-03],\n",
       "                         [-6.2503e-02, -6.6805e-02, -9.4305e-02]],\n",
       "               \n",
       "                        [[ 6.5120e-02, -1.7570e-02, -1.0832e-02],\n",
       "                         [ 9.2128e-02,  2.7579e-02,  1.2931e-02],\n",
       "                         [ 1.6286e-02, -2.4532e-02, -3.5434e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0922e-02,  1.8134e-02,  4.2176e-02],\n",
       "                         [-2.0092e-02, -9.4295e-03,  5.4325e-03],\n",
       "                         [ 3.1948e-03,  1.3265e-02, -7.2129e-03]],\n",
       "               \n",
       "                        [[-5.9090e-02, -9.1712e-02, -2.1451e-02],\n",
       "                         [-6.0784e-02, -1.4078e-01, -3.8332e-02],\n",
       "                         [-1.5779e-02, -2.2616e-02,  5.9401e-03]],\n",
       "               \n",
       "                        [[-8.1204e-02, -7.7038e-02, -9.6568e-02],\n",
       "                         [-2.6237e-02, -4.9521e-02, -7.9959e-02],\n",
       "                         [-2.5973e-02, -1.8555e-02, -6.3057e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.9266e-02,  4.0524e-02, -3.0485e-02],\n",
       "                         [-1.0831e-01, -1.1167e-01, -6.4734e-02],\n",
       "                         [-1.1240e-01, -1.2972e-01, -1.5778e-01]],\n",
       "               \n",
       "                        [[-6.1923e-04,  3.3190e-02,  3.2161e-03],\n",
       "                         [ 1.4266e-03, -2.0553e-02, -2.6923e-02],\n",
       "                         [-9.1989e-02, -4.8260e-02, -3.1548e-02]],\n",
       "               \n",
       "                        [[-5.8679e-02, -5.4401e-02, -1.1736e-02],\n",
       "                         [ 4.7042e-02, -1.6447e-02,  1.1700e-02],\n",
       "                         [ 3.6274e-02, -1.9248e-03, -3.4840e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.2920e-02, -1.4643e-02, -2.0132e-02],\n",
       "                         [-1.0133e-02, -3.7037e-02, -4.4286e-02],\n",
       "                         [ 4.1362e-02,  1.2530e-02, -2.1155e-02]],\n",
       "               \n",
       "                        [[ 1.2688e-01,  9.4313e-02,  1.5433e-01],\n",
       "                         [ 1.0828e-01, -4.7670e-04,  4.7439e-02],\n",
       "                         [ 1.8724e-01,  2.0627e-03,  6.1665e-02]],\n",
       "               \n",
       "                        [[-5.2117e-02, -9.4144e-02, -4.4678e-02],\n",
       "                         [ 5.7710e-02, -4.8003e-02,  1.3487e-02],\n",
       "                         [ 1.7341e-02, -3.3854e-02, -1.9437e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8308e-03,  5.8241e-02,  6.7932e-02],\n",
       "                         [-4.4768e-02, -1.0959e-01,  2.2238e-02],\n",
       "                         [-3.2077e-02, -9.4716e-02, -3.6316e-02]],\n",
       "               \n",
       "                        [[ 2.3040e-02, -1.0335e-01, -7.7519e-02],\n",
       "                         [-3.3525e-02, -1.3824e-01, -1.1108e-01],\n",
       "                         [-1.3530e-01, -2.0288e-01, -1.7364e-01]],\n",
       "               \n",
       "                        [[ 3.4518e-02, -1.8217e-02,  4.1507e-02],\n",
       "                         [ 6.4769e-05, -1.1188e-02, -2.6051e-02],\n",
       "                         [-6.5270e-03, -5.6412e-02, -5.0611e-02]]]], device='cuda:0')),\n",
       "              ('r7.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r7.bn2.weight',\n",
       "               tensor([0.9717, 0.9130, 0.9761, 0.9367, 0.9528, 0.9095, 0.9061, 0.9757, 0.9483,\n",
       "                       0.9515, 0.9169, 0.9288, 0.9093, 0.9070, 0.8928, 0.9233, 0.9501, 0.9976,\n",
       "                       0.9721, 0.9395, 0.9294, 0.9790, 0.9789, 0.9339, 0.9174, 0.9256, 0.9844,\n",
       "                       0.9181, 0.9375, 0.9140, 0.9328, 0.9124, 0.9567, 0.9622, 0.9021, 0.9040,\n",
       "                       1.0062, 0.9341, 0.9791, 0.9449, 0.9149, 0.9460, 0.8485, 0.9384, 0.9342,\n",
       "                       0.9271, 1.0224, 0.9436, 0.9160, 0.9880, 0.8996, 0.9578, 1.0007, 0.9618,\n",
       "                       0.9204, 0.9474, 0.9586, 0.9113, 0.8770, 0.9408, 0.9150, 0.9366, 0.8999,\n",
       "                       0.9282, 0.9531, 0.9857, 0.9198, 0.9317, 0.9547, 0.9514, 0.8555, 0.9159,\n",
       "                       0.9389, 0.8963, 0.9330, 0.9456, 0.9514, 0.9412, 0.9360, 0.8843, 0.9614,\n",
       "                       0.9406, 0.9381, 0.9442, 0.9591, 0.8721, 0.8773, 0.8867, 0.8915, 0.9296,\n",
       "                       0.9211, 0.9513, 0.9762, 0.8973, 0.9495, 0.8955, 0.9147, 0.9526, 0.9696,\n",
       "                       0.9605, 0.9217, 0.9120, 0.9509, 0.8672, 0.9002, 0.9492, 0.9802, 0.9692,\n",
       "                       0.9599, 0.9312, 0.8743, 0.9593, 0.9526, 0.9361, 0.9184, 0.9742, 0.9331,\n",
       "                       0.9095, 0.9354, 0.9449, 0.9151, 0.9928, 0.8922, 0.9389, 0.9425, 0.8959,\n",
       "                       0.9543, 0.8881, 0.9569, 0.9517, 0.9381, 0.9602, 0.9097, 0.9273, 0.9457,\n",
       "                       0.9263, 0.8915, 1.0120, 0.9509, 0.9543, 0.8988, 0.9272, 0.9425, 0.9478,\n",
       "                       0.9375, 0.9736, 0.9337, 0.9273, 0.9222, 0.9593, 0.9167, 0.9428, 0.9603,\n",
       "                       0.9593, 0.9503, 0.9416, 0.9364, 0.9076, 0.9092, 0.9726, 0.9701, 0.9283,\n",
       "                       0.9302, 0.9859, 0.9366, 0.9163, 0.9328, 0.9705, 0.9363, 0.8798, 0.9065,\n",
       "                       0.8949, 0.9552, 0.9187, 0.9396, 0.9427, 0.9066, 0.8894, 0.9124, 0.9541,\n",
       "                       0.9365, 0.9549, 0.9193, 0.9482, 1.0007, 0.9131, 0.9122, 0.9403, 0.9364,\n",
       "                       0.9299, 0.9096, 0.9453, 0.9931, 1.0026, 0.9631, 0.9256, 0.9614, 0.8622,\n",
       "                       0.9937, 0.9379, 0.8760, 0.9528, 0.9325, 0.9593, 0.9481, 0.9635, 0.9204,\n",
       "                       0.9735, 0.9378, 0.9313, 0.9098, 0.8943, 0.9261, 0.9534, 0.8963, 0.9279,\n",
       "                       0.9259, 0.9119, 0.9625, 0.8862, 0.9457, 0.9847, 0.9074, 0.9518, 0.9040,\n",
       "                       0.9097, 0.9316, 0.9594, 0.9381, 0.9167, 0.9310, 0.9273, 0.9573, 0.9313,\n",
       "                       0.9779, 0.8948, 0.8707, 0.9508, 0.9755, 0.9307, 0.9594, 0.9517, 0.9197,\n",
       "                       0.8944, 0.9230, 0.9871, 0.9359, 0.8879, 0.8728, 0.9638, 0.9417, 0.9332,\n",
       "                       0.9369, 0.9212, 0.9912, 1.0158, 0.9178, 0.9519, 0.8895, 0.9025, 0.8955,\n",
       "                       0.9776, 0.9210, 0.9283, 0.9371, 0.9554, 0.8615, 0.9329, 0.9295, 0.9466,\n",
       "                       0.9233, 0.9820, 0.9474, 0.9465, 0.9452, 0.9522, 0.9581, 0.9510, 0.9262,\n",
       "                       0.9338, 0.9654, 0.9227, 0.8969, 0.9142, 0.9365, 0.9174, 0.9192, 0.9511,\n",
       "                       0.9104, 0.9240, 0.9579, 0.8990, 0.9516, 0.9217, 0.9492, 0.9377, 0.9340,\n",
       "                       0.9495, 0.9494, 0.8661, 0.9164, 0.8869, 0.9345, 0.9270, 0.9168, 0.9699,\n",
       "                       0.9004, 0.9148, 0.9416, 0.9282, 0.8964, 0.9843, 0.8993, 0.8943, 0.9606,\n",
       "                       0.9340, 0.8720, 0.8951, 0.9022, 0.9621, 0.9262, 0.9815, 0.8459, 0.8947,\n",
       "                       0.9502, 0.9215, 0.9025, 0.9407, 0.9380, 0.9305, 0.9482, 0.9281, 0.9803,\n",
       "                       0.9070, 0.8970, 1.0131, 0.9189, 0.9406, 0.8944, 0.9487, 0.9238, 0.9533,\n",
       "                       0.9177, 0.9527, 0.9720, 0.9687, 0.9492, 0.8937, 0.9397, 0.9556, 0.9379,\n",
       "                       0.9535, 0.9418, 0.9771, 0.9770, 0.9729, 0.9401, 0.9142, 0.9533, 0.9921,\n",
       "                       0.9692, 0.9318, 0.8930, 0.9110, 0.9698, 0.9929, 0.9258, 0.9503, 0.9333,\n",
       "                       0.9569, 0.9421, 0.8718, 0.9308, 0.9013, 0.9390, 0.9155, 0.9705, 0.9698,\n",
       "                       0.9605, 1.0101, 0.9895, 0.9124, 0.9441, 0.9591, 0.9851, 0.9248, 0.8919,\n",
       "                       0.9985, 0.9875, 0.8964, 0.9284, 0.9595, 0.9522, 0.9596, 0.9454, 0.8886,\n",
       "                       0.9322, 0.9618, 0.9428, 0.9285, 0.9164, 0.9302, 0.9497, 0.8961, 0.9571,\n",
       "                       0.9081, 0.9086, 0.9419, 0.9492, 0.9078, 0.9386, 0.9324, 0.8777, 0.9209,\n",
       "                       0.9201, 0.9043, 0.9361, 0.9325, 0.9406, 0.9903, 0.9285, 0.9315, 0.9360,\n",
       "                       0.9090, 0.9498, 0.8913, 0.9469, 0.9294, 0.9345, 0.9506, 0.9573, 0.9364,\n",
       "                       0.9222, 0.9116, 0.9699, 0.9210, 1.0176, 0.9635, 0.9161, 0.9279, 0.9767,\n",
       "                       0.9085, 0.9282, 0.9453, 0.9743, 0.8821, 0.8647, 0.9680, 0.9341, 0.9421,\n",
       "                       0.9377, 0.9441, 0.9332, 0.8973, 0.9200, 0.9798, 0.9372, 0.9335, 0.8897,\n",
       "                       0.9460, 0.9729, 0.9526, 0.9108, 0.9070, 0.9405, 0.9071, 0.9474, 0.8975,\n",
       "                       0.9063, 0.9107, 0.9229, 0.9313, 0.9158, 0.9523, 0.9794, 0.9647, 0.9425,\n",
       "                       0.9784, 0.9275, 0.9629, 0.9265, 0.9247, 0.9552, 0.9359, 0.9530, 0.9556,\n",
       "                       0.9505, 0.9074, 0.9617, 0.9469, 0.9797, 0.8951, 0.9757, 0.9490, 0.9524,\n",
       "                       0.9317, 0.9155, 0.9586, 0.9607, 0.9014, 0.9011, 0.9451, 0.8918, 0.9695,\n",
       "                       0.9358, 0.9359, 0.9274, 0.8925, 0.9379, 0.9447, 0.9462, 0.9222],\n",
       "                      device='cuda:0')),\n",
       "              ('r7.bn2.bias',\n",
       "               tensor([-0.2224, -0.1239, -0.2438, -0.1778, -0.2066, -0.1833, -0.1739, -0.2395,\n",
       "                       -0.1871, -0.2672, -0.1557, -0.2068, -0.1092, -0.1576, -0.1421, -0.1455,\n",
       "                       -0.1764, -0.2455, -0.2202, -0.1855, -0.1991, -0.1854, -0.1828, -0.1609,\n",
       "                       -0.2075, -0.1216, -0.2040, -0.1988, -0.1900, -0.1616, -0.2370, -0.1257,\n",
       "                       -0.1955, -0.2496, -0.1723, -0.1796, -0.2659, -0.1647, -0.2265, -0.1797,\n",
       "                       -0.1677, -0.2631, -0.1056, -0.1600, -0.2459, -0.2089, -0.2844, -0.1923,\n",
       "                       -0.1502, -0.2334, -0.1086, -0.2028, -0.3157, -0.1821, -0.1336, -0.2115,\n",
       "                       -0.2080, -0.1498, -0.0916, -0.2427, -0.1980, -0.1952, -0.1363, -0.1467,\n",
       "                       -0.2402, -0.2556, -0.1724, -0.1658, -0.2496, -0.2307, -0.1148, -0.2399,\n",
       "                       -0.2453, -0.1410, -0.1788, -0.1735, -0.1653, -0.1835, -0.1619, -0.1343,\n",
       "                       -0.2039, -0.1796, -0.1915, -0.1864, -0.2034, -0.0910, -0.1084, -0.1974,\n",
       "                       -0.1817, -0.2357, -0.1360, -0.2639, -0.2542, -0.0046, -0.1805, -0.1893,\n",
       "                       -0.0927, -0.2693, -0.2757, -0.2241, -0.1685, -0.1754, -0.1758, -0.0723,\n",
       "                       -0.1780, -0.1871, -0.2821, -0.2630, -0.2096, -0.2058, -0.0710, -0.2042,\n",
       "                       -0.2190, -0.1931, -0.1546, -0.2478, -0.1527, -0.1081, -0.1881, -0.2338,\n",
       "                       -0.1546, -0.2241, -0.1201, -0.1822, -0.2417, -0.1597, -0.2459, -0.1038,\n",
       "                       -0.2084, -0.1983, -0.1553, -0.2111, -0.1049, -0.1829, -0.2099, -0.2211,\n",
       "                       -0.1982, -0.2909, -0.1758, -0.2417, -0.1321, -0.1469, -0.1612, -0.2189,\n",
       "                       -0.1809, -0.2044, -0.2468, -0.1579, -0.1554, -0.2136, -0.1935, -0.1618,\n",
       "                       -0.2328, -0.1862, -0.1619, -0.1935, -0.1820, -0.2033, -0.1057, -0.2408,\n",
       "                       -0.2528, -0.1872, -0.1734, -0.2038, -0.1901, -0.2217, -0.2286, -0.2089,\n",
       "                       -0.2229, -0.0442, -0.1673, -0.1603, -0.1673, -0.1920, -0.1823, -0.1690,\n",
       "                       -0.1862, -0.1388, -0.1336, -0.1901, -0.1937, -0.1948, -0.1666, -0.2008,\n",
       "                       -0.2707, -0.1381, -0.1652, -0.2294, -0.1603, -0.2036, -0.1561, -0.1940,\n",
       "                       -0.2426, -0.2723, -0.2240, -0.2037, -0.1883, -0.0909, -0.2448, -0.2042,\n",
       "                       -0.1018, -0.2138, -0.1578, -0.2270, -0.1983, -0.2047, -0.1990, -0.2489,\n",
       "                       -0.1866, -0.2071, -0.1448, -0.1172, -0.1737, -0.1853, -0.1500, -0.2189,\n",
       "                       -0.2398, -0.1658, -0.2547, -0.1100, -0.2191, -0.2603, -0.1648, -0.1440,\n",
       "                       -0.1811, -0.1577, -0.1521, -0.2139, -0.2140, -0.1461, -0.1960, -0.1406,\n",
       "                       -0.2097, -0.1845, -0.2221, -0.1793, -0.0602, -0.2037, -0.2230, -0.2520,\n",
       "                       -0.1849, -0.2262, -0.2075, -0.1259, -0.2276, -0.2429, -0.1935, -0.1217,\n",
       "                       -0.1479, -0.2247, -0.1935, -0.1849, -0.1898, -0.1271, -0.2105, -0.3267,\n",
       "                       -0.1600, -0.2061, -0.0986, -0.2221, -0.1618, -0.2517, -0.1447, -0.1538,\n",
       "                       -0.1654, -0.2120, -0.1419, -0.1648, -0.2300, -0.2589, -0.1561, -0.2490,\n",
       "                       -0.2339, -0.1055, -0.1943, -0.2582, -0.1309, -0.1614, -0.1912, -0.1643,\n",
       "                       -0.2357, -0.1716, -0.1047, -0.1922, -0.1678, -0.1306, -0.1810, -0.2485,\n",
       "                       -0.1069, -0.1933, -0.1971, -0.0576, -0.1828, -0.1805, -0.1661, -0.2710,\n",
       "                       -0.1281, -0.2450, -0.1763, -0.0430, -0.1593, -0.1801, -0.1744, -0.1618,\n",
       "                       -0.1290, -0.2792, -0.1802, -0.1947, -0.2145, -0.1985, -0.1143, -0.2545,\n",
       "                       -0.1795, -0.1685, -0.2811, -0.1582, -0.0915, -0.0945, -0.1658, -0.2258,\n",
       "                       -0.1312, -0.2149, -0.0312, -0.1570, -0.2154, -0.1927, -0.1616, -0.2036,\n",
       "                       -0.2137, -0.1938, -0.1979, -0.2331, -0.2017, -0.1740, -0.1698, -0.2684,\n",
       "                       -0.1883, -0.2857, -0.1468, -0.1938, -0.2061, -0.2120, -0.1585, -0.1873,\n",
       "                       -0.2568, -0.2068, -0.2127, -0.1787, -0.1837, -0.2498, -0.2523, -0.2261,\n",
       "                       -0.2394, -0.2177, -0.2621, -0.2681, -0.1572, -0.1452, -0.1827, -0.2274,\n",
       "                       -0.2030, -0.2307, -0.1586, -0.1643, -0.2318, -0.2459, -0.1677, -0.2208,\n",
       "                       -0.2062, -0.2689, -0.2228, -0.0873, -0.1354, -0.1777, -0.1959, -0.1964,\n",
       "                       -0.2525, -0.1938, -0.2370, -0.4046, -0.2435, -0.1530, -0.2167, -0.2294,\n",
       "                       -0.2371, -0.2196, -0.1806, -0.2591, -0.2466, -0.1632, -0.1669, -0.2199,\n",
       "                       -0.2242, -0.1848, -0.1966,  0.0108, -0.1811, -0.2347, -0.2083, -0.1519,\n",
       "                       -0.2201, -0.1784, -0.2093, -0.1015, -0.2349,  0.0140, -0.1762, -0.1700,\n",
       "                       -0.2358, -0.1650, -0.2172, -0.1960, -0.0900, -0.1845, -0.2208, -0.1630,\n",
       "                       -0.2049, -0.1927, -0.2102, -0.2750, -0.1969, -0.1402, -0.2040, -0.2354,\n",
       "                       -0.2414, -0.0819, -0.2610, -0.2067, -0.1731, -0.2046, -0.2137, -0.1419,\n",
       "                       -0.2005, -0.1737, -0.2625, -0.1893, -0.2744, -0.2510, -0.0120, -0.1715,\n",
       "                       -0.2964, -0.2111, -0.1316, -0.2192, -0.2516, -0.1706, -0.1714, -0.2277,\n",
       "                       -0.1837, -0.2475, -0.1586, -0.1566, -0.2340, -0.1263, -0.1658, -0.2299,\n",
       "                       -0.1991, -0.1921, -0.1516, -0.2155, -0.2518, -0.2208, -0.1178, -0.1170,\n",
       "                       -0.1683, -0.0757, -0.1977, -0.1428, -0.1789, -0.1154, -0.1228, -0.1583,\n",
       "                       -0.1374, -0.2396, -0.2940, -0.2199, -0.2443, -0.2290, -0.1598, -0.2530,\n",
       "                       -0.2104, -0.1785, -0.2151, -0.1380, -0.2303, -0.1606, -0.2023, -0.1789,\n",
       "                       -0.1941, -0.1950, -0.2608, -0.1271, -0.2423, -0.1894, -0.2022, -0.1783,\n",
       "                       -0.2044, -0.2327, -0.2105, -0.1148, -0.1249, -0.2062, -0.1337, -0.2067,\n",
       "                       -0.1934, -0.2359, -0.1466, -0.1820, -0.2136, -0.1779, -0.2296, -0.1382],\n",
       "                      device='cuda:0')),\n",
       "              ('r7.bn2.running_mean',\n",
       "               tensor([ -6.5047,  -5.3834,  -4.3394,  -6.5598,  -5.2100,  -4.7342,  -5.8678,\n",
       "                        -4.8611,  -6.9134,  -5.4257,  -6.0923,  -5.6531,  -5.8916,  -5.4888,\n",
       "                        -7.2336, -11.0133,  -6.2247,  -5.2554,  -5.6392,  -5.6853,  -5.5891,\n",
       "                        -6.3670,  -6.6189,  -6.6730,  -5.0708,  -5.7858,  -4.6248,  -3.6878,\n",
       "                        -5.1400,  -3.7973,  -2.6318,  -6.3224,  -4.6777,  -4.9045,  -4.0672,\n",
       "                        -4.5940,  -5.2080,  -5.5671,  -5.1543,  -4.3557,  -5.1654,  -5.6228,\n",
       "                        -6.8858,  -5.4711,  -3.1207,  -5.4110,  -5.1113,  -4.3251,  -4.0562,\n",
       "                        -4.2030,  -6.8157,  -6.4872,  -4.8298,  -6.1979,  -6.7515,  -6.4640,\n",
       "                        -5.9101,  -4.2392,  -6.6921,  -3.2457,  -4.5188,  -5.8596,  -5.7779,\n",
       "                        -4.8517,  -4.4069,  -4.4827,  -6.6843,  -4.6093,  -5.2582,  -6.3327,\n",
       "                        -4.4614,  -3.5767,  -3.5141,  -5.4910,  -6.0113,  -5.4302,  -6.7712,\n",
       "                        -5.9046,  -6.0526,  -6.7573,  -6.5156,  -7.0306,  -3.8924,  -4.9783,\n",
       "                        -5.6494,  -8.2273,  -6.3705,  -3.8528,  -6.7939,  -5.7944,  -7.4988,\n",
       "                        -4.3214,  -5.0384,  -7.9522,  -5.9524,  -6.5588,  -5.6547,  -4.3446,\n",
       "                        -4.5437,  -4.3129,  -7.2512,  -4.9288,  -4.6078,  -9.0274,  -4.7917,\n",
       "                        -4.6527,  -4.1107,  -5.6324,  -6.0442,  -5.4890,  -8.7349,  -6.3792,\n",
       "                        -5.3910,  -4.8181,  -5.0659,  -5.1969,  -5.4231,  -6.8621,  -6.4755,\n",
       "                        -4.3759,  -8.1341,  -5.4974,  -4.8706,  -6.8687,  -2.7170,  -7.5019,\n",
       "                        -5.7387,  -5.5710,  -5.6425,  -6.3572,  -6.6108,  -3.4785,  -7.0061,\n",
       "                        -6.1478,  -4.4723,  -5.2310,  -3.3850,  -4.4658,  -6.1584,  -3.6668,\n",
       "                        -4.9159,  -5.7630,  -3.7758,  -4.4607,  -7.1705,  -5.3137,  -1.9640,\n",
       "                        -5.0400,  -7.7769,  -5.5959,  -4.4499,  -4.5548,  -5.6868,  -4.7184,\n",
       "                        -7.4148,  -5.4919,  -5.8784,  -4.2865,  -7.5885,  -6.2025,  -4.3891,\n",
       "                        -5.9398,  -6.3016,  -6.7051,  -4.9676,  -4.5565,  -4.2193,  -4.0875,\n",
       "                        -4.9562, -10.5509,  -6.2565,  -5.1978,  -4.5883,  -5.8668,  -5.7523,\n",
       "                        -7.9719,  -4.9769,  -4.0320,  -5.9515,  -5.5241,  -5.5235,  -5.7023,\n",
       "                        -7.7586,  -6.1237,  -7.1895,  -7.1133,  -4.8818,  -5.3510,  -4.0751,\n",
       "                        -4.6812,  -5.3479,  -6.5545,  -5.2774,  -3.2030,  -4.9575,  -4.1289,\n",
       "                        -5.5427,  -5.0537,  -4.0705,  -4.3577,  -7.4600,  -6.4358,  -4.3584,\n",
       "                        -6.1924,  -6.0973,  -6.4365,  -5.7339,  -5.3836,  -4.8447,  -4.4284,\n",
       "                        -8.0627,  -6.2774,  -6.3681,  -4.8612,  -6.0912,  -4.2241,  -2.6148,\n",
       "                        -4.5621,  -3.8960,  -4.1783,  -4.8949,  -4.4486,  -5.1529,  -5.4163,\n",
       "                        -4.2838,  -3.8416,  -5.7468,  -7.1703,  -5.8164,  -5.2562,  -4.3502,\n",
       "                        -4.7946,  -5.1316,  -5.1239,  -6.0842,  -2.7315,  -6.7839,  -4.4701,\n",
       "                        -5.2983,  -5.4330,  -6.0287,  -5.6194,  -4.8739,  -5.7558,  -2.7325,\n",
       "                        -6.1370,  -5.4193,  -5.3274,  -5.3243,  -5.2026,  -7.3834,  -5.7468,\n",
       "                        -5.5724,  -6.3800,  -6.3764,  -4.4240,  -5.0782,  -3.2261,  -6.5687,\n",
       "                        -5.7605,  -4.5061,  -4.8621,  -3.8889,  -7.8137,  -4.7405,  -3.4277,\n",
       "                        -6.4122,  -6.5722,  -4.4151,  -5.3028,  -5.9517,  -5.6009,  -4.1512,\n",
       "                        -5.4406,  -5.2223,  -4.2680,  -6.0924,  -5.5139,  -4.2208,  -6.7749,\n",
       "                        -3.5012,  -5.3162,  -6.3330,  -5.1085,  -3.6613,  -7.3893,  -5.0035,\n",
       "                        -4.5415,  -6.7272,  -5.1863,  -5.0219,  -6.1392,  -6.1785,  -3.8553,\n",
       "                        -6.1411,  -3.4860,  -7.5895,  -3.7453,  -6.7498, -10.8121,  -7.1370,\n",
       "                        -4.3553,  -6.2931,  -6.8552,  -6.8005,  -4.1375,  -5.7191,  -5.2267,\n",
       "                        -6.8369,  -5.4497,  -6.6373,  -4.4975,  -6.5733,  -5.1684,  -6.8870,\n",
       "                        -4.1639,  -5.9354,  -5.1476,  -4.8615,  -5.0132,  -7.0292,  -5.8057,\n",
       "                        -6.5346,  -4.8743,  -5.4411,  -4.5033,  -6.2015,  -5.3239,  -5.3508,\n",
       "                        -4.6878,  -5.4915,  -5.3017,  -7.7171,  -4.7259,  -6.1248,  -4.8290,\n",
       "                        -4.9877,  -3.2916,  -7.4348,  -6.7916,  -4.2830,  -8.2072,  -5.7486,\n",
       "                        -5.5649,  -4.4308,  -5.0755,  -4.0968,  -5.3765,  -6.8900,  -4.4096,\n",
       "                        -4.1414,  -3.7119,  -2.5380,  -5.8704,  -5.1822,  -5.0280,  -5.6833,\n",
       "                        -4.5232,  -6.2978,  -5.0268,  -6.4707,  -3.4034,  -7.0921,  -6.4168,\n",
       "                        -5.0574,  -5.5178,  -7.8075,  -5.0245,  -5.3783,  -5.8280,  -4.7745,\n",
       "                       -11.2845,  -5.7381,  -5.2625,  -4.4684,  -4.8977,  -3.6538,  -5.6235,\n",
       "                        -5.2454,  -4.9073,  -5.4216,  -6.2902,  -6.1170,  -3.6798,  -5.0594,\n",
       "                        -5.8710,  -5.3672,  -4.4072,  -6.3372,  -3.8372,  -7.2487,  -5.2506,\n",
       "                        -5.2242,  -8.7649,  -6.5776,  -8.0356,  -6.0745,  -5.0252,  -4.2622,\n",
       "                        -6.9052,  -3.9897,  -5.7526,  -4.6821,  -7.6072,  -5.0283,  -3.3043,\n",
       "                        -6.0062,  -7.2608,  -3.8473,  -4.5786,  -4.2035,  -5.2681,  -7.8668,\n",
       "                        -3.7895,  -3.2894,  -5.1383,  -4.4318,  -5.1715,  -5.3368,  -6.3376,\n",
       "                        -6.5227,  -8.0415,  -6.4518,  -2.4416,  -3.1904,  -7.8995,  -3.5487,\n",
       "                        -6.1593,  -5.0222,  -5.2764,  -4.4679,  -7.2325,  -4.2887,  -6.0764,\n",
       "                        -4.3721,  -5.2722,  -4.2828,  -5.1913,  -9.3870,  -5.4654,  -5.4826,\n",
       "                        -4.3900,  -5.7555,  -5.7400,  -6.1451,  -4.1676,  -1.7119,  -5.2383,\n",
       "                        -5.8755,  -5.4122,  -5.3194,  -5.5584,  -4.8731,  -5.9809,  -6.9666,\n",
       "                        -4.2296,  -5.5086,  -3.7508,  -5.0121,  -5.2834,  -5.0175,  -4.1746,\n",
       "                        -6.1148,  -8.2539,  -4.7631,  -7.1834,  -5.6116,  -5.8027,  -4.7840,\n",
       "                        -6.2066,  -5.3327,  -5.3652,  -6.3813,  -6.4332,  -4.7646,  -4.7224,\n",
       "                        -3.9427,  -5.5631,  -5.5365,  -4.0448,  -5.5796,  -5.5827,  -4.5320,\n",
       "                        -5.4088,  -5.2058,  -6.8274,  -5.2752,  -5.1964,  -4.7274,  -5.1554,\n",
       "                        -5.2818,  -3.9035,  -5.5289,  -4.9045,  -7.0037,  -6.3020,  -4.7466,\n",
       "                        -4.1892,  -6.2709,  -7.5810,  -5.4364,  -5.6911,  -4.8994,  -4.5880,\n",
       "                        -5.6634,  -3.6786,  -4.0788,  -4.4416,  -5.7206,  -6.4085,  -4.7961,\n",
       "                        -7.9665], device='cuda:0')),\n",
       "              ('r7.bn2.running_var',\n",
       "               tensor([168.6221, 264.5679, 141.3081, 371.3007, 229.9247, 379.8452, 287.9670,\n",
       "                       226.2842, 396.0829, 205.5713, 309.0811, 264.0377, 384.1656, 393.2864,\n",
       "                       433.2225, 574.9973, 253.8024, 227.9492, 232.2793, 306.3538, 239.3730,\n",
       "                       263.3057, 235.9476, 352.5978, 262.1911, 245.5336, 164.4242, 243.2130,\n",
       "                       264.9913, 311.2836, 237.7479, 370.7470, 118.1110, 205.0149, 370.3786,\n",
       "                       309.4901, 156.4851, 267.8537, 259.5399, 286.8047, 409.4624, 208.7965,\n",
       "                       583.0005, 258.1809, 156.4510, 202.4220, 162.3052, 200.9938, 228.5072,\n",
       "                       178.0635, 404.7402, 304.5700, 176.1961, 268.2959, 372.0104, 236.3240,\n",
       "                       243.7328, 268.6024, 489.9067, 302.3853, 284.3070, 252.4707, 360.0827,\n",
       "                       254.9634, 234.3224, 253.2047, 366.9907, 144.9627, 199.0143, 303.4310,\n",
       "                       330.8903, 174.2050, 157.3960, 333.2058, 259.9594, 403.7339, 337.2419,\n",
       "                       243.2007, 314.8638, 420.7770, 337.2362, 320.8234, 211.0108, 207.1896,\n",
       "                       283.4476, 642.7654, 417.4518, 246.5953, 358.8777, 348.1288, 383.6497,\n",
       "                       223.1295, 228.8129, 806.8472, 267.3159, 331.3150, 373.4694, 239.2951,\n",
       "                       163.7429, 206.9262, 384.7201, 345.9524, 207.3769, 667.8798, 252.8849,\n",
       "                       292.2008, 254.8923, 266.7055, 217.5758, 307.6970, 724.7117, 319.2935,\n",
       "                       256.4646, 295.3244, 297.1343, 237.7309, 245.3565, 453.9975, 224.4334,\n",
       "                       211.2525, 411.0059, 199.1595, 411.6153, 275.0124, 235.3905, 464.1568,\n",
       "                       286.5862, 411.8560, 303.1646, 401.7532, 317.4290, 143.1403, 455.3098,\n",
       "                       337.4842, 227.9260, 227.4897, 148.1073, 157.9457, 317.0810, 278.2835,\n",
       "                       278.4480, 305.1193, 249.6443, 255.2027, 428.2819, 233.0611, 211.1475,\n",
       "                       213.5551, 374.1666, 292.3389, 285.2602, 280.3265, 314.0663, 220.1708,\n",
       "                       309.2396, 215.0812, 274.6262, 275.8115, 427.7743, 226.0883, 195.5838,\n",
       "                       274.9754, 242.4191, 252.3536, 321.6797, 236.3169, 209.4301, 154.9258,\n",
       "                       240.5611, 741.7017, 323.8325, 399.3837, 221.7318, 275.9432, 261.0905,\n",
       "                       326.4223, 278.2821, 364.6087, 359.5796, 283.8686, 226.6613, 274.4948,\n",
       "                       342.5015, 222.0769, 200.2908, 453.0024, 209.0167, 288.8948, 244.2974,\n",
       "                       235.2605, 328.4401, 312.6219, 222.6275, 115.7027, 165.1826, 244.9159,\n",
       "                       225.0258, 432.4406, 136.2299, 231.2714, 481.3745, 312.7620, 285.0279,\n",
       "                       202.1879, 245.2841, 262.7035, 207.0667, 179.0704, 223.6680, 205.8623,\n",
       "                       382.0232, 447.4109, 291.9570, 253.2675, 361.2016, 196.4589, 228.1951,\n",
       "                       260.7521, 236.2957, 418.1790, 287.4590, 206.7079, 318.2905, 234.9266,\n",
       "                       299.8910, 210.4937, 266.8259, 320.0775, 285.4452, 354.4886, 222.4502,\n",
       "                       290.3402, 253.5852, 323.0187, 193.5987, 251.2897, 469.5293, 171.6998,\n",
       "                       170.9716, 238.5090, 243.0931, 242.3692, 225.6052, 299.2956, 251.8845,\n",
       "                       234.4119, 258.1647, 394.1734, 253.5820, 335.5515, 305.2123, 229.1465,\n",
       "                       271.2648, 360.0780, 299.7567, 183.1601, 306.0194, 222.5663, 363.7705,\n",
       "                       331.2721, 357.9032, 218.4336, 308.4577, 370.6278, 293.4466, 194.7718,\n",
       "                       379.8851, 432.9632, 184.1128, 247.3519, 314.3219, 228.8361, 223.9394,\n",
       "                       334.9771, 278.1184, 168.8851, 426.0204, 224.6774, 252.2103, 351.6812,\n",
       "                       225.5524, 246.7332, 356.3876, 284.1299, 247.7526, 381.7549, 258.3465,\n",
       "                       179.5202, 423.4344, 234.9687, 229.2702, 480.1457, 316.1066, 220.1086,\n",
       "                       318.7610, 222.5601, 397.7957, 340.5082, 221.6393, 874.0778, 298.1085,\n",
       "                       361.8124, 369.3906, 318.9074, 418.0448, 210.1878, 326.6104, 338.8434,\n",
       "                       235.8427, 268.1596, 329.3434, 189.5955, 335.5569, 278.9586, 228.0502,\n",
       "                       296.8517, 560.0018, 492.9255, 297.5792, 191.2421, 387.4753, 169.2707,\n",
       "                       633.8466, 283.7420, 200.0815, 361.8070, 337.7127, 277.3432, 221.2744,\n",
       "                       214.2334, 217.8718, 177.5478, 328.8835, 317.0807, 317.0611, 115.2500,\n",
       "                       308.0797, 274.9991, 424.1671, 311.8441, 291.4143, 352.5865, 254.3372,\n",
       "                       278.8213, 190.0216, 185.1007, 191.9033, 351.5529, 273.0168, 232.0298,\n",
       "                       173.9642, 241.1122, 141.8535, 284.5619, 257.4565, 139.8493, 267.7987,\n",
       "                       308.6071, 310.5000, 241.2957, 271.5746, 167.9140, 411.4569, 229.8098,\n",
       "                       235.6196, 201.3409, 414.9662, 322.5150, 302.2161, 213.0224, 214.8642,\n",
       "                       582.4824, 310.2390, 289.6277, 214.6223, 288.7250, 187.5367, 197.9046,\n",
       "                       276.2183, 128.5167, 224.8110, 370.4543, 360.4084, 312.5517, 200.9621,\n",
       "                       247.8891, 246.4480, 150.3114, 243.9641, 269.1894, 300.7723, 262.4070,\n",
       "                       277.0412, 431.8910, 328.1110, 515.8649, 391.5545, 154.0371, 220.8134,\n",
       "                       340.4599, 319.3420, 312.7624, 234.7862, 519.2866, 219.5111, 549.9898,\n",
       "                       225.9807, 321.6734, 180.3040, 230.8898, 177.3593, 222.3982, 562.4677,\n",
       "                       305.0702, 202.1630, 334.8236, 208.2120, 164.4494, 245.1721, 240.6589,\n",
       "                       289.8251, 317.5945, 251.4394, 256.4246, 201.6662, 437.5628, 227.1352,\n",
       "                       263.6592, 260.2391, 220.7757, 218.7709, 386.1066, 290.2577, 304.8949,\n",
       "                       235.7647, 328.7645, 141.0205, 258.0273, 823.6898, 304.5752, 159.7863,\n",
       "                       307.3605, 339.0606, 255.1508, 189.3614, 267.6726, 227.8804, 212.0964,\n",
       "                       254.6427, 226.0631, 288.2821, 360.7894, 216.4263, 423.8368, 304.3871,\n",
       "                       156.9469, 238.9358, 352.8258, 278.2168, 283.0146, 241.3012, 176.8386,\n",
       "                       360.3269, 464.2532, 270.5529, 512.0709, 237.1966, 319.4131, 377.2726,\n",
       "                       392.0793, 386.3033, 334.6810, 431.2563, 222.0363, 180.6846, 189.6009,\n",
       "                       217.7008, 213.0090, 298.7470, 196.8504, 234.7263, 297.3290, 259.2330,\n",
       "                       245.4402, 165.4330, 239.0365, 248.5493, 300.4099, 244.6025, 308.4599,\n",
       "                       185.6154, 301.7815, 241.5254, 294.4604, 305.9915, 308.1420, 307.4357,\n",
       "                       194.1235, 276.2154, 381.5280, 366.6428, 256.8336, 256.6314, 217.8416,\n",
       "                       293.1455, 197.7210, 225.2304, 246.8779, 236.2649, 238.1003, 248.5903,\n",
       "                       462.3088], device='cuda:0')),\n",
       "              ('r7.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r7.c2.weights',\n",
       "               tensor([[[[ 3.8053e-03,  1.6554e-03,  4.4901e-02],\n",
       "                         [-2.6896e-02, -1.9862e-02, -1.2452e-02],\n",
       "                         [-3.0949e-02,  1.2945e-02,  2.9636e-02]],\n",
       "               \n",
       "                        [[ 2.9659e-02,  2.0750e-02,  6.3779e-02],\n",
       "                         [-1.9111e-02, -9.1820e-03,  2.0651e-02],\n",
       "                         [ 1.8142e-02,  4.8448e-02, -2.6967e-02]],\n",
       "               \n",
       "                        [[ 2.0463e-02,  8.6551e-03,  1.3813e-03],\n",
       "                         [-3.1319e-02, -3.1702e-02, -3.5465e-03],\n",
       "                         [-2.2001e-02,  1.1508e-02, -6.3405e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.8288e-02, -1.3025e-02, -1.7892e-02],\n",
       "                         [ 1.4752e-02,  2.0207e-02,  2.1693e-02],\n",
       "                         [-3.0725e-02,  5.3688e-03,  6.8819e-03]],\n",
       "               \n",
       "                        [[ 5.0383e-02,  2.8919e-02,  3.6616e-03],\n",
       "                         [-9.6453e-04,  1.5946e-03, -4.8650e-03],\n",
       "                         [ 8.9127e-03, -5.2936e-03, -7.4905e-03]],\n",
       "               \n",
       "                        [[ 1.3845e-02, -1.7706e-02, -6.3950e-03],\n",
       "                         [-1.9436e-02, -2.4428e-02, -3.5888e-02],\n",
       "                         [ 4.1508e-02,  1.7599e-02, -1.1267e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.7804e-02, -6.9778e-03, -5.0044e-03],\n",
       "                         [-1.1698e-02,  3.6636e-02,  3.6897e-02],\n",
       "                         [-2.5772e-02, -2.4606e-02, -1.1064e-01]],\n",
       "               \n",
       "                        [[ 2.1963e-02,  3.5979e-03,  4.2920e-02],\n",
       "                         [ 5.6034e-02, -2.2948e-02, -2.9511e-02],\n",
       "                         [ 5.7282e-03, -4.5891e-02,  5.2695e-02]],\n",
       "               \n",
       "                        [[-3.7940e-02, -7.6223e-02, -1.0232e-02],\n",
       "                         [-6.2694e-02, -3.5814e-02, -4.0669e-03],\n",
       "                         [-5.4011e-02, -9.2845e-02, -1.7761e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8312e-02, -3.8796e-02,  1.5558e-02],\n",
       "                         [ 1.3144e-01,  9.1814e-02,  8.2567e-02],\n",
       "                         [ 4.6903e-02,  7.5072e-02,  2.7351e-03]],\n",
       "               \n",
       "                        [[ 9.1378e-02,  1.8612e-02,  3.7100e-02],\n",
       "                         [ 4.9387e-02,  1.9384e-02,  4.4777e-02],\n",
       "                         [-4.4131e-03, -3.3825e-02,  4.9408e-02]],\n",
       "               \n",
       "                        [[-5.0799e-02, -3.0842e-02, -9.8068e-02],\n",
       "                         [-7.1560e-02, -6.2868e-02, -3.3765e-02],\n",
       "                         [-1.6530e-01, -9.0773e-02, -9.5682e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.1427e-02, -5.0865e-02,  1.1352e-02],\n",
       "                         [ 4.3085e-02, -1.1213e-02,  1.1144e-02],\n",
       "                         [ 4.2374e-02, -4.9915e-02, -2.7857e-02]],\n",
       "               \n",
       "                        [[-7.6005e-02,  4.0246e-02,  1.5071e-02],\n",
       "                         [-5.4417e-02,  4.4567e-02,  7.9579e-02],\n",
       "                         [-8.4594e-02, -5.0427e-02,  5.4306e-02]],\n",
       "               \n",
       "                        [[ 7.2367e-04,  1.9415e-02,  2.3624e-03],\n",
       "                         [ 2.8801e-02,  8.8969e-03,  8.3538e-02],\n",
       "                         [ 3.0184e-03,  2.0384e-02,  6.2398e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.7567e-02, -1.1458e-02,  5.9910e-02],\n",
       "                         [-6.0820e-02,  2.5109e-02,  5.7763e-02],\n",
       "                         [-8.3260e-04, -2.0347e-02, -3.0262e-02]],\n",
       "               \n",
       "                        [[ 7.3512e-02,  1.2184e-02,  4.3760e-02],\n",
       "                         [ 5.3103e-02,  8.2844e-05, -1.6664e-02],\n",
       "                         [ 7.7920e-02,  1.6523e-02,  4.8753e-02]],\n",
       "               \n",
       "                        [[-5.4237e-02, -3.7838e-02, -6.9192e-02],\n",
       "                         [-1.4437e-02, -1.2339e-02, -3.7380e-02],\n",
       "                         [ 4.5727e-02, -3.4036e-03,  2.6346e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.7736e-02,  5.9159e-02,  6.8141e-02],\n",
       "                         [ 3.1876e-02, -5.9009e-02, -3.6765e-03],\n",
       "                         [ 1.4239e-02,  6.7770e-03,  2.9970e-02]],\n",
       "               \n",
       "                        [[ 2.7011e-02, -1.9482e-02, -3.8198e-02],\n",
       "                         [ 1.2431e-02, -1.2119e-02,  1.6890e-02],\n",
       "                         [-1.0932e-02, -1.8951e-02, -5.6456e-02]],\n",
       "               \n",
       "                        [[ 2.9768e-03,  4.6748e-02, -3.0941e-02],\n",
       "                         [ 6.9197e-03,  5.5370e-02,  2.9919e-02],\n",
       "                         [-2.3143e-02,  5.9726e-03, -2.0799e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.4802e-02, -3.8878e-02, -6.8408e-02],\n",
       "                         [-9.7801e-03, -6.1420e-02, -3.2363e-02],\n",
       "                         [-1.2606e-02, -4.3031e-02, -6.7148e-02]],\n",
       "               \n",
       "                        [[ 4.9537e-02,  7.2559e-03,  1.7694e-03],\n",
       "                         [-2.5142e-03, -1.7817e-03, -3.3704e-02],\n",
       "                         [-1.8152e-02, -7.2364e-03, -4.6322e-02]],\n",
       "               \n",
       "                        [[ 4.1854e-02,  8.7019e-02,  3.2256e-02],\n",
       "                         [ 8.8337e-03,  5.1045e-02,  3.7495e-02],\n",
       "                         [-3.5673e-02, -3.8223e-02, -3.0805e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.8464e-04,  4.0813e-02, -2.6661e-02],\n",
       "                         [ 9.0969e-03, -2.2843e-02, -1.7619e-02],\n",
       "                         [-3.4718e-02, -1.6088e-02,  2.1186e-03]],\n",
       "               \n",
       "                        [[ 4.3775e-02,  1.0952e-02,  4.7873e-02],\n",
       "                         [-1.2167e-03,  2.0596e-02,  8.8909e-03],\n",
       "                         [ 2.6298e-02,  5.6440e-02,  6.5376e-02]],\n",
       "               \n",
       "                        [[ 5.9827e-03, -4.2166e-03, -2.9723e-02],\n",
       "                         [-1.4859e-02, -1.6637e-02,  2.2869e-02],\n",
       "                         [ 2.8593e-02,  1.3225e-02,  3.7053e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.5643e-02,  5.4833e-02,  1.9407e-02],\n",
       "                         [ 4.8187e-02,  2.3747e-03,  1.0541e-02],\n",
       "                         [ 6.8903e-02,  6.4321e-02,  1.7074e-02]],\n",
       "               \n",
       "                        [[-1.9943e-02, -2.6297e-02, -5.9776e-02],\n",
       "                         [ 1.2025e-02, -1.2455e-02, -1.0983e-03],\n",
       "                         [-3.0567e-02, -2.7381e-02, -6.3406e-03]],\n",
       "               \n",
       "                        [[ 1.6607e-02,  2.0732e-02, -6.9093e-04],\n",
       "                         [-9.1688e-04,  1.3838e-02,  3.4273e-02],\n",
       "                         [ 1.4203e-02,  1.6058e-02,  5.9705e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.0937e-02, -5.8115e-02, -5.7871e-02],\n",
       "                         [-3.9967e-02, -1.6563e-02,  1.3089e-02],\n",
       "                         [ 1.5725e-02,  3.9860e-02,  3.5516e-02]],\n",
       "               \n",
       "                        [[-5.4626e-02,  1.3361e-02, -5.8100e-02],\n",
       "                         [-3.8835e-03,  1.6473e-02, -2.4370e-02],\n",
       "                         [ 1.3193e-02,  5.2740e-02,  5.8236e-02]],\n",
       "               \n",
       "                        [[ 4.6594e-03, -4.3601e-02, -7.3656e-02],\n",
       "                         [-4.4007e-02, -3.4199e-02, -4.4143e-02],\n",
       "                         [ 1.7254e-02,  6.0278e-03, -5.6089e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.8048e-02, -1.8357e-02,  8.1405e-03],\n",
       "                         [ 8.8265e-02,  6.4280e-02,  1.0230e-01],\n",
       "                         [ 2.0861e-03,  1.0380e-01,  8.4684e-02]],\n",
       "               \n",
       "                        [[-2.0554e-02, -2.9618e-02, -8.6458e-02],\n",
       "                         [ 1.0337e-02, -9.9020e-03, -4.5333e-02],\n",
       "                         [ 4.3878e-03, -2.4676e-02, -1.0224e-03]],\n",
       "               \n",
       "                        [[ 1.8431e-02,  1.4109e-02, -4.2671e-02],\n",
       "                         [-3.6679e-02, -6.9554e-02, -6.9394e-02],\n",
       "                         [-3.5071e-03,  6.4470e-03, -1.3445e-02]]]], device='cuda:0')),\n",
       "              ('r7.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r7.c3.weights',\n",
       "               tensor([[[[ 0.0036]],\n",
       "               \n",
       "                        [[ 0.0724]],\n",
       "               \n",
       "                        [[ 0.0438]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0753]],\n",
       "               \n",
       "                        [[ 0.0697]],\n",
       "               \n",
       "                        [[ 0.0646]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0394]],\n",
       "               \n",
       "                        [[-0.0405]],\n",
       "               \n",
       "                        [[ 0.0546]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0501]],\n",
       "               \n",
       "                        [[-0.0453]],\n",
       "               \n",
       "                        [[ 0.0105]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0330]],\n",
       "               \n",
       "                        [[-0.0273]],\n",
       "               \n",
       "                        [[ 0.0408]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0386]],\n",
       "               \n",
       "                        [[-0.0233]],\n",
       "               \n",
       "                        [[-0.0439]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0340]],\n",
       "               \n",
       "                        [[ 0.0133]],\n",
       "               \n",
       "                        [[ 0.0202]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0395]],\n",
       "               \n",
       "                        [[ 0.0482]],\n",
       "               \n",
       "                        [[-0.0342]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0498]],\n",
       "               \n",
       "                        [[ 0.0513]],\n",
       "               \n",
       "                        [[ 0.0333]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0084]],\n",
       "               \n",
       "                        [[ 0.0787]],\n",
       "               \n",
       "                        [[-0.0352]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0496]],\n",
       "               \n",
       "                        [[-0.0116]],\n",
       "               \n",
       "                        [[-0.0865]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0253]],\n",
       "               \n",
       "                        [[-0.0900]],\n",
       "               \n",
       "                        [[-0.0259]]]], device='cuda:0')),\n",
       "              ('r7.c3.mask',\n",
       "               tensor([[[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]],\n",
       "               \n",
       "                        [[1.]]]], device='cuda:0')),\n",
       "              ('r8.bn1.weight',\n",
       "               tensor([0.7898, 0.8643, 0.7557, 0.7846, 1.1567, 0.6826, 0.8544, 0.9780, 0.8370,\n",
       "                       0.9993, 0.8907, 0.8785, 0.9958, 0.8275, 0.7038, 1.1082, 0.8783, 0.8040,\n",
       "                       0.9329, 0.9939, 0.9231, 0.9316, 0.9725, 1.0162, 1.1453, 0.8940, 0.8413,\n",
       "                       0.8663, 0.8361, 1.1763, 0.9615, 1.0707, 0.9068, 0.9217, 0.9802, 0.8416,\n",
       "                       0.8559, 0.9374, 1.0079, 1.0483, 0.9789, 0.9907, 1.0089, 1.1233, 1.0129,\n",
       "                       0.8995, 0.8871, 0.9944, 1.0422, 1.0511, 0.8391, 1.0638, 0.8707, 1.1458,\n",
       "                       0.8922, 0.9878, 0.8767, 0.7730, 1.0248, 0.9216, 0.8565, 1.0534, 0.8879,\n",
       "                       0.6887, 0.9665, 0.9628, 0.7398, 0.8929, 0.9819, 1.0789, 0.8255, 0.8888,\n",
       "                       0.7835, 1.0084, 0.7861, 0.8578, 1.1773, 1.0719, 0.8402, 0.9046, 0.9595,\n",
       "                       0.9365, 0.9379, 0.8394, 1.0584, 1.0620, 0.9332, 0.8878, 1.0453, 0.9472,\n",
       "                       1.1266, 0.9408, 1.0011, 0.9802, 0.9203, 0.9427, 0.9954, 0.9148, 1.0374,\n",
       "                       0.8925, 0.8437, 0.9075, 0.8163, 0.9903, 0.9356, 0.9931, 0.8521, 1.0072,\n",
       "                       1.2455, 0.9401, 0.9002, 0.8143, 1.0530, 1.0454, 0.8474, 0.9482, 1.0415,\n",
       "                       1.0387, 0.8575, 0.8856, 0.9461, 0.9198, 0.9592, 0.9488, 0.9080, 0.8374,\n",
       "                       0.9717, 0.9487, 0.9686, 0.8585, 1.0707, 1.1409, 0.9206, 1.0787, 1.0477,\n",
       "                       0.9522, 0.9070, 0.9290, 0.8914, 1.0254, 0.9902, 0.9784, 0.9242, 1.0164,\n",
       "                       0.8898, 0.6535, 0.9329, 1.0820, 0.9866, 0.9489, 0.8754, 0.9881, 1.0418,\n",
       "                       1.0928, 1.0014, 0.7269, 0.9793, 0.8549, 0.9597, 0.8622, 0.7214, 0.7987,\n",
       "                       0.9594, 0.9148, 0.9869, 0.9307, 0.8798, 0.9997, 0.9339, 0.9385, 1.1058,\n",
       "                       1.0568, 0.8868, 1.0988, 0.8569, 0.7445, 0.9018, 0.8737, 0.7415, 1.0296,\n",
       "                       0.9450, 0.8417, 0.9408, 0.8073, 0.8776, 0.9382, 0.9648, 0.8802, 0.9973,\n",
       "                       0.8999, 0.9148, 0.8908, 1.0107, 0.8706, 0.8261, 1.0177, 1.0928, 0.9183,\n",
       "                       1.0208, 1.1126, 0.8502, 0.9184, 1.0589, 0.8394, 1.0477, 0.8741, 0.8663,\n",
       "                       0.9045, 1.0765, 0.8244, 0.9261, 0.9334, 0.9326, 0.6217, 1.2082, 0.8033,\n",
       "                       0.8362, 0.8127, 0.9717, 0.8156, 0.8658, 0.8313, 1.0948, 0.7122, 0.9879,\n",
       "                       1.0306, 0.9349, 0.8045, 0.9617, 0.9758, 0.9274, 0.7965, 1.0500, 0.9795,\n",
       "                       0.8334, 0.9985, 1.0399, 0.8233, 0.8901, 0.9969, 0.9838, 1.0042, 1.0007,\n",
       "                       0.9074, 1.1524, 1.0094, 1.0240, 1.0327, 1.0181, 0.8283, 0.8190, 0.9730,\n",
       "                       0.8016, 1.0299, 0.9541, 0.9889, 0.8588, 0.9589, 0.8925, 0.9768, 0.9346,\n",
       "                       0.9710, 0.9179, 0.9478, 0.7462, 0.9076, 0.8754, 1.0884, 0.9490, 0.8003,\n",
       "                       0.9954, 1.0468, 0.7719, 1.1224, 1.1062, 0.9845, 0.8819, 0.9691, 0.9371,\n",
       "                       0.9186, 0.9110, 1.2500, 0.8091, 0.8346, 1.0505, 0.9449, 0.7431, 0.8456,\n",
       "                       0.9924, 0.8762, 0.8872, 1.0076, 0.8226, 1.1065, 0.9096, 0.7909, 0.8737,\n",
       "                       1.0948, 0.9743, 0.9587, 0.8300, 0.8897, 0.8548, 0.9827, 0.8931, 0.8659,\n",
       "                       0.8055, 0.8000, 0.9143, 0.9462, 1.1633, 0.9613, 0.9366, 1.0635, 0.9465,\n",
       "                       0.9258, 0.9805, 1.0526, 1.2006, 0.8095, 0.8560, 0.8180, 0.9777, 1.1042,\n",
       "                       0.9320, 1.0255, 0.8621, 1.0344, 0.7442, 0.7069, 0.9718, 0.8690, 0.9275,\n",
       "                       1.0384, 1.0431, 0.8734, 0.7871, 0.8443, 0.9750, 1.1795, 0.8877, 1.1271,\n",
       "                       1.0468, 0.9212, 0.8667, 0.8509, 0.8766, 0.9516, 1.2829, 1.1253, 0.8448,\n",
       "                       1.0180, 1.0824, 0.9143, 0.7958, 1.0397, 0.9155, 1.0336, 1.0035, 0.9232,\n",
       "                       0.8935, 0.8159, 0.9511, 0.8652, 1.0146, 1.0303, 0.9301, 0.8455, 0.7523,\n",
       "                       0.8021, 0.7954, 0.9102, 0.8955, 0.8352, 0.8297, 0.8223, 0.9749, 1.0951,\n",
       "                       0.8389, 0.9154, 0.9478, 1.0521, 0.9994, 1.1609, 0.9163, 1.0748, 0.8365,\n",
       "                       1.1691, 1.1254, 1.0466, 0.9283, 1.0396, 0.7776, 0.8114, 0.9083, 1.0716,\n",
       "                       1.0195, 0.7297, 0.9697, 0.9830, 0.7717, 0.9613, 1.0331, 0.8946, 1.0715,\n",
       "                       1.0767, 0.9485, 0.8376, 0.9181, 0.9401, 0.9762, 0.9916, 1.1645, 0.9391,\n",
       "                       0.9089, 0.9020, 1.0509, 0.9819, 0.9265, 1.0508, 1.1835, 1.0623, 0.8236,\n",
       "                       0.7605, 0.9315, 1.0282, 1.0132, 0.9117, 0.9234, 0.9140, 0.8405, 0.8943,\n",
       "                       1.1247, 1.0303, 0.8683, 0.9095, 0.8937, 0.8141, 1.2892, 1.0882, 0.9391,\n",
       "                       0.8385, 0.8710, 0.9387, 1.0305, 1.1042, 0.8843, 1.0671, 0.7965, 0.9631,\n",
       "                       1.0764, 0.7025, 1.0086, 0.9714, 1.0601, 0.7636, 0.7996, 0.8299, 0.8920,\n",
       "                       1.0387, 0.8047, 0.8808, 0.8820, 0.8258, 0.9587, 0.8523, 0.8319, 0.9744,\n",
       "                       0.8769, 0.8784, 0.8828, 0.8690, 0.8705, 0.9476, 1.0527, 0.9644, 0.8233,\n",
       "                       1.1636, 0.9011, 1.0169, 0.8092, 0.9314, 0.9757, 0.8824, 1.0325, 0.6846,\n",
       "                       0.9941, 0.9584, 0.9713, 0.9556, 0.8647, 0.9017, 1.0262, 0.8910, 1.0080,\n",
       "                       1.1466, 0.7907, 0.8934, 1.0588, 0.7719, 0.9434, 1.0086, 1.0943, 1.0512,\n",
       "                       0.9159, 1.0438, 1.1137, 0.8753, 0.8874, 0.8320, 1.0413, 0.9565],\n",
       "                      device='cuda:0')),\n",
       "              ('r8.bn1.bias',\n",
       "               tensor([-0.3037, -0.2113, -0.2563, -0.4281, -0.4365, -0.2448, -0.1715, -0.3088,\n",
       "                       -0.2811, -0.3005, -0.3373, -0.1707, -0.1214, -0.2613, -0.4833, -0.1541,\n",
       "                       -0.3373, -0.1819, -0.3557, -0.2533, -0.1406, -0.3074, -0.1171, -0.1604,\n",
       "                       -0.1708, -0.0607, -0.3796, -0.3102, -0.2890, -0.2668, -0.2624, -0.0669,\n",
       "                       -0.3509, -0.1381, -0.1475, -0.1192, -0.3518, -0.2111, -0.2772, -0.1524,\n",
       "                       -0.0193, -0.0242, -0.4028, -0.0853, -0.3042, -0.2143, -0.0996, -0.1778,\n",
       "                       -0.3218, -0.1887, -0.1340, -0.3143, -0.3220, -0.1359, -0.4210, -0.1791,\n",
       "                       -0.3765, -0.2724, -0.3632, -0.1727, -0.1270, -0.1281, -0.3240, -0.3261,\n",
       "                       -0.3133, -0.1724, -0.4455, -0.2429, -0.3719, -0.1996, -0.3357, -0.2581,\n",
       "                       -0.3543, -0.2237, -0.2280, -0.1696, -0.1069, -0.1953, -0.3037, -0.3665,\n",
       "                       -0.3395, -0.2247, -0.2169, -0.4203, -0.1399, -0.1379, -0.2649, -0.3448,\n",
       "                       -0.1620, -0.2414, -0.2277, -0.1287, -0.1703, -0.2829, -0.1846, -0.2841,\n",
       "                       -0.1523, -0.2999, -0.2559, -0.2717, -0.1912, -0.2837, -0.4020, -0.2239,\n",
       "                       -0.2923, -0.1908, -0.2239, -0.3103, -0.2898, -0.1175, -0.2375, -0.4406,\n",
       "                       -0.2073, -0.3708, -0.2165, -0.4037, -0.2246, -0.1467, -0.5213, -0.1974,\n",
       "                       -0.2781, -0.2477, -0.2690, -0.1719, -0.2408, -0.2087, -0.3415, -0.2239,\n",
       "                       -0.1250, -0.0931, -0.1208, -0.2836, -0.2774, -0.2599, -0.1244, -0.3027,\n",
       "                       -0.2710, -0.1186, -0.1524, -0.3972, -0.1907, -0.0492, -0.2758, -0.1114,\n",
       "                       -0.1544, -0.2732, -0.1412, -0.0692, -0.1615, -0.0301, -0.3531, -0.3653,\n",
       "                       -0.1543, -0.2853, -0.1817, -0.3045, -0.2567, -0.3651, -0.2087, -0.2436,\n",
       "                       -0.3424, -0.2672, -0.3429, -0.3119, -0.0086, -0.1855, -0.2530, -0.3138,\n",
       "                       -0.2041, -0.3177, -0.2746, -0.1482, -0.2204, -0.1480, -0.2866, -0.3226,\n",
       "                       -0.3605, -0.2154, -0.1634, -0.1995, -0.1817, -0.3390, -0.2755, -0.3284,\n",
       "                       -0.3262, -0.2429, -0.2402, -0.2684, -0.1379, -0.2922, -0.2823, -0.3776,\n",
       "                       -0.1702, -0.3054, -0.1818, -0.3618, -0.0679, -0.2125, -0.3051, -0.2737,\n",
       "                       -0.1151, -0.1058, -0.2476, -0.3014, -0.3553, -0.2776, -0.2343, -0.1939,\n",
       "                       -0.2904, -0.2382, -0.3029, -0.1998, -0.4031, -0.3528, -0.1925, -0.0917,\n",
       "                       -0.2851, -0.3304, -0.2459, -0.1995, -0.2274, -0.1217,  0.0094, -0.4274,\n",
       "                       -0.1674, -0.0751, -0.2416, -0.1969, -0.2352, -0.1399, -0.0783, -0.2991,\n",
       "                       -0.3105, -0.2845, -0.3965, -0.2333, -0.1634, -0.2753, -0.1652, -0.0751,\n",
       "                       -0.1737, -0.3377, -0.1532, -0.3179, -0.5400,  0.0173, -0.2248, -0.1515,\n",
       "                       -0.1355, -0.2931, -0.2794, -0.2876, -0.2565, -0.2444, -0.3283, -0.1143,\n",
       "                       -0.4964, -0.2674, -0.3205, -0.1667, -0.2862, -0.2269, -0.1817, -0.1115,\n",
       "                       -0.1196, -0.5049, -0.2379, -0.2182, -0.3317, -0.1117, -0.1914, -0.0337,\n",
       "                       -0.2916, -0.0616, -0.2327, -0.2295, -0.3385, -0.2013, -0.1982, -0.2737,\n",
       "                       -0.2796, -0.1932, -0.0374, -0.4042, -0.1932, -0.3135, -0.2566, -0.3773,\n",
       "                       -0.1548, -0.2107, -0.2657, -0.2252, -0.2666, -0.1186, -0.1145, -0.3135,\n",
       "                       -0.4631, -0.1441, -0.2395, -0.2787, -0.1676, -0.1917, -0.3574, -0.2720,\n",
       "                       -0.3606, -0.2675, -0.2279, -0.1395, -0.2859, -0.4361, -0.2474, -0.2976,\n",
       "                       -0.1002,  0.1706, -0.2969, -0.1579, -0.3007, -0.1717,  0.0146, -0.1646,\n",
       "                       -0.1289, -0.2355, -0.3106, -0.1292, -0.1908, -0.1650, -0.2292, -0.2651,\n",
       "                       -0.3697, -0.3180, -0.2053, -0.3166, -0.2736, -0.1440, -0.3066,  0.0908,\n",
       "                       -0.3993, -0.2025, -0.4562, -0.5847, -0.2632, -0.2162, -0.2139, -0.1350,\n",
       "                       -0.2589, -0.0838, -0.4459, -0.2607, -0.5306, -0.3027, -0.0413, -0.2041,\n",
       "                       -0.1212, -0.3299, -0.4447, -0.1043, -0.2215, -0.1215, -0.3707, -0.3733,\n",
       "                       -0.3974, -0.2655, -0.2139, -0.2180, -0.1795, -0.2240, -0.1435, -0.3066,\n",
       "                       -0.3261, -0.4919, -0.2281, -0.2886, -0.1268, -0.1886, -0.1942, -0.2898,\n",
       "                       -0.3496, -0.0327, -0.1476, -0.1873, -0.2861, -0.2650, -0.1992, -0.1828,\n",
       "                       -0.2928, -0.1660, -0.2354, -0.0570, -0.4340, -0.3000, -0.3436, -0.2463,\n",
       "                       -0.2212, -0.3736, -0.1940, -0.3529, -0.0012, -0.2252, -0.3819, -0.3055,\n",
       "                       -0.3002, -0.1119, -0.3131, -0.2564, -0.2602, -0.2424, -0.2347, -0.1760,\n",
       "                       -0.2557, -0.2618, -0.2904, -0.1189, -0.1801, -0.0772, -0.1957, -0.3343,\n",
       "                       -0.1038, -0.1657,  0.0144, -0.0448, -0.1393, -0.2410, -0.3649, -0.3941,\n",
       "                       -0.2090, -0.1931, -0.2078, -0.2747, -0.3000, -0.3149, -0.2501, -0.1958,\n",
       "                       -0.0802, -0.0777, -0.2201, -0.4356, -0.3895, -0.1934, -0.1721, -0.2595,\n",
       "                       -0.3050, -0.3810, -0.2281, -0.3687, -0.1171, -0.2227, -0.1811, -0.1651,\n",
       "                       -0.3679, -0.0218, -0.2635, -0.3391, -0.2190, -0.2262, -0.1783, -0.1274,\n",
       "                       -0.2664, -0.1057, -0.2397, -0.1022, -0.4711, -0.0699, -0.4234, -0.2750,\n",
       "                       -0.3597, -0.2280, -0.3320, -0.1650, -0.1271, -0.2606, -0.3397, -0.3833,\n",
       "                       -0.3544, -0.1412, -0.1396, -0.1993, -0.2768, -0.4767, -0.1793, -0.1974,\n",
       "                       -0.3233, -0.2139, -0.1915,  0.0145, -0.1097, -0.2244, -0.3099, -0.2175,\n",
       "                       -0.2821, -0.3631, -0.2741, -0.3791, -0.2132, -0.3094, -0.1404, -0.3761,\n",
       "                       -0.1872, -0.2760, -0.4121, -0.4240, -0.1340, -0.1755, -0.1689, -0.1204,\n",
       "                       -0.1572, -0.2623, -0.0806, -0.2898, -0.1894, -0.3798, -0.3440, -0.2613],\n",
       "                      device='cuda:0')),\n",
       "              ('r8.bn1.running_mean',\n",
       "               tensor([-5.3981e-02, -1.0282e-01, -9.4487e-02, -1.5722e-01,  7.4101e-03,\n",
       "                       -4.2030e-02,  4.3635e-02, -3.2092e-02,  1.7248e-02, -1.8003e-01,\n",
       "                        5.2380e-02, -9.7532e-02,  7.8737e-03, -8.2384e-02, -1.9300e-02,\n",
       "                       -1.4155e-01, -4.7141e-02, -1.3278e-01, -8.9109e-03,  1.0446e-02,\n",
       "                       -1.6088e-01, -1.9391e-02, -3.9003e-02, -1.4888e-01, -3.8578e-02,\n",
       "                       -2.9758e-02,  8.0547e-03, -1.1980e-01, -2.0478e-02, -1.8670e-01,\n",
       "                       -8.5352e-02,  6.5693e-02, -1.9103e-01, -7.3934e-02, -3.4960e-02,\n",
       "                       -5.0176e-02, -3.2307e-01, -1.1176e-01, -4.8896e-02, -2.5120e-01,\n",
       "                       -7.5631e-02, -9.1378e-02, -9.0144e-03, -4.8434e-02, -1.1973e-01,\n",
       "                       -4.5366e-02, -2.4837e-01,  7.8003e-03, -4.6846e-03, -1.4861e-01,\n",
       "                       -4.0559e-02, -2.0966e-01,  1.8011e-02, -1.7010e-01, -1.3951e-01,\n",
       "                       -1.0112e-01,  2.8208e-02, -1.7465e-01, -1.9390e-02, -8.0768e-02,\n",
       "                       -9.0969e-02, -7.4410e-02,  1.1312e-01, -6.8700e-02, -7.6413e-02,\n",
       "                       -2.3896e-01, -1.3847e-01, -2.2589e-02,  6.1635e-02, -1.1523e-01,\n",
       "                       -2.3241e-02,  1.6671e-02, -1.3587e-02, -4.8031e-02, -2.2273e-02,\n",
       "                       -2.6529e-02, -5.9397e-02, -2.1220e-02, -6.0204e-02, -6.5844e-02,\n",
       "                       -3.4524e-01, -1.3089e-02, -2.8926e-01, -1.0638e-01, -1.1290e-02,\n",
       "                        6.3466e-02, -5.1701e-02,  5.5412e-02, -1.9228e-02, -1.2520e-01,\n",
       "                        1.0080e-01, -1.5168e-01, -1.4773e-01, -7.4172e-02, -1.1783e-01,\n",
       "                       -2.4039e-01, -1.7039e-02, -1.0113e-01, -8.2202e-02, -3.5288e-02,\n",
       "                       -1.5278e-01,  7.5610e-02,  2.2304e-02, -8.3415e-02, -1.1870e-01,\n",
       "                        6.2700e-02,  4.5000e-02, -4.5606e-04, -3.4476e-02, -1.9181e-02,\n",
       "                       -1.9013e-02, -2.1817e-01, -5.6375e-02, -1.1136e-02,  1.4618e-02,\n",
       "                        3.2974e-02, -1.1555e-01,  4.5054e-04,  2.5385e-02, -2.7907e-02,\n",
       "                        1.7536e-02, -8.6073e-02,  8.8982e-03, -1.6282e-01, -6.1965e-02,\n",
       "                       -2.7824e-02, -5.9269e-02, -4.1175e-02, -2.0596e-01, -2.8451e-02,\n",
       "                       -5.9648e-02, -1.1316e-01,  1.0498e-02, -6.9833e-03, -7.1003e-02,\n",
       "                       -3.3317e-01,  5.6898e-02, -2.6418e-02, -3.1688e-02,  8.6252e-02,\n",
       "                       -3.2827e-02, -8.7349e-02, -1.1164e-01, -3.0588e-01, -3.1841e-02,\n",
       "                       -1.6328e-01, -7.0911e-02,  4.5001e-02, -1.1251e-01, -1.0567e-01,\n",
       "                        4.8275e-02, -9.3649e-02, -5.0066e-03,  9.0738e-02, -2.0023e-01,\n",
       "                       -1.7642e-01, -8.0207e-02, -9.4416e-02,  2.3285e-02, -3.2422e-02,\n",
       "                       -1.0145e-02, -9.0588e-02,  4.7834e-02, -1.5262e-01, -8.5684e-02,\n",
       "                        6.1450e-04, -1.5609e-01,  5.8357e-02, -2.8387e-02, -6.1408e-02,\n",
       "                       -7.4284e-02, -1.4808e-02, -1.1163e-01,  8.5510e-02, -5.2790e-02,\n",
       "                       -2.6447e-01, -1.2448e-01, -1.8129e-01, -2.3575e-01,  7.7777e-02,\n",
       "                       -9.0544e-02,  5.4320e-02, -4.7434e-02, -8.1251e-02, -6.8822e-02,\n",
       "                        4.5987e-02, -6.7582e-02, -1.5982e-02, -8.5925e-02, -6.1341e-02,\n",
       "                       -6.0835e-02, -1.0867e-01, -1.1428e-01, -1.9913e-01, -2.6786e-02,\n",
       "                        3.6284e-02, -1.4626e-01, -9.3483e-02, -6.2987e-02, -1.0509e-01,\n",
       "                       -1.9449e-01, -1.5701e-01,  1.5241e-02, -1.2947e-01,  2.7480e-02,\n",
       "                       -1.7732e-01,  1.6489e-02,  5.2330e-02, -8.9938e-02,  2.3479e-02,\n",
       "                       -2.3827e-01, -5.2072e-02, -1.6775e-01, -6.4764e-02, -2.4562e-02,\n",
       "                        6.6225e-02, -1.4073e-01, -1.0400e-01, -3.5919e-02, -7.6528e-02,\n",
       "                       -1.1835e-01, -9.4624e-02, -1.3001e-01, -9.9121e-02,  2.3008e-02,\n",
       "                       -1.3932e-01, -5.1032e-02, -7.2181e-02, -1.5562e-01, -4.5242e-02,\n",
       "                       -1.6856e-01, -3.3379e-02, -5.2600e-02, -9.1365e-02, -1.4028e-01,\n",
       "                       -4.7248e-02, -4.3274e-02, -3.6877e-02, -1.6734e-01, -1.3503e-01,\n",
       "                       -3.1114e-02, -7.8281e-02,  3.3413e-02, -7.8233e-02,  2.9868e-02,\n",
       "                       -1.9410e-02, -9.0496e-04, -1.1396e-01, -1.0031e-01, -2.3060e-01,\n",
       "                       -5.7087e-02, -5.3288e-02,  1.4369e-02,  1.0636e-03, -1.2610e-01,\n",
       "                       -2.5548e-02, -5.3842e-02, -1.4609e-01, -6.2914e-02,  2.0410e-03,\n",
       "                       -9.9299e-02,  3.5997e-02, -1.3035e-02, -8.0691e-03, -1.9582e-01,\n",
       "                        5.8889e-03, -2.5241e-01, -2.3226e-02, -4.1869e-03, -1.3946e-01,\n",
       "                        6.1201e-02, -1.8759e-01, -2.0181e-02, -7.1262e-02, -1.7845e-01,\n",
       "                        3.1732e-02, -3.7339e-02, -1.6457e-01,  9.7823e-02,  3.5799e-02,\n",
       "                        2.7529e-02, -9.3694e-02, -1.0468e-01, -1.5742e-01, -2.9043e-02,\n",
       "                        7.0980e-02, -1.1021e-01, -8.6845e-02, -3.5131e-03,  9.6360e-02,\n",
       "                       -5.5812e-02, -1.7177e-01,  9.8613e-02,  3.9395e-02, -1.4024e-01,\n",
       "                       -1.7425e-01, -2.5588e-02, -1.4456e-01, -1.4594e-01, -1.1732e-01,\n",
       "                       -1.7133e-02, -3.0600e-01, -5.9758e-04,  5.3412e-03, -3.1633e-02,\n",
       "                        4.1972e-03,  2.1474e-03, -5.2074e-02, -3.2050e-02, -3.1674e-02,\n",
       "                       -5.6309e-02, -7.0597e-03,  2.6177e-02, -4.1821e-01,  2.7664e-02,\n",
       "                        1.2618e-01, -7.9449e-02, -8.6351e-02, -1.6984e-01, -1.9020e-02,\n",
       "                        3.9561e-02, -7.9006e-02, -4.4898e-02, -2.8421e-01, -8.1246e-02,\n",
       "                       -2.4099e-01, -1.1922e-01,  3.1394e-02, -2.1829e-01, -1.4383e-02,\n",
       "                        2.9513e-02,  1.1940e-02,  3.7455e-02,  9.1829e-03,  4.2024e-02,\n",
       "                        7.1964e-02,  2.4824e-02, -9.4372e-02, -1.5867e-01,  1.5324e-02,\n",
       "                       -2.0813e-01, -3.9592e-01, -1.9829e-02, -2.0131e-01, -2.5873e-02,\n",
       "                       -1.0721e-01, -2.4044e-03, -7.6213e-02,  3.0346e-02, -1.2724e-01,\n",
       "                       -8.1896e-02,  2.0306e-02, -5.7191e-02, -1.4558e-01, -2.0497e-01,\n",
       "                       -2.6894e-01, -1.4567e-01, -1.3330e-01, -9.5625e-03, -9.9600e-02,\n",
       "                       -2.4550e-01, -1.6451e-01, -7.7705e-02,  4.7614e-02, -3.4557e-02,\n",
       "                       -5.7674e-02,  1.3461e-01, -2.9008e-03,  9.9462e-03, -1.2844e-01,\n",
       "                       -4.1335e-01, -1.0425e-01, -7.8125e-02, -1.7458e-01, -1.1113e-01,\n",
       "                       -6.0987e-02, -8.4707e-02, -2.7779e-01, -1.4359e-01, -2.7569e-01,\n",
       "                       -2.4880e-02, -5.8283e-02, -1.1578e-01, -1.7283e-01, -1.6127e-01,\n",
       "                       -6.6774e-03, -1.0444e-01,  5.1169e-02,  4.1566e-02, -2.1604e-02,\n",
       "                        1.4082e-02, -1.3196e-01, -1.6751e-01, -6.7242e-02, -1.2642e-01,\n",
       "                        5.4465e-02, -9.4625e-02, -1.7099e-01, -4.7503e-02, -1.9965e-02,\n",
       "                       -1.0852e-01, -1.0966e-01, -2.5273e-01,  9.2601e-03,  6.9462e-03,\n",
       "                       -5.1849e-02, -1.6301e-01, -2.2310e-02, -4.6098e-02, -9.9913e-02,\n",
       "                        2.6325e-02, -1.0257e-01, -1.3520e-01,  3.9474e-02,  7.1440e-02,\n",
       "                       -2.4893e-02, -2.1646e-02, -1.3383e-01, -1.6103e-03, -1.4926e-01,\n",
       "                       -6.7782e-02, -1.0183e-01, -2.3315e-01, -1.9328e-01,  3.1646e-03,\n",
       "                       -5.8070e-02, -1.7126e-01,  6.3303e-02, -1.3556e-02, -4.7011e-02,\n",
       "                       -1.0936e-02, -1.3638e-01, -2.7003e-01,  1.4335e-02, -2.6351e-02,\n",
       "                       -7.5812e-02, -9.7991e-02, -9.5002e-02,  8.4157e-03, -1.8542e-02,\n",
       "                       -1.4199e-02, -5.0427e-02,  5.7786e-02, -1.1585e-02,  1.0216e-01,\n",
       "                       -1.9072e-02, -2.1662e-01, -2.2572e-02, -1.4520e-01, -1.0795e-01,\n",
       "                       -6.2120e-02, -1.0368e-01, -2.9413e-01, -3.1704e-05,  5.8723e-03,\n",
       "                       -1.4248e-01, -1.2434e-01, -4.6769e-02, -8.5450e-02, -5.4911e-02,\n",
       "                       -1.4296e-01, -6.6671e-02, -1.1002e-01, -2.5701e-02, -8.7548e-02,\n",
       "                       -1.1131e-01, -2.0264e-02,  3.2764e-02, -1.2302e-01, -2.9977e-03,\n",
       "                        6.7934e-02, -7.2716e-02, -1.2963e-01, -4.7932e-02, -4.8712e-02,\n",
       "                       -1.1418e-02,  2.1845e-02,  9.2615e-04, -7.0312e-02, -5.3180e-02,\n",
       "                        8.1909e-02, -8.7847e-02, -1.5013e-01, -1.7091e-01,  6.2350e-03,\n",
       "                       -1.0247e-01, -1.4346e-01, -2.8401e-02, -2.3041e-02, -2.1911e-01,\n",
       "                       -6.1583e-02, -9.8422e-02,  1.3108e-02, -1.0806e-01, -4.8091e-02,\n",
       "                        2.5112e-02, -1.1196e-01, -2.6294e-02,  5.2261e-02, -9.1108e-02,\n",
       "                       -2.9574e-01,  8.1920e-02, -1.4032e-01, -5.0422e-02, -1.4589e-01,\n",
       "                        4.2169e-03,  1.2325e-02, -7.9637e-02, -9.8299e-02,  1.4430e-01,\n",
       "                        1.0077e-02, -2.4949e-02], device='cuda:0')),\n",
       "              ('r8.bn1.running_var',\n",
       "               tensor([1.7799, 0.7545, 0.7267, 0.7610, 3.2043, 1.6969, 1.4819, 3.0854, 0.9297,\n",
       "                       0.7806, 2.0986, 0.6962, 1.8643, 0.7099, 0.6237, 2.5766, 0.8612, 1.3550,\n",
       "                       2.4583, 2.2284, 0.7554, 0.9770, 1.3832, 1.4654, 3.2203, 1.2021, 0.5024,\n",
       "                       0.9875, 0.9776, 1.0603, 2.7472, 2.7706, 1.1728, 2.0998, 2.5979, 1.8251,\n",
       "                       0.7182, 0.9899, 0.8737, 0.8936, 3.5154, 1.9990, 2.3762, 1.7235, 2.1568,\n",
       "                       0.8013, 2.9885, 1.1030, 0.8394, 0.8637, 2.0831, 1.3348, 0.7289, 0.9117,\n",
       "                       0.7141, 2.9357, 0.6771, 0.7218, 3.2624, 1.5510, 0.7609, 2.5293, 0.6384,\n",
       "                       1.0789, 1.4984, 1.0221, 0.7093, 0.9742, 1.4717, 2.5443, 0.9941, 0.7585,\n",
       "                       0.6391, 3.6410, 1.1617, 2.7929, 3.1613, 2.1912, 0.6539, 0.9173, 0.8948,\n",
       "                       2.1286, 1.0523, 1.0176, 3.4451, 1.8367, 0.8851, 1.3539, 1.9361, 0.9014,\n",
       "                       2.2463, 2.3103, 1.0333, 3.1813, 1.5612, 1.2784, 3.4967, 1.0256, 0.8609,\n",
       "                       0.8266, 0.9430, 1.8571, 0.7856, 4.3970, 1.1674, 2.3336, 2.4843, 2.5071,\n",
       "                       3.3943, 2.3275, 0.7867, 0.8053, 3.0378, 1.3164, 1.5679, 2.7415, 1.0117,\n",
       "                       1.4151, 0.6368, 2.0104, 2.5610, 0.7374, 2.5626, 1.0387, 0.9722, 0.6141,\n",
       "                       2.4661, 2.1612, 1.7837, 1.0139, 1.7139, 1.2071, 0.8365, 2.9776, 1.3323,\n",
       "                       0.9018, 1.9089, 1.7414, 2.1271, 2.4767, 1.4247, 2.3185, 0.9955, 1.2286,\n",
       "                       2.4608, 0.8962, 2.2814, 1.9804, 3.6268, 2.2128, 1.4440, 0.8157, 1.3673,\n",
       "                       4.7361, 0.9800, 0.7208, 0.9448, 0.6943, 2.4473, 1.3052, 0.6285, 0.6948,\n",
       "                       0.9758, 1.1994, 2.8373, 2.7054, 2.0781, 2.8975, 2.0604, 0.7275, 3.8536,\n",
       "                       3.0946, 2.0555, 2.9540, 2.4215, 0.7450, 0.6797, 1.9177, 2.4881, 0.8372,\n",
       "                       2.0137, 0.7291, 0.8837, 0.9176, 0.7177, 3.2793, 0.8198, 1.5849, 2.1462,\n",
       "                       0.9335, 2.3528, 1.2585, 3.2391, 0.8915, 1.9565, 2.9752, 1.6226, 3.6357,\n",
       "                       0.9628, 1.2084, 0.6297, 1.8939, 3.4117, 0.5223, 1.7650, 0.8002, 1.8152,\n",
       "                       1.1381, 0.9975, 0.7111, 0.8075, 1.0186, 0.7610, 0.6479, 3.3403, 1.4875,\n",
       "                       0.7709, 0.7971, 0.8556, 0.9309, 0.7564, 0.8786, 1.3114, 1.0078, 1.3950,\n",
       "                       1.5445, 3.4352, 2.6733, 0.9110, 1.5718, 0.8169, 0.9540, 2.9468, 0.7115,\n",
       "                       0.9558, 0.8890, 2.1435, 0.7165, 0.9315, 1.5514, 1.7132, 1.4709, 3.3877,\n",
       "                       2.0157, 3.1429, 3.4265, 1.0526, 1.8289, 2.3897, 0.6952, 0.8154, 2.1073,\n",
       "                       1.0139, 3.4332, 2.3035, 0.8015, 0.6423, 0.6817, 1.7531, 2.3951, 0.8790,\n",
       "                       2.6461, 2.3758, 3.6254, 0.6350, 0.7164, 0.9283, 2.4248, 2.6218, 0.7147,\n",
       "                       1.0711, 0.9131, 1.0166, 2.6934, 1.0208, 0.8275, 0.7126, 2.3715, 0.8898,\n",
       "                       2.3191, 2.2869, 4.2713, 1.0113, 0.7431, 2.6195, 1.9861, 0.6976, 0.7813,\n",
       "                       4.0329, 0.8462, 1.5189, 0.9009, 2.2904, 2.9841, 0.9304, 1.9749, 0.6587,\n",
       "                       2.6731, 0.9732, 0.7175, 2.8473, 3.1547, 0.6344, 1.7611, 0.8920, 2.1462,\n",
       "                       1.0810, 1.5969, 1.4362, 3.2672, 3.5466, 3.0681, 2.4088, 1.3784, 3.5078,\n",
       "                       0.7378, 2.6124, 3.1849, 1.9430, 0.8483, 1.4359, 1.2468, 1.9961, 0.8959,\n",
       "                       3.4732, 1.1294, 2.3818, 3.3424, 1.0843, 0.9023, 1.6776, 0.8302, 3.5061,\n",
       "                       1.5939, 3.9113, 0.7348, 0.6476, 1.3990, 1.1943, 3.8255, 0.7589, 0.8767,\n",
       "                       3.1928, 0.7309, 2.8028, 2.4961, 0.7271, 0.9063, 2.8588, 0.8276, 1.0057,\n",
       "                       0.9123, 2.3963, 0.7384, 0.8079, 1.0644, 0.8691, 2.6429, 0.8751, 0.8821,\n",
       "                       0.8640, 4.5641, 2.0592, 3.6013, 0.7916, 1.6655, 1.8474, 1.2942, 0.6859,\n",
       "                       0.6082, 3.2341, 0.8204, 3.1876, 1.0043, 0.6525, 0.7065, 1.4181, 0.9015,\n",
       "                       1.5456, 0.7302, 0.8686, 1.8772, 0.7133, 0.8022, 1.3065, 1.6488, 0.7838,\n",
       "                       2.3155, 3.0086, 0.7128, 1.1907, 1.0742, 1.0239, 0.6958, 0.7712, 2.3293,\n",
       "                       1.9398, 0.9436, 0.8415, 1.7369, 0.9233, 1.6199, 1.0660, 1.1621, 4.4948,\n",
       "                       3.2691, 0.7715, 1.2240, 1.0526, 1.2406, 2.6762, 2.3636, 2.7804, 1.8558,\n",
       "                       0.5956, 1.1735, 2.7615, 1.7443, 2.1000, 0.9935, 2.7960, 2.2880, 0.6664,\n",
       "                       0.8230, 1.9187, 1.8872, 0.8849, 2.3194, 3.2877, 0.8966, 0.7941, 1.5065,\n",
       "                       1.0497, 0.9653, 0.9038, 0.7441, 0.7913, 0.7634, 1.3695, 3.3450, 2.7127,\n",
       "                       0.6980, 0.6016, 0.7649, 1.8564, 3.0893, 0.9493, 3.6413, 0.7468, 1.5769,\n",
       "                       3.1005, 0.7292, 0.9210, 1.9434, 3.1450, 1.6848, 0.6673, 1.2105, 1.4794,\n",
       "                       1.7849, 0.6545, 1.1839, 0.7186, 1.7946, 0.9720, 0.8062, 0.6891, 1.8649,\n",
       "                       1.2627, 1.3482, 0.8927, 0.6052, 0.8088, 0.8328, 2.4207, 1.8810, 0.8735,\n",
       "                       2.6088, 1.5910, 2.5039, 1.0967, 2.5736, 0.9213, 0.8097, 3.1490, 0.9307,\n",
       "                       1.0284, 2.3333, 1.0488, 1.2399, 0.7065, 0.6788, 3.3459, 0.8368, 2.6034,\n",
       "                       3.1165, 1.0127, 3.1067, 2.7416, 0.5351, 0.8574, 1.0012, 0.9999, 3.3936,\n",
       "                       1.8218, 2.8392, 2.2507, 5.7288, 0.7343, 0.6418, 2.6139, 2.0698],\n",
       "                      device='cuda:0')),\n",
       "              ('r8.bn1.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r8.c1.weights',\n",
       "               tensor([[[[ 0.0275,  0.0660,  0.0809],\n",
       "                         [ 0.0515, -0.0173,  0.0151],\n",
       "                         [ 0.0727,  0.0058, -0.0304]],\n",
       "               \n",
       "                        [[ 0.2177, -0.1810, -0.0745],\n",
       "                         [-0.1130,  0.0444, -0.0577],\n",
       "                         [-0.0745,  0.0108,  0.0197]],\n",
       "               \n",
       "                        [[ 0.0466, -0.0095,  0.0232],\n",
       "                         [ 0.1107,  0.1558,  0.0986],\n",
       "                         [-0.0337, -0.0540,  0.0051]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0490, -0.0656,  0.0254],\n",
       "                         [ 0.0644, -0.0161,  0.0609],\n",
       "                         [ 0.0032,  0.0005,  0.0863]],\n",
       "               \n",
       "                        [[ 0.0187,  0.0093, -0.0282],\n",
       "                         [-0.0330, -0.0820, -0.0087],\n",
       "                         [-0.0180, -0.0491, -0.0050]],\n",
       "               \n",
       "                        [[-0.0247, -0.1133, -0.0440],\n",
       "                         [-0.0758, -0.1076, -0.0665],\n",
       "                         [ 0.0013,  0.0429, -0.0664]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0267, -0.0342,  0.0401],\n",
       "                         [ 0.0315, -0.0791,  0.0762],\n",
       "                         [-0.0490, -0.0295,  0.0887]],\n",
       "               \n",
       "                        [[-0.1562, -0.0653,  0.1191],\n",
       "                         [-0.1051, -0.0306, -0.0081],\n",
       "                         [ 0.0270,  0.1064, -0.0090]],\n",
       "               \n",
       "                        [[-0.0230,  0.0553, -0.0381],\n",
       "                         [ 0.0206,  0.0522,  0.0802],\n",
       "                         [ 0.0048, -0.0440,  0.0851]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.2789, -0.0793,  0.0119],\n",
       "                         [-0.0661, -0.0566, -0.0172],\n",
       "                         [-0.0514, -0.1261, -0.0652]],\n",
       "               \n",
       "                        [[-0.0646, -0.0324,  0.0729],\n",
       "                         [ 0.0771, -0.0266, -0.0073],\n",
       "                         [ 0.0473, -0.0389, -0.0420]],\n",
       "               \n",
       "                        [[-0.1129, -0.0542, -0.1036],\n",
       "                         [-0.1578, -0.2251, -0.1627],\n",
       "                         [-0.0731, -0.0919, -0.0906]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0554, -0.0103,  0.0020],\n",
       "                         [-0.0012, -0.0579, -0.0739],\n",
       "                         [ 0.0401, -0.0107, -0.0354]],\n",
       "               \n",
       "                        [[ 0.0640,  0.0778,  0.0304],\n",
       "                         [-0.0765,  0.1086,  0.0827],\n",
       "                         [-0.0789, -0.1513, -0.1376]],\n",
       "               \n",
       "                        [[ 0.0341,  0.0458,  0.0496],\n",
       "                         [-0.0183, -0.0050, -0.0086],\n",
       "                         [-0.0304, -0.0907, -0.0567]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0453,  0.0400,  0.0008],\n",
       "                         [-0.0978, -0.0254, -0.0100],\n",
       "                         [-0.1286, -0.0907, -0.1247]],\n",
       "               \n",
       "                        [[-0.0298, -0.1066, -0.0445],\n",
       "                         [-0.0203, -0.1109, -0.0696],\n",
       "                         [-0.0302, -0.0082,  0.0718]],\n",
       "               \n",
       "                        [[ 0.0190, -0.0486, -0.0954],\n",
       "                         [-0.0421, -0.0312, -0.0891],\n",
       "                         [-0.0390,  0.0304,  0.0725]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0076,  0.0150,  0.0235],\n",
       "                         [ 0.0260, -0.0051, -0.0392],\n",
       "                         [-0.0117, -0.1283, -0.0628]],\n",
       "               \n",
       "                        [[-0.0154, -0.0114, -0.0540],\n",
       "                         [-0.0278, -0.0419, -0.0242],\n",
       "                         [ 0.0186,  0.0042, -0.0148]],\n",
       "               \n",
       "                        [[ 0.0921,  0.0342, -0.0395],\n",
       "                         [ 0.0281,  0.0583, -0.0504],\n",
       "                         [ 0.0147, -0.0514, -0.0341]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0992,  0.1190,  0.1039],\n",
       "                         [ 0.0688,  0.0379, -0.0490],\n",
       "                         [ 0.0783, -0.0051, -0.0527]],\n",
       "               \n",
       "                        [[-0.0659,  0.0093, -0.0249],\n",
       "                         [-0.0917, -0.0694, -0.0705],\n",
       "                         [-0.0160, -0.0902, -0.1206]],\n",
       "               \n",
       "                        [[-0.1100, -0.0579, -0.0691],\n",
       "                         [-0.1072, -0.0774, -0.0707],\n",
       "                         [-0.0685, -0.0303, -0.0241]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0430, -0.0406, -0.0670],\n",
       "                         [-0.1100, -0.1180, -0.0865],\n",
       "                         [-0.0544, -0.0446,  0.0348]],\n",
       "               \n",
       "                        [[-0.0506, -0.1037, -0.1396],\n",
       "                         [-0.0628, -0.0109, -0.0593],\n",
       "                         [-0.0311, -0.0674,  0.0164]],\n",
       "               \n",
       "                        [[ 0.0411,  0.0958, -0.0104],\n",
       "                         [ 0.0456,  0.0632,  0.0792],\n",
       "                         [ 0.1700, -0.0490,  0.0760]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0685, -0.0317, -0.0167],\n",
       "                         [-0.0098, -0.0619, -0.0297],\n",
       "                         [-0.0925, -0.1111, -0.0855]],\n",
       "               \n",
       "                        [[-0.1718, -0.1342, -0.0774],\n",
       "                         [-0.0206, -0.0367,  0.0027],\n",
       "                         [-0.0312, -0.0858, -0.0215]],\n",
       "               \n",
       "                        [[-0.0372, -0.0670, -0.0321],\n",
       "                         [-0.0679, -0.0403, -0.1027],\n",
       "                         [-0.0483,  0.0694, -0.0573]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0899, -0.0562, -0.0909],\n",
       "                         [-0.0208, -0.0391, -0.0550],\n",
       "                         [-0.0227, -0.0766, -0.1019]],\n",
       "               \n",
       "                        [[-0.0176,  0.0032, -0.0878],\n",
       "                         [ 0.0056, -0.0340,  0.0348],\n",
       "                         [ 0.0658,  0.0274, -0.0075]],\n",
       "               \n",
       "                        [[-0.1639, -0.1004, -0.1091],\n",
       "                         [ 0.0047,  0.0892, -0.0411],\n",
       "                         [-0.0514, -0.0657, -0.1336]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0457, -0.0120, -0.0751],\n",
       "                         [-0.0082, -0.0406, -0.0258],\n",
       "                         [-0.0478, -0.0688, -0.0721]],\n",
       "               \n",
       "                        [[ 0.0479,  0.0866,  0.0497],\n",
       "                         [ 0.0809,  0.0411,  0.0321],\n",
       "                         [ 0.0337, -0.0099, -0.1144]],\n",
       "               \n",
       "                        [[ 0.0728,  0.0477,  0.0521],\n",
       "                         [-0.0525, -0.0245,  0.0230],\n",
       "                         [-0.0015,  0.0127,  0.0790]]]], device='cuda:0')),\n",
       "              ('r8.c1.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('r8.bn2.weight',\n",
       "               tensor([0.9215, 0.9438, 0.9014, 0.9478, 0.9639, 0.8717, 0.9359, 0.9069, 0.9259,\n",
       "                       0.8975, 0.9052, 0.8695, 0.9038, 0.9281, 0.9499, 0.9268, 0.9195, 0.8871,\n",
       "                       0.8815, 0.8872, 0.9322, 0.9024, 0.9004, 0.9448, 0.9100, 0.9055, 0.8853,\n",
       "                       0.9002, 0.9000, 0.9073, 0.8700, 0.9003, 0.8940, 0.9222, 0.9221, 0.9122,\n",
       "                       0.9484, 0.9075, 0.8951, 0.9354, 0.9227, 0.9161, 0.9097, 0.9196, 0.9203,\n",
       "                       0.8946, 0.9123, 0.8861, 0.9287, 0.9352, 0.9232, 0.9137, 0.8865, 0.8955,\n",
       "                       0.9517, 0.9586, 0.9194, 0.9078, 0.9003, 0.8998, 0.9189, 0.9185, 0.9013,\n",
       "                       0.9329, 0.9104, 0.9007, 0.8769, 0.8866, 0.8981, 0.9180, 0.8965, 0.9220,\n",
       "                       0.9154, 0.8550, 0.9469, 0.8871, 0.9054, 0.9067, 0.8718, 0.9048, 0.8979,\n",
       "                       0.8778, 0.8677, 0.9249, 0.8896, 0.8862, 0.8953, 0.8998, 0.9101, 0.9131,\n",
       "                       0.9279, 1.0142, 0.9087, 0.9345, 0.8853, 0.9071, 0.8803, 0.9217, 0.9114,\n",
       "                       0.9044, 0.9340, 0.9416, 0.8947, 0.8984, 0.8884, 0.8553, 0.9442, 0.8966,\n",
       "                       0.9096, 0.9001, 0.8704, 0.9145, 0.9664, 0.9001, 0.8957, 0.9375, 0.9460,\n",
       "                       0.9499, 0.9209, 0.8970, 0.8909, 0.8935, 0.9280, 0.9543, 0.9200, 0.9192,\n",
       "                       0.9351, 0.8901, 0.8954, 0.9290, 0.9110, 0.8837, 0.9119, 0.9570, 0.9054,\n",
       "                       0.9091, 0.8972, 0.9002, 0.8713, 0.9002, 0.9106, 0.8751, 0.9161, 0.8890,\n",
       "                       0.8911, 0.9249, 0.9125, 0.8825, 0.9698, 0.9209, 0.9165, 0.9139, 0.9139,\n",
       "                       0.9008, 0.8841, 0.9230, 0.8782, 0.9021, 0.8805, 0.9159, 0.9027, 0.8903,\n",
       "                       0.9179, 0.9000, 0.9289, 0.8554, 0.9712, 0.9201, 0.8852, 0.9086, 0.8985,\n",
       "                       0.9206, 0.8896, 0.8763, 0.8977, 0.9663, 0.9082, 0.9023, 0.9036, 0.8911,\n",
       "                       0.9114, 0.9457, 0.9109, 0.9143, 0.9019, 0.9771, 0.9495, 0.9202, 0.9142,\n",
       "                       0.9298, 0.9213, 0.9189, 0.8919, 0.9323, 0.8930, 0.9066, 0.8948, 0.9323,\n",
       "                       0.8858, 0.8960, 0.8980, 0.8971, 0.8930, 0.9267, 0.9222, 0.8968, 0.9041,\n",
       "                       0.8965, 0.9490, 0.9123, 0.8956, 0.9283, 0.9008, 0.9130, 0.9339, 0.9545,\n",
       "                       0.9297, 0.9468, 0.8920, 0.9014, 0.8926, 0.9039, 0.9070, 0.9400, 0.9290,\n",
       "                       0.8828, 0.8742, 0.9237, 0.8932, 0.8827, 0.8976, 0.9191, 0.9041, 0.8744,\n",
       "                       0.9083, 0.9271, 0.8954, 0.9333, 0.9090, 0.9261, 0.8606, 0.8996, 0.9030,\n",
       "                       0.9182, 0.9174, 0.9320, 0.9046, 0.9397, 0.9177, 0.9381, 0.9035, 0.9385,\n",
       "                       0.8878, 0.8957, 0.9224, 0.9081, 0.8878, 0.9034, 0.9446, 0.9310, 0.9074,\n",
       "                       0.9155, 0.8997, 0.9093, 0.9122, 0.8916, 0.8631, 0.9154, 0.8821, 0.8822,\n",
       "                       0.9047, 0.8776, 0.9570, 0.9048, 0.9636, 0.8679, 0.8833, 0.8994, 0.8809,\n",
       "                       0.8990, 0.8992, 0.9101, 0.9318, 0.8877, 0.9226, 0.9211, 0.8735, 0.8862,\n",
       "                       0.9149, 0.9082, 0.8805, 0.8999, 0.9498, 0.9136, 0.9589, 0.9004, 0.9050,\n",
       "                       0.9010, 0.8627, 0.9188, 0.8967, 0.8892, 0.9120, 0.9176, 0.8792, 0.9370,\n",
       "                       0.8947, 0.9094, 0.9116, 0.8923, 0.8816, 0.9072, 0.9106, 0.8996, 0.9085,\n",
       "                       0.8912, 0.9306, 0.8942, 0.8637, 0.8791, 0.8995, 0.9045, 0.8838, 0.9497,\n",
       "                       0.9243, 0.9082, 0.9322, 0.9380, 0.9485, 0.9452, 0.8852, 0.9193, 0.9058,\n",
       "                       0.9014, 0.9013, 0.8914, 0.9041, 0.8828, 0.9058, 0.9271, 0.8936, 0.9029,\n",
       "                       0.9461, 0.8964, 0.8970, 0.9639, 0.8926, 0.9064, 0.9128, 0.8835, 0.8811,\n",
       "                       0.9436, 0.8788, 0.9215, 0.9063, 0.8920, 0.8992, 0.8902, 0.8689, 0.8999,\n",
       "                       0.9021, 0.9672, 0.8920, 0.9290, 0.9255, 0.9141, 0.9229, 0.9364, 0.9321,\n",
       "                       0.9161, 0.8984, 0.8985, 0.9299, 0.9149, 0.8825, 0.8896, 0.9109, 0.8788,\n",
       "                       0.9303, 0.9280, 0.8991, 0.9831, 0.9041, 0.9052, 0.8902, 0.8981, 0.9176,\n",
       "                       0.9028, 0.9461, 0.9083, 0.9197, 0.9083, 0.8893, 0.9185, 0.8886, 0.9103,\n",
       "                       0.9119, 0.8984, 0.9302, 0.9360, 0.8971, 0.8758, 0.9240, 0.9149, 0.9663,\n",
       "                       0.9228, 0.8975, 0.9108, 0.9642, 0.9259, 0.9367, 0.9261, 0.9338, 0.9127,\n",
       "                       0.8973, 0.8914, 0.9045, 0.9172, 0.9348, 0.8696, 0.8934, 0.9121, 0.9126,\n",
       "                       0.8464, 0.8927, 0.9241, 0.9250, 0.8850, 0.9028, 0.9516, 0.8901, 0.9275,\n",
       "                       0.9201, 0.9324, 0.9481, 0.8914, 0.8875, 0.9195, 0.9049, 0.9072, 0.8862,\n",
       "                       0.9385, 0.8950, 0.9093, 0.9690, 0.8980, 0.9201, 0.8886, 0.8894, 0.9107,\n",
       "                       0.9202, 0.9272, 0.8762, 0.9191, 0.8928, 0.8906, 0.8945, 0.8780, 0.9207,\n",
       "                       0.8917, 0.9058, 1.0013, 0.9200, 0.9213, 0.8779, 0.8986, 0.9340, 0.9017,\n",
       "                       0.8598, 0.9001, 0.8964, 0.8941, 0.8633, 0.9415, 0.8992, 0.8929, 0.8898,\n",
       "                       0.9155, 0.9033, 0.9049, 0.8966, 0.9447, 0.8726, 0.8840, 0.9045, 0.9096,\n",
       "                       0.8618, 0.9377, 0.8791, 0.8946, 0.9051, 0.9162, 0.9098, 0.9065, 0.9215,\n",
       "                       0.9342, 0.9121, 0.8909, 0.9518, 0.9234, 0.9333, 0.9000, 0.9161, 0.8962,\n",
       "                       0.8982, 0.8967, 0.9749, 0.8878, 0.9054, 0.8762, 0.9194, 0.9084],\n",
       "                      device='cuda:0')),\n",
       "              ('r8.bn2.bias',\n",
       "               tensor([-0.2386, -0.2774, -0.1566, -0.2580, -0.2729, -0.2416, -0.3057, -0.2641,\n",
       "                       -0.2538, -0.1979, -0.2116, -0.1930, -0.1516, -0.2107, -0.2388, -0.2432,\n",
       "                       -0.2264, -0.2236, -0.1918, -0.0737, -0.2358, -0.2187, -0.2117, -0.2504,\n",
       "                       -0.2038, -0.1833, -0.2185, -0.1490, -0.1555, -0.2250, -0.1928, -0.1824,\n",
       "                       -0.2168, -0.2035, -0.1886, -0.2432, -0.2272, -0.1718, -0.1488, -0.2689,\n",
       "                       -0.2185, -0.2515, -0.2450, -0.2516, -0.1953, -0.1963, -0.1781, -0.2056,\n",
       "                       -0.3144, -0.2429, -0.2558, -0.2279, -0.1765, -0.2070, -0.1842, -0.3743,\n",
       "                       -0.2535, -0.2535, -0.1968, -0.2816, -0.2183, -0.2389, -0.1605, -0.2646,\n",
       "                       -0.2055, -0.2838, -0.2030, -0.1996, -0.1982, -0.2449, -0.2245, -0.2595,\n",
       "                       -0.2658, -0.1432, -0.2538, -0.1994, -0.2018, -0.2213, -0.1875, -0.1815,\n",
       "                       -0.1597, -0.1572, -0.2418, -0.2315, -0.2178, -0.2381, -0.2059, -0.2601,\n",
       "                       -0.2076, -0.2058, -0.1857, -0.3597, -0.2576, -0.2338, -0.1969, -0.1567,\n",
       "                       -0.1998, -0.1962, -0.2196, -0.2266, -0.1930, -0.2498, -0.2143, -0.1895,\n",
       "                       -0.1577, -0.1205, -0.2487, -0.1931, -0.1913, -0.1871, -0.1991, -0.2236,\n",
       "                       -0.3482, -0.2208, -0.1125, -0.2386, -0.2785, -0.2702, -0.2123, -0.1716,\n",
       "                       -0.1968, -0.0418, -0.2053, -0.2251, -0.2476, -0.2257, -0.2666, -0.2273,\n",
       "                       -0.2504, -0.2436, -0.2089, -0.1320, -0.2407, -0.2929, -0.2106, -0.1990,\n",
       "                       -0.2270, -0.2172, -0.1825, -0.2238, -0.1805, -0.1731, -0.2037, -0.1677,\n",
       "                       -0.2799, -0.2678, -0.2273, -0.1524, -0.3443, -0.1975, -0.2432, -0.1247,\n",
       "                       -0.2743, -0.0608, -0.2078, -0.2054, -0.1695, -0.2547, -0.1879, -0.1662,\n",
       "                       -0.1843, -0.2140, -0.0980, -0.2586, -0.1540, -0.1555, -0.2538, -0.2782,\n",
       "                       -0.2166, -0.1419, -0.1938, -0.2360, -0.1739, -0.2250, -0.2318, -0.3198,\n",
       "                       -0.2232, -0.2483, -0.2282, -0.2285, -0.1266, -0.2714, -0.2261, -0.2054,\n",
       "                       -0.2140, -0.3602, -0.2930, -0.1369, -0.2368, -0.2739, -0.1705, -0.2553,\n",
       "                       -0.2157, -0.2220, -0.1578, -0.2033, -0.1503, -0.2116, -0.1675, -0.2334,\n",
       "                       -0.2414, -0.1636, -0.1613, -0.2556, -0.2457, -0.1344, -0.1849, -0.2484,\n",
       "                       -0.2624, -0.2352, -0.2122, -0.1820, -0.2204, -0.2222, -0.1576, -0.2137,\n",
       "                       -0.2206, -0.2318, -0.0200, -0.1743, -0.2365, -0.1486, -0.2308, -0.3721,\n",
       "                       -0.1520, -0.1596, -0.1839, -0.2309, -0.1912, -0.1750, -0.2616, -0.2191,\n",
       "                       -0.2083, -0.1749, -0.2242, -0.2227, -0.2306, -0.2084, -0.2052, -0.2505,\n",
       "                       -0.1503, -0.2854, -0.2470, -0.2185, -0.1921, -0.2164, -0.2495, -0.2810,\n",
       "                       -0.2097, -0.2909, -0.1959, -0.2446, -0.1894, -0.2306, -0.2326, -0.2125,\n",
       "                       -0.2127, -0.2295, -0.2196, -0.2325, -0.1494, -0.0853, -0.2089, -0.2154,\n",
       "                       -0.1859, -0.2117, -0.1792, -0.1955, -0.2006, -0.2094, -0.2094, -0.1838,\n",
       "                       -0.3070, -0.2162, -0.3114, -0.1867, -0.2136, -0.2093, -0.0813, -0.1938,\n",
       "                       -0.1873, -0.2043, -0.2586, -0.1682, -0.2424, -0.2579, -0.1904, -0.1892,\n",
       "                       -0.2146, -0.1802, -0.2038, -0.2070, -0.1919, -0.2283, -0.2931, -0.1925,\n",
       "                       -0.2375, -0.1733, -0.0985, -0.1944, -0.2024, -0.1700, -0.2348, -0.2418,\n",
       "                       -0.2431, -0.2296, -0.1791, -0.2227, -0.1995, -0.1299, -0.1936, -0.2016,\n",
       "                       -0.2121, -0.2162, -0.1740, -0.2494, -0.2196, -0.1984, -0.1267, -0.2099,\n",
       "                       -0.2183, -0.1812, -0.2129, -0.2698, -0.2541, -0.2077, -0.2404, -0.2219,\n",
       "                       -0.2831, -0.2043, -0.1701, -0.1497, -0.2125, -0.2542, -0.1738, -0.2309,\n",
       "                       -0.2255, -0.1586, -0.2242, -0.2369, -0.2467, -0.1673, -0.2510, -0.2299,\n",
       "                       -0.2202, -0.3181, -0.1830, -0.1896, -0.2407, -0.2015, -0.2049, -0.2719,\n",
       "                       -0.2036, -0.2120, -0.2199, -0.2466, -0.1287, -0.2103, -0.2129, -0.2602,\n",
       "                       -0.1997, -0.2468, -0.1845, -0.1934, -0.2643, -0.1742, -0.2162, -0.3033,\n",
       "                       -0.2811, -0.2006, -0.2116, -0.1461, -0.1388, -0.1752, -0.1558, -0.0769,\n",
       "                       -0.2479, -0.2012, -0.2498, -0.1365, -0.2398, -0.3580, -0.2430, -0.1798,\n",
       "                       -0.2477, -0.1736, -0.1823, -0.2101, -0.3496, -0.1487, -0.2428, -0.2347,\n",
       "                       -0.1907, -0.1284, -0.0469, -0.2412, -0.2135, -0.1447, -0.2153, -0.2575,\n",
       "                       -0.1761, -0.1792, -0.2582, -0.2374, -0.3009, -0.2054, -0.2115, -0.2043,\n",
       "                       -0.3025, -0.2274, -0.2350, -0.1900, -0.2399, -0.1490, -0.2153, -0.2018,\n",
       "                       -0.1994, -0.1991, -0.3211, -0.1917, -0.1662, -0.2191, -0.2618, -0.0956,\n",
       "                       -0.1896, -0.2436, -0.2599, -0.1651, -0.2143, -0.1738, -0.2328, -0.1953,\n",
       "                       -0.2259, -0.1879, -0.2465, -0.1597, -0.2522, -0.2272, -0.1269, -0.2265,\n",
       "                       -0.1638, -0.2518, -0.2733, -0.2286, -0.3341, -0.2001, -0.2311, -0.2158,\n",
       "                       -0.1774, -0.2253, -0.2364, -0.2571, -0.1505, -0.1479, -0.1388, -0.2640,\n",
       "                       -0.1517, -0.1485, -0.1688, -0.2528, -0.2512, -0.3857, -0.2112, -0.2303,\n",
       "                       -0.2023, -0.2268, -0.1577, -0.2019, -0.1627, -0.2472, -0.0860, -0.1973,\n",
       "                       -0.0866, -0.3430, -0.2113, -0.1831, -0.1717, -0.2169, -0.2164, -0.1799,\n",
       "                       -0.2047, -0.2255, -0.1605, -0.1490, -0.1852, -0.2476, -0.0669, -0.2763,\n",
       "                       -0.2053, -0.2050, -0.2244, -0.2672, -0.2304, -0.1789, -0.2095, -0.2739,\n",
       "                       -0.2593, -0.1889, -0.1996, -0.2633, -0.1878, -0.1803, -0.2017, -0.1875,\n",
       "                       -0.1857, -0.1737, -0.3072, -0.2034, -0.2310, -0.1376, -0.2346, -0.2449],\n",
       "                      device='cuda:0')),\n",
       "              ('r8.bn2.running_mean',\n",
       "               tensor([ -5.1836,  -4.5112,  -5.7196,  -6.4172,  -5.4284,  -5.6580,  -6.9517,\n",
       "                        -4.8076,  -6.3457,  -4.8449,  -5.9913,  -3.8876,  -4.6255,  -6.4319,\n",
       "                        -3.9476,  -6.1639,  -8.2473,  -6.5363,  -6.6418,  -5.5731,  -3.9885,\n",
       "                        -7.4955,  -5.4213,  -6.8974,  -6.0186,  -5.8802,  -5.3298,  -5.7403,\n",
       "                        -7.8201,  -5.7433,  -5.9839,  -7.0124,  -1.5435,  -4.6092,  -6.5439,\n",
       "                        -6.2595,  -6.6304,  -6.4544,  -6.7314,  -5.6314,  -5.7752,  -5.4344,\n",
       "                        -6.2649,  -4.9748,  -4.9256,  -5.2414,  -5.4486,  -5.2124,  -6.7946,\n",
       "                        -5.5852,  -5.5827,  -7.0901,  -3.0323,  -4.5416,  -6.0490,  -6.1719,\n",
       "                        -5.3312,  -4.6455,  -5.9517,  -5.8689,  -7.1117,  -4.7304,  -6.0749,\n",
       "                        -5.1702,  -4.3774,  -5.4075,  -4.1639,  -5.5822,  -7.5350,  -6.1630,\n",
       "                        -6.8988,  -6.2545,  -5.2492,  -4.2483,  -6.7665,  -5.7375,  -4.9404,\n",
       "                        -5.7645,  -4.9468,  -7.3550,  -5.0091,  -6.4306,  -4.7734,  -5.4311,\n",
       "                        -6.9573,  -6.5124,  -3.1291,  -4.8643,  -6.4010,  -5.6220,  -7.3851,\n",
       "                        -7.1020,  -5.0861,  -6.7830,  -6.8185,  -5.9885,  -5.4479,  -3.4518,\n",
       "                        -5.7995,  -4.6732,  -4.9150,  -6.5764,  -5.8763,  -6.1339,  -3.8327,\n",
       "                        -7.2835,  -5.6652,  -4.0898,  -6.2277,  -4.9226,  -5.6061,  -4.7245,\n",
       "                        -6.0558,  -6.2591,  -5.5531,  -4.4359,  -5.1658,  -5.9710,  -5.4528,\n",
       "                        -7.4549,  -6.3989,  -3.6777,  -3.8193,  -4.9814,  -5.5491,  -7.8396,\n",
       "                        -5.8650,  -3.3598,  -6.6394,  -5.2382,  -6.9901,  -4.6058,  -5.9099,\n",
       "                        -7.3980,  -4.3851,  -3.8924,  -5.8301,  -4.8308,  -4.1420,  -6.7289,\n",
       "                        -5.8922,  -5.3211,  -6.2445,  -7.1146,  -7.4828,  -6.9253,  -5.5264,\n",
       "                        -6.8088,  -6.9280,  -6.9656,  -3.7806,  -7.7560,  -5.6202,  -4.8222,\n",
       "                        -4.8616,  -5.2653,  -6.3041,  -6.2298,  -7.8694,  -6.1176,  -5.3748,\n",
       "                        -4.2328,  -3.9093,  -4.4604,  -7.1808,  -4.8233,  -5.5492,  -4.6710,\n",
       "                        -5.1717,  -6.4658,  -4.4352,  -4.9796,  -4.6705,  -5.3954,  -6.6868,\n",
       "                        -4.4971,  -5.3201,  -6.8177,  -5.1673,  -4.1847,  -7.7040,  -6.1338,\n",
       "                        -4.4425,  -6.9827,  -5.2326,  -5.6697,  -7.1224,  -6.1435,  -6.6747,\n",
       "                        -6.2672,  -5.1555,  -8.1842,  -4.1603,  -5.3543,  -4.5953,  -7.7227,\n",
       "                        -5.4585,  -5.9320,  -5.9775,  -4.2597,  -5.3187,  -5.2483,  -7.7966,\n",
       "                        -5.2457,  -5.7245,  -5.9384,  -5.4558,  -7.1155,  -6.6222,  -4.7147,\n",
       "                        -4.4797,  -8.8890,  -6.2079,  -4.0818,  -6.3428,  -6.6419,  -5.8920,\n",
       "                        -7.7270,  -7.0831,  -5.6595,  -3.7391,  -6.9906,  -5.7668,  -6.1284,\n",
       "                       -10.4672,  -6.1168,  -5.6889,  -4.9572,  -6.6161,  -6.2829,  -4.5283,\n",
       "                        -4.3831,  -4.4826,  -6.1975,  -6.7176,  -5.2996,  -3.8634,  -6.1208,\n",
       "                        -5.2110,  -4.6404,  -3.3989,  -4.2962,  -6.0679,  -5.6665,  -6.2349,\n",
       "                        -4.8788,  -7.2998,  -6.5805,  -4.0227,  -4.6523,  -5.4051,  -4.5749,\n",
       "                        -6.5164,  -6.0776,  -4.8452,  -2.9295,  -4.5797,  -4.5963,  -5.9736,\n",
       "                        -6.3057,  -6.5888,  -3.8726,  -5.5123,  -5.8037,  -7.4359,  -6.8413,\n",
       "                        -5.5942,  -5.6633,  -5.5412,  -6.1896,  -3.7952,  -5.8182,  -5.3296,\n",
       "                        -5.1352,  -6.7106,  -5.5626,  -5.5820,  -5.2493, -10.0128,  -4.6243,\n",
       "                        -4.5209,  -7.5753,  -6.7291,  -5.1582,  -5.6639,  -5.1375,  -5.0443,\n",
       "                        -5.9644,  -7.6016,  -6.8863,  -4.8182,  -6.5497,  -4.1522,  -4.5521,\n",
       "                        -6.3459,  -6.1585,  -6.6793,  -5.6993,  -6.6600,  -7.2092,  -6.2608,\n",
       "                        -4.7775,  -6.3913,  -4.0894,  -5.5640,  -5.4018,  -5.0924,  -5.0017,\n",
       "                        -3.4725,  -4.5171,  -5.5993,  -6.2587,  -5.7346,  -6.3685,  -6.1721,\n",
       "                        -4.3581,  -7.3849,  -7.3356,  -6.5768,  -5.7556,  -5.1817,  -4.0248,\n",
       "                        -7.0597,  -6.1731,  -6.5167,  -6.0104,  -4.9691,  -6.0818,  -5.5608,\n",
       "                        -5.5506,  -7.9539,  -3.6613,  -5.4681,  -4.5325,  -5.0868,  -2.6292,\n",
       "                        -5.9394,  -7.7265,  -5.0745,  -5.8483,  -5.8253,  -5.4705,  -5.5157,\n",
       "                        -5.9859,  -6.5265,  -5.9662,  -4.3832,  -5.7308,  -5.4196,  -5.2056,\n",
       "                        -3.8321,  -4.3054,  -4.2190,  -4.2672,  -5.7880,  -4.9554,  -7.2780,\n",
       "                        -4.9969,  -4.4639,  -8.4460,  -7.6573,  -6.1472,  -6.4703,  -5.1476,\n",
       "                        -3.8448,  -6.8356,  -6.2655,  -6.5990,  -4.7005,  -3.7106,  -6.4406,\n",
       "                        -8.5958,  -5.0895,  -2.3547,  -8.4868,  -2.9411,  -5.1301,  -6.1861,\n",
       "                        -4.8382,  -5.7029,  -7.5156,  -6.7867,  -4.6130,  -5.0437,  -2.8135,\n",
       "                        -8.5240,  -5.4124,  -8.4274,  -6.2428,  -3.6367,  -6.3130,  -4.4221,\n",
       "                        -4.7235,  -7.1230,  -9.8504,  -3.3495,  -6.3950,  -6.6204,  -6.2431,\n",
       "                        -3.6867,  -8.3726,  -4.7729,  -5.6016,  -3.2270,  -7.0428,  -7.3842,\n",
       "                        -4.3204,  -4.3191,  -4.0948,  -7.0168,  -3.7931,  -6.5321,  -6.0740,\n",
       "                        -6.0613,  -7.3516,  -8.6211,  -5.7867,  -4.5157,  -4.8645,  -6.2505,\n",
       "                        -4.0046,  -7.5642,  -4.4495,  -7.6239,  -4.0964,  -4.4949,  -4.9117,\n",
       "                        -5.4928,  -5.0958,  -7.2947,  -6.2065,  -6.3866,  -7.4987,  -4.2251,\n",
       "                        -4.2795,  -3.8730,  -4.6670,  -6.3623,  -6.5103,  -6.4038,  -7.8397,\n",
       "                        -5.6433,  -3.5791,  -6.8111,  -5.1052,  -6.4993,  -6.4694,  -5.8332,\n",
       "                        -4.7195,  -6.2913,  -4.8019,  -5.3383,  -3.7328,  -5.4425,  -4.2352,\n",
       "                        -5.4326,  -8.3245,  -8.3761,  -5.3556,  -3.3513,  -6.2610,  -4.8026,\n",
       "                        -5.6742,  -5.7739,  -6.1127,  -7.2061,  -5.8609,  -6.9802,  -6.0276,\n",
       "                        -4.8277,  -3.7418,  -5.5725,  -8.8509,  -5.1795,  -2.7944,  -6.2993,\n",
       "                        -5.9497,  -5.8053,  -4.7726,  -7.2557,  -6.6159,  -4.6534,  -3.0456,\n",
       "                        -4.1010,  -5.8922,  -5.9174,  -7.9059,  -4.2242,  -3.7361,  -6.2118,\n",
       "                        -4.9173,  -4.4166,  -5.3532,  -6.3965,  -4.3730,  -6.1153,  -4.0547,\n",
       "                        -8.1220,  -2.5655,  -7.1954,  -6.2835,  -5.1957,  -5.3892,  -5.2161,\n",
       "                        -5.8657,  -5.4314,  -6.3379,  -4.6887,  -4.5556,  -4.7973,  -7.2520,\n",
       "                        -3.9350], device='cuda:0')),\n",
       "              ('r8.bn2.running_var',\n",
       "               tensor([328.0545, 291.7687, 608.9241, 409.3234, 417.2599, 509.1194, 383.6814,\n",
       "                       329.4090, 558.6397, 524.0566, 450.3594, 690.1361, 507.5893, 523.1523,\n",
       "                       386.6892, 610.4067, 504.8336, 428.2565, 524.3994, 573.2056, 628.8031,\n",
       "                       538.5467, 573.4734, 489.8781, 554.6646, 658.8429, 493.8750, 560.7995,\n",
       "                       593.4073, 480.3795, 570.2225, 640.0502, 427.7451, 341.8981, 582.6226,\n",
       "                       485.8006, 558.3060, 495.2189, 619.4293, 394.0326, 512.8262, 495.1021,\n",
       "                       402.9503, 527.9790, 428.8868, 415.1307, 371.6808, 432.2257, 278.2359,\n",
       "                       377.2675, 343.1864, 479.2538, 657.7733, 504.8877, 522.0745, 291.4204,\n",
       "                       491.5344, 432.5414, 559.3687, 472.9395, 539.8458, 524.8475, 478.6380,\n",
       "                       363.0855, 361.0045, 305.5228, 501.1838, 451.7402, 501.2766, 456.4919,\n",
       "                       621.6540, 419.5003, 499.3279, 727.5064, 514.6778, 453.7002, 599.3898,\n",
       "                       442.0991, 502.6451, 530.2167, 499.8754, 392.8941, 605.6376, 414.1053,\n",
       "                       570.0092, 578.1177, 569.1568, 364.3366, 657.3987, 422.7654, 555.9257,\n",
       "                       399.9475, 391.0423, 422.3058, 617.7818, 544.5428, 415.6184, 574.3323,\n",
       "                       425.8949, 390.5588, 572.4059, 429.7138, 531.1835, 443.6660, 520.7445,\n",
       "                       377.4321, 405.3902, 472.8151, 451.7863, 387.7868, 590.2565, 456.7329,\n",
       "                       415.9197, 609.3835, 676.7657, 285.1044, 470.5437, 456.5629, 530.1826,\n",
       "                       476.1673, 478.7274, 912.6967, 454.2416, 478.0485, 305.8553, 418.6638,\n",
       "                       319.9594, 410.2064, 557.8006, 369.1388, 499.8571, 714.4471, 458.3689,\n",
       "                       455.1496, 485.1941, 517.4104, 482.2866, 319.1355, 640.7552, 477.2120,\n",
       "                       554.1779, 637.8150, 500.1880, 505.4124, 640.4592, 362.4854, 493.2014,\n",
       "                       550.7516, 305.7683, 468.1672, 352.8760, 735.7812, 382.3352, 657.1899,\n",
       "                       572.0172, 504.0909, 684.1107, 315.3064, 548.1816, 589.4045, 572.1671,\n",
       "                       429.1476, 498.4040, 413.3981, 520.9872, 701.7536, 513.0289, 386.2344,\n",
       "                       367.4077, 629.0825, 417.9496, 422.3185, 722.2501, 318.7248, 586.5115,\n",
       "                       374.4370, 387.2000, 619.3631, 389.9908, 440.3430, 694.3392, 468.5544,\n",
       "                       469.4401, 554.9498, 468.9234, 275.3661, 521.6617, 415.4643, 532.4436,\n",
       "                       433.6828, 431.1872, 527.1877, 379.1786, 506.0207, 527.9293, 487.4954,\n",
       "                       511.5720, 379.3246, 609.9691, 570.1558, 475.8958, 581.3646, 527.0472,\n",
       "                       415.1771, 527.8893, 645.2538, 501.7328, 382.2711, 434.5998, 381.0581,\n",
       "                       614.9180, 541.8239, 392.7699, 495.6790, 533.1276, 370.7063, 598.8256,\n",
       "                       436.6855, 768.4417, 536.0320, 419.5072, 511.0013, 441.0956, 288.7851,\n",
       "                       702.4689, 452.2234, 584.0422, 462.1197, 575.1411, 476.5655, 506.0428,\n",
       "                       437.6641, 514.4343, 471.3387, 455.6134, 409.5515, 599.9758, 528.4229,\n",
       "                       470.3148, 353.6521, 562.9548, 418.5731, 398.6251, 573.6127, 617.8965,\n",
       "                       360.5784, 536.4703, 360.9231, 536.3719, 378.1385, 517.4979, 437.3262,\n",
       "                       613.6334, 522.5267, 550.6047, 340.7344, 471.3860, 371.0635, 462.9813,\n",
       "                       466.7771, 536.7280, 521.4172, 522.6498, 585.4716, 650.2234, 510.0725,\n",
       "                       614.0395, 451.4978, 396.0071, 724.8596, 440.1936, 643.4783, 304.2822,\n",
       "                       584.8997, 400.1396, 606.0007, 405.1060, 427.1352, 944.5521, 575.6376,\n",
       "                       582.3691, 639.5911, 404.6632, 412.9613, 469.1678, 498.3678, 590.9844,\n",
       "                       523.4683, 598.4677, 458.5849, 404.2194, 772.6328, 556.9894, 514.0311,\n",
       "                       318.2181, 591.4057, 353.1644, 416.9071, 882.0217, 508.2563, 564.2180,\n",
       "                       449.9843, 594.0877, 421.9963, 386.4683, 389.8472, 467.2616, 498.8513,\n",
       "                       454.4903, 567.9528, 495.0689, 514.8212, 561.1758, 351.2764, 493.5232,\n",
       "                       411.7061, 297.3281, 520.2990, 714.6652, 604.6057, 455.7321, 617.9519,\n",
       "                       407.3710, 352.5768, 494.4083, 520.7857, 540.2049, 387.9706, 341.0816,\n",
       "                       436.6201, 694.1578, 465.2033, 385.4477, 506.3587, 409.3831, 478.4998,\n",
       "                       451.0021, 551.8364, 341.4911, 439.0865, 448.1676, 541.1558, 453.2853,\n",
       "                       517.4869, 487.6200, 364.3451, 395.5118, 601.3389, 415.3816, 524.0420,\n",
       "                       460.2625, 341.6231, 494.9698, 599.0070, 382.5174, 401.8641, 722.1563,\n",
       "                       522.9214, 465.1320, 602.1616, 394.8777, 420.1605, 477.8341, 569.8289,\n",
       "                       439.9973, 480.4513, 419.2728, 304.8925, 354.3926, 570.6727, 464.5312,\n",
       "                       592.2379, 464.9184, 525.9778, 706.0862, 757.6746, 412.4339, 472.7732,\n",
       "                       526.4365, 514.2917, 460.3405, 351.5811, 466.4169, 580.7425, 347.0457,\n",
       "                       601.9837, 480.1417, 607.6157, 344.5596, 469.0698, 508.4988, 638.8843,\n",
       "                       618.7141, 673.7309, 728.6080, 428.5836, 501.5107, 651.2908, 496.0301,\n",
       "                       279.4420, 578.4794, 571.2653, 331.3639, 391.0900, 428.2986, 399.7860,\n",
       "                       480.2527, 542.2590, 343.9397, 488.7677, 261.9111, 435.0064, 389.2820,\n",
       "                       499.8220, 587.7535, 614.8113, 434.9720, 449.4704, 327.7643, 516.3019,\n",
       "                       612.1254, 417.4497, 391.8964, 973.8941, 460.5158, 340.7251, 415.9420,\n",
       "                       621.9073, 552.0272, 531.8318, 623.1822, 426.4910, 560.3524, 414.3496,\n",
       "                       445.6655, 703.5871, 554.4198, 342.0728, 741.9971, 497.6689, 622.9283,\n",
       "                       350.0409, 381.4326, 365.6564, 249.7137, 614.5142, 453.9371, 578.4828,\n",
       "                       614.0524, 447.1062, 394.5624, 414.8481, 664.4105, 525.0844, 518.0728,\n",
       "                       570.2784, 651.0499, 592.4200, 444.0931, 477.2480, 553.1876, 184.6596,\n",
       "                       405.0737, 486.3943, 516.7484, 517.4656, 447.8467, 608.7318, 710.2061,\n",
       "                       349.9781, 718.9827, 647.7476, 999.9139, 353.0476, 400.5225, 549.0610,\n",
       "                       711.1807, 477.1328, 334.6142, 478.6403, 548.7012, 421.2659, 635.8660,\n",
       "                       583.4224, 540.4949, 490.8026, 875.3700, 416.0492, 493.5534, 601.2094,\n",
       "                       360.5869, 423.0896, 503.7880, 435.6240, 462.1949, 406.4267, 357.6414,\n",
       "                       640.5357, 428.3998, 524.2080, 783.9828, 421.6764, 488.3652, 499.6098,\n",
       "                       446.9210, 503.8035, 349.8086, 549.0092, 699.8232, 521.3923, 542.8693,\n",
       "                       507.0591], device='cuda:0')),\n",
       "              ('r8.bn2.num_batches_tracked', tensor(29400, device='cuda:0')),\n",
       "              ('r8.c2.weights',\n",
       "               tensor([[[[ 4.2635e-02,  2.4718e-02,  2.4598e-02],\n",
       "                         [-1.5438e-03,  1.4237e-03, -2.4654e-02],\n",
       "                         [ 1.9603e-02, -1.1174e-03,  3.1402e-02]],\n",
       "               \n",
       "                        [[ 6.3071e-03,  6.1493e-04,  9.6222e-03],\n",
       "                         [-3.0255e-03, -2.9173e-02, -3.5483e-02],\n",
       "                         [-2.3239e-02, -1.5535e-02, -3.8881e-02]],\n",
       "               \n",
       "                        [[ 1.4005e-02, -3.0011e-02,  1.4254e-03],\n",
       "                         [-2.6919e-02, -5.0407e-02, -4.0685e-02],\n",
       "                         [-2.3535e-02, -3.4863e-02, -3.1429e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1521e-02, -1.9308e-02, -1.1350e-02],\n",
       "                         [-5.1061e-02, -4.1830e-02, -2.7785e-02],\n",
       "                         [-4.0321e-02, -3.9896e-02, -6.1448e-02]],\n",
       "               \n",
       "                        [[ 9.7217e-02,  1.0454e-01,  1.0841e-01],\n",
       "                         [ 4.8332e-02,  3.2110e-02,  1.2395e-01],\n",
       "                         [ 6.2809e-02,  3.7451e-02,  9.4862e-02]],\n",
       "               \n",
       "                        [[ 2.1599e-02,  6.6178e-03,  1.3020e-02],\n",
       "                         [-2.3948e-02, -9.9153e-03, -3.9581e-03],\n",
       "                         [-1.9226e-02, -2.1273e-02,  1.6231e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.2921e-03, -1.7476e-02, -3.3287e-03],\n",
       "                         [-1.9189e-02, -1.7534e-03, -2.7545e-02],\n",
       "                         [ 1.9240e-02, -3.7843e-02, -3.1465e-02]],\n",
       "               \n",
       "                        [[ 4.6105e-02, -6.1640e-04,  1.4666e-02],\n",
       "                         [ 1.6457e-02, -4.9211e-03,  3.7754e-03],\n",
       "                         [ 2.1793e-02,  2.2776e-02,  3.0324e-02]],\n",
       "               \n",
       "                        [[-2.7772e-02, -2.4345e-02, -4.7451e-02],\n",
       "                         [-3.8022e-02, -2.5144e-02, -4.7266e-02],\n",
       "                         [-3.0502e-02, -4.3000e-02, -1.0949e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.3149e-02,  1.8958e-02,  1.7979e-02],\n",
       "                         [ 1.4504e-02,  2.0866e-02, -1.9781e-02],\n",
       "                         [ 6.3606e-03,  1.7053e-02, -4.1983e-02]],\n",
       "               \n",
       "                        [[ 2.5656e-02,  8.7609e-03,  4.3282e-02],\n",
       "                         [ 8.9924e-03,  2.0751e-02,  4.6581e-02],\n",
       "                         [ 2.4260e-02,  1.8111e-02,  9.0879e-03]],\n",
       "               \n",
       "                        [[ 2.7309e-02, -1.0057e-03,  2.8782e-02],\n",
       "                         [ 2.2217e-03,  1.1030e-03, -8.9973e-03],\n",
       "                         [ 2.1777e-02, -6.1173e-03,  8.3034e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.6869e-03,  2.0886e-03,  1.9897e-02],\n",
       "                         [ 1.0037e-02, -2.5798e-03, -1.8738e-02],\n",
       "                         [-1.8111e-03,  1.8179e-02,  1.9024e-02]],\n",
       "               \n",
       "                        [[ 3.5891e-02,  6.6298e-03,  4.0498e-02],\n",
       "                         [ 2.7037e-02, -2.0694e-02,  2.8803e-02],\n",
       "                         [ 4.5099e-02, -1.0111e-02, -9.0007e-04]],\n",
       "               \n",
       "                        [[-7.9267e-03, -1.7675e-03,  1.2593e-02],\n",
       "                         [-7.2949e-02, -1.1542e-02, -4.5870e-02],\n",
       "                         [-5.7096e-02, -3.3859e-02, -3.5668e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.1002e-02,  4.5358e-02,  1.7457e-02],\n",
       "                         [ 3.5448e-02,  1.2568e-03,  5.9534e-03],\n",
       "                         [ 1.9632e-02,  1.4137e-02, -5.3423e-03]],\n",
       "               \n",
       "                        [[ 2.2630e-02, -1.4764e-03,  5.4695e-02],\n",
       "                         [ 1.2669e-02,  8.8349e-03, -5.6370e-04],\n",
       "                         [ 1.3549e-02,  1.2032e-02,  3.7990e-02]],\n",
       "               \n",
       "                        [[-7.3481e-02, -2.7870e-02, -3.3076e-02],\n",
       "                         [-3.4401e-02, -3.2352e-03, -3.8740e-02],\n",
       "                         [-5.0468e-02, -2.9487e-02, -4.2657e-04]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 9.1707e-03, -8.5870e-03,  6.3141e-03],\n",
       "                         [-1.7725e-02,  1.0969e-02, -1.8716e-02],\n",
       "                         [ 2.1294e-02,  9.7740e-03, -5.4539e-03]],\n",
       "               \n",
       "                        [[-1.2853e-02, -2.7099e-02,  9.8180e-05],\n",
       "                         [-2.8656e-02, -2.1940e-02,  1.0752e-02],\n",
       "                         [-3.8211e-02, -2.8057e-02,  5.7675e-03]],\n",
       "               \n",
       "                        [[ 4.3078e-02,  1.2313e-02, -1.1172e-03],\n",
       "                         [ 7.0527e-02,  2.8051e-02,  3.9544e-02],\n",
       "                         [ 5.4971e-02,  9.4890e-03,  4.0073e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.5185e-02, -3.0024e-02, -3.0135e-02],\n",
       "                         [-3.0931e-02, -1.7243e-02,  1.0616e-02],\n",
       "                         [ 1.1371e-02, -3.1751e-03, -1.6690e-02]],\n",
       "               \n",
       "                        [[ 4.1921e-02,  1.7491e-03,  3.3254e-02],\n",
       "                         [ 7.2201e-03,  1.9962e-02,  4.5821e-02],\n",
       "                         [ 1.7139e-03,  1.0384e-02,  4.2490e-02]],\n",
       "               \n",
       "                        [[ 7.6054e-02,  4.3488e-02,  1.6623e-02],\n",
       "                         [ 5.2209e-02,  2.3424e-02,  4.3718e-02],\n",
       "                         [ 6.8602e-02,  9.2867e-04,  2.7463e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.0714e-02,  2.4505e-02,  2.3147e-02],\n",
       "                         [-8.0055e-03,  3.4150e-02,  1.2817e-02],\n",
       "                         [-2.1995e-02, -1.0151e-02,  3.9185e-03]],\n",
       "               \n",
       "                        [[ 1.4814e-02,  1.9346e-02,  2.4043e-02],\n",
       "                         [ 5.5672e-02,  2.7334e-02,  6.8632e-03],\n",
       "                         [ 4.8197e-02,  2.7100e-02,  2.5042e-02]],\n",
       "               \n",
       "                        [[ 6.5885e-02,  8.9273e-02,  7.6056e-02],\n",
       "                         [ 5.4036e-02,  5.2297e-02,  6.7516e-02],\n",
       "                         [ 4.2010e-02,  3.5428e-02,  3.0608e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.3694e-02,  8.5624e-03, -9.9047e-03],\n",
       "                         [ 2.0420e-02,  2.0057e-02, -2.5436e-02],\n",
       "                         [-2.8810e-02,  1.6563e-02,  1.4430e-02]],\n",
       "               \n",
       "                        [[ 1.5635e-03,  7.1116e-03, -6.7845e-03],\n",
       "                         [-7.3724e-03, -2.7338e-02,  4.2606e-04],\n",
       "                         [ 1.9131e-03,  4.7864e-03,  8.0991e-03]],\n",
       "               \n",
       "                        [[-1.0099e-01, -6.0691e-02, -2.7498e-02],\n",
       "                         [-8.1990e-02, -1.3957e-02, -4.3794e-02],\n",
       "                         [-4.9868e-02, -1.2228e-02, -2.4552e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.9058e-02, -2.0710e-02, -1.5325e-03],\n",
       "                         [-1.1460e-02,  3.1930e-02,  3.0804e-02],\n",
       "                         [-2.3554e-02,  2.2699e-03,  5.9611e-02]],\n",
       "               \n",
       "                        [[ 3.2315e-02,  1.0956e-02,  6.2653e-02],\n",
       "                         [ 1.9994e-02,  4.9081e-02,  4.9163e-02],\n",
       "                         [ 4.2336e-02,  3.2138e-02,  7.2155e-02]],\n",
       "               \n",
       "                        [[-1.7205e-02, -3.6484e-02, -1.9441e-02],\n",
       "                         [-6.7195e-02, -4.2051e-02, -5.4551e-02],\n",
       "                         [-7.8141e-02, -4.4484e-02, -8.4903e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.9254e-02,  1.7652e-02,  8.5190e-03],\n",
       "                         [ 2.7558e-02, -5.9026e-03, -1.7296e-02],\n",
       "                         [ 1.6259e-02, -1.4693e-02,  3.1985e-02]],\n",
       "               \n",
       "                        [[-3.8078e-02, -5.3336e-02, -8.6413e-02],\n",
       "                         [-2.3951e-02, -2.7592e-02, -5.2656e-02],\n",
       "                         [-1.6062e-02, -4.0593e-02, -8.8962e-02]],\n",
       "               \n",
       "                        [[-9.7193e-02, -5.9980e-02, -5.1579e-02],\n",
       "                         [-5.6945e-02, -4.7218e-02, -5.5121e-02],\n",
       "                         [-9.5016e-02, -2.1842e-02, -3.2874e-02]]]], device='cuda:0')),\n",
       "              ('r8.c2.mask',\n",
       "               tensor([[[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]],\n",
       "               \n",
       "               \n",
       "                       [[[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]],\n",
       "               \n",
       "                        [[1., 1., 1.],\n",
       "                         [1., 1., 1.],\n",
       "                         [1., 1., 1.]]]], device='cuda:0')),\n",
       "              ('d1.weights',\n",
       "               tensor([[ 0.0354,  0.0370,  0.0198,  ...,  0.0119, -0.0074,  0.0042],\n",
       "                       [ 0.0129,  0.0259,  0.0175,  ..., -0.0065, -0.0162,  0.0227],\n",
       "                       [-0.0192,  0.0027, -0.0098,  ...,  0.0055, -0.0163, -0.0042],\n",
       "                       ...,\n",
       "                       [ 0.0336,  0.0124,  0.0293,  ...,  0.0076,  0.0148,  0.0111],\n",
       "                       [ 0.0144,  0.0146,  0.0098,  ...,  0.0363,  0.0332,  0.0238],\n",
       "                       [-0.0247,  0.0143, -0.0149,  ...,  0.0058, -0.0114,  0.0182]],\n",
       "                      device='cuda:0')),\n",
       "              ('d1.mask',\n",
       "               tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                       [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                       [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                       ...,\n",
       "                       [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                       [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "                       [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "              ('d1.bias',\n",
       "               tensor([-0.0390, -0.0182,  0.0042,  0.0654,  0.0189,  0.0027,  0.0162, -0.0377,\n",
       "                       -0.0114,  0.0291], device='cuda:0'))])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./saved-models/cifar-resnet-fast-100-epochs.pth')\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.06, 0.0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_clean_accuracy(model,test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0414465970>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = next(iter(test_loader))[0][1]\n",
    "plt.imshow(x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f041419ec40>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = f_adv[0][1].cpu().detach()\n",
    "plt.imshow(f.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0414650160>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAexElEQVR4nO2da4xlV5Xf/+u+H/W4Vbe6uqtf7nZjYszLRh3HMxBC8DBxEJFhFDkQCfmDMx5FgxSkyQeLSIFI+cBEAcSHiKgZrPFMCOCACU5kJYBnJItEMbSh3W67B7ttt93drn5V1a131X2tfKjbUdvZ/13lrq5bDfv/k1p9a6+7z9lnn7POuXf/71rL3B1CiN9+Mts9ACFEf5CzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJkNtMZzO7B8DXAWQB/Jm7fzn2/tGxUd+7fy8ZSGwoHdKa5WMjfXpGTrMbGUVYpuzm+dhnLk1RWzeieo6OjFJbTC1tNGaC7YvLy7RPa5nPVbu1QG3dNh9IrlwKto/V+HEtrSxR28LiHN9Xjl8H8PD5bDebtEu+wM9noVSgtmyGjyMTuVaZ/O1k7ADQyYUv4qX5JTSXm0HjNTu7mWUB/AcAHwNwFsAvzOxxd3+B9dm7fy+eeOqJoK2erdN9uc8G2+dtmPbJdsIXPQB0yUQBQPYN7hQNWw22L4zxsT/2Z/+J2pbb/IPVfX/wGWrrtvhF8F//26PB9qO/OkH7nDsRnl8AuHT+f1Pb4qUWte1457uC7Q988p/SPr964Si1/eznP6W2nTuHqM1b4RvI1OmztM+um/gNad8t+6htqMr7lbtVamt12qQ9fL0BwHw9fA0/9ejPaJ/NfIy/E8Apd3/F3ZsAvgvg3k1sTwixhWzG2fcAOHPV32d7bUKIG5AtX6AzswfN7KiZHZ2+PL3VuxNCEDbj7OcAXP0FZm+v7U24+xF3P+zuh0fH+HcaIcTWshln/wWAW8zsoJkVAHwawOPXZ1hCiOvNNa/Gu3vbzD4H4H9iTXp72N2fj/XJdoGBpfDKY6fE5Z+OhVcyW8b7NJ3bBsLKBACgO8TlpPZ8eBz1Lr9n2hLfXnOKf6258MYktdUj9+guWSEvLXLJaPQmPh8rK3yl+7137Ke2f/KP/lmwfWRXWHoFgOPPHaO2pTNcXZlcvkRtw4P5YHvHuOpy9tn/7wPq/2NukUuRO4pj1FYsFaltcXk+2J4t8PNSO7A72O6tsH8Bm9TZ3f0JAGEtTQhxQ6Ff0AmRCHJ2IRJBzi5EIsjZhUgEObsQibCp1fi3jQFWDMsJjTkehTTsi8H22giXJpZshNpWZnkEVcl4dNL4WFg+aeV4IEw+E5Z+AGBxjE//nokytWV5HAwymfA8Nlf5MR/ct4va/uCe36G2j979UWprTIWP++QJHoBiNX7MhVH+S+zO6mlqm18MB5MscyUM7Qo/L4uv86ChyQy35bnSh8Hh8DN3ZMcA7TPQrAXbY5FyerILkQhydiESQc4uRCLI2YVIBDm7EInQ39X4CIUazz8GEgPRQI12qTX45gw8uGO1FMkZ1w2vdM9FAnIGBsO52AAgyxemYeApt9olng4KhUqwebHA7+u7Dx+itnvuuZvasuAqxOsrF4LtzSYf+7xx5aI8xOd4aPRWapsiKZ8Wjj9H+5ScB5MgooTEUht2wqcFALBCVtBjuQE7TZK3LqbUcJMQ4rcJObsQiSBnFyIR5OxCJIKcXYhEkLMLkQg3jPTWJD/sB4ByLay91bpcZ+hm+H2sE5EnlnM8gKacIeLKLJdIxg8eoLZmgecsK0dKEHVX+LFNDIRlxX/wux/j+yLBSQDw/cd+TG0f/DufoLYuCURqDoXzrQFAbpkHwqDJ5yNnvN/v3nog2P7GPI+E+fUpWtQIc4jleFvhtoiy3CHT3y7yC7VNqgKxUlKAnuxCJIOcXYhEkLMLkQhydiESQc4uRCLI2YVIhE1Jb2Z2GsA8gA6AtrsfvtZt1ZYa1LZKAsCKc1zqmCPRXwBgTa6DDGd4VJaT9HRjXS5d3f3hu6itQ+QTAPAuH+OlOW47cEu4vNLJeS55HX3q59Q2XuMSVYZJkQBKtXD+tPkzPMdfNselyPl8RIp8iZdrunkinF9vbFe4lBcAzF7kNszx8k/cEo+IaxFjq8tz0LVb5NqPSG/XQ2f/++5++TpsRwixhehjvBCJsFlndwA/NrNnzOzB6zEgIcTWsNmP8R9y93NmNg7gJ2b2N+7+1NVv6N0EHgSAfft47m8hxNayqSe7u5/r/X8RwA8B3Bl4zxF3P+zuh+v10c3sTgixCa7Z2c2samaDV14D+H0AJ67XwIQQ15fNfIzfCeCHZnZlO//Z3f9HrEMHQIOoTTzWDECDtA/zklG1SDmphhWobaUTkS5y4X6LPDckbLZBbYUCH+PrZ3m5psIqlwd3HNodbH/5xK9pn327eFLM2miN2gar4dJKANAqh/Wk1W7k+VLjCScdfIzns1x6e+2NS8H2YeeybWaRz/0AeB0n3guRXkCOqMQt4xKrZ3aE242LfNfs7O7+CoD3X2t/IUR/kfQmRCLI2YVIBDm7EIkgZxciEeTsQiRCXxNOZgAMkNtLpsb7FZgaFtMzeDk31Fb4YVtEDst4WPIqRiKNfIDbVs7zRJWlCo82e2XyDLXVy2Hp7UN7+K8Xj5HjAoByjkepDRV4osfpqfCxZSPxX9nsIrXV8oPUttDhUWpn5qeD7TMZHnE4S2r6AQAXB4GIOohSpHxchti8xec+kwlfwxaZXz3ZhUgEObsQiSBnFyIR5OxCJIKcXYhE6OtqfBc8Txdfe+Z4ma90I1Jup1Hiq77ZBr//ZWphm4OPoxtZqa+M89Xs8pkpaitE1oRHx8Kr8YUyX32utCf59lpc1sgWIsvP3Ua4D/jcFyLPnkJhlu+rfYGaFhrh3HulLJ/7pnNbK5JpLsNjayJFo4AuMdbyfK5iOe0YerILkQhydiESQc4uRCLI2YVIBDm7EIkgZxciEfoqvUXhsQdoEKUpEusS1Tqqi1y4KHCFik4WD2cBOhGRxCK32kI9XLYIAHZF5mqlGj6ABecTUu3yJHq1AzVqy0UCYbLFcH66aofrU+NdLuWVihFbl5+B/NJMsH06Mh8rsUJOEbUxdg0XedpDkJgWWC0S1DJL8v91+AWsJ7sQiSBnFyIR5OxCJIKcXYhEkLMLkQhydiESYV3pzcweBvAJABfd/T29tlEA3wNwAMBpAPe5e1jjuIosgBFSnsaWucxQy4cjx2LldvJlHkeXWeZSzUIxEkk3E7Z1IyFIuYh8suJc8lrI8Uiu58+8QW3Nk68F21tD4XJBAFAf4rnOBkcGqM0HuPRWJbnfYhGC+TyP5iuVYnGR3NbpMD2MXwOVMqnHBGDJI9psgcuKsXSJZaL0WSYivd0UPmeW3VwOuj8HcM9b2h4C8KS73wLgyd7fQogbmHWdvVdv/a236XsBPNJ7/QiAT17fYQkhrjfX+p19p7tfyXhwHmsVXYUQNzCbXqBzdwf4FzEze9DMjprZ0amp8Pc4IcTWc63OfsHMJgCg9/9F9kZ3P+Luh939cL0+eo27E0Jslmt19scB3N97fT+AH12f4QghtoqNSG/fAfARAGNmdhbAFwF8GcCjZvYAgNcA3LeRnXUANIitXot0JGpHLACpmIuUcapxyStjPOLJ2mFZYzUbuWcu8SlerHBBJt/iYVJP/Z9nqO3YM88F2+94z/ton9/79O9RW6XCo++6kcOeboRlufZcjfZpFrl6WyhxOQyZSMJPknh0pRiR0CySrTSSjLISuSKXiOQMAFYNXwc+EpPR2Hzwk7Kus7v7Z4jp7vX6CiFuHPQLOiESQc4uRCLI2YVIBDm7EIkgZxciEfqacDILoEZsjUi/aGJJghuXY+ZXebzcbJdLduenLwXbB5o8amxklP+QaLDII9HOT3NpaD7Do8NmVgeD7dPLfD4y8zyyrRmJUpt9jdejm58L11hbitXFK/D5yDXr1FYoj1Nbq0pk1gvhc7kuZS6XLsUKsEXUPGYaqXD5NeckejQStKknuxCJIGcXIhHk7EIkgpxdiESQswuRCHJ2IRKhr9JbF1xmqDV4v2kWTBSRM3K1yDgWuKzlLa5djGXCElV5hCcvHBjg99NmJBJqJdIvs8wlmUw+3K+Q5xFZw/t4FGAhUmNtaJgneswOt4LtrTaf+2wkoqw5zsfoL++ntrvedyjY/nf/9rton7/6Xz+ltlPHf0FtzctcilyIRMQ1EU5UuRC5Psq5sNybifTRk12IRJCzC5EIcnYhEkHOLkQiyNmFSIS+rsY7gDaxNSO3nSorDTXC+/DQFGCwwndWzfEpaXh4FbywEMv7xVeYkeH98k0e7FKJlELKlcJ53G75wN+ifW6u8yCTZoevnncjs/xGI5xw+HKHl7UaavMTmlvh89FyPo9vkICcd7/rvbTPR6o89OrMfFhlAID5QR5c82KTS0d/1ZwNtttuPo7BfdVge/a5iIpDLUKI3yrk7EIkgpxdiESQswuRCHJ2IRJBzi5EImyk/NPDAD4B4KK7v6fX9iUAfwjgitbwBXd/Yr1tZQEMkTiTbCTRnHcjibUoPCCgscDlk7YxcRBYbIflk0qZS0Ze5Ntb6XJZK0cCHQCgOsGDKgbPhHPevXviFtrn9GkewHF++Ry13TxxM7WVLywG29vtsMwEABmLSJgDvPzTSqTE1ouvN4Ltx18/Q/t8fIXnKLw9w6XUXLFGbdU27/fsSliyKw/z+dhZCQcG5TL8utnIk/3PAdwTaP+au9/e+7euowshtpd1nd3dnwKgwupC/Iazme/snzOz42b2sJlFfssmhLgRuFZn/waAQwBuBzAJ4CvsjWb2oJkdNbOjU1P6gCDEdnFNzu7uF9y94+5dAN8EcGfkvUfc/bC7H67XecEEIcTWck3ObmYTV/35KQAnrs9whBBbxUakt+8A+AiAMTM7C+CLAD5iZrdjLZDtNIA/2sjOOgDmiIqW6fI8bnkSeVWMlBLKFnmetsllLq3MX+C27o5wmaH9Fs4hBgDdYiRPWykS9ZbnxzY8xHXKd+y/Kdi+e4yf6ldfe43aXr/4MrWND/GyS91ueB4zRL4EAMtySdG6kUi/PJe12qthCXDq9Ku0z/LwPmpbmglHFQLAYItHvf3ycjj6DgCmy2GZ+ObhvbTPQDF8zNlIJOW6zu7unwk0f2u9fkKIGwv9gk6IRJCzC5EIcnYhEkHOLkQiyNmFSIS+JpzMAhhkA+EBYGiQQJ5Cl0e2dRa5bW6GS2WXWlx6G+iE5byGhctCAcABrFLbSiSYz3JspoD9eS5TvvPWW4PtQzW+PXvuJWprtniE4PQMn6tqJZwQ8cAQ/2HV3CyPECxW+WTtrfCIuNPLYeltdbVJ+3Qz/Jhj0tZLM3yOn2/xX4/mDoZ/bV4f3UX7VLth6S0TSb6pJ7sQiSBnFyIR5OxCJIKcXYhEkLMLkQhydiESoa/SWwfADLm95Oe5VDbsYe1ttcz1umUiTQBAfjUsCwEA5njyxU45nOSvUuXS20qTS03tSDJKdy5rHXr3+7itGh7jdJ7XSjvrPCIrV+CRaPvG+bE9+/LZYHuzzaXITJufz3xEKut0+DzSqn8s8ymAkQIfRyPD5+MEeETc9BDvt+vgnmD77iHunrmh8Pm0HPcjPdmFSAQ5uxCJIGcXIhHk7EIkgpxdiEToeyDMMMkbl6vxVUTvhO9JrciqeuO1cAAEAJxb4rbLbT6OfbnwivBihue7i1QtQnOG76vV4Sv8O8b46nMnGz6lC6d4nrn5ucvUVt3BgzEuRfLrPX/6fLB98jwv/zRe4SvWq5MXqa07wM9nfiF8bgoZrk6cb/JAqdPOA1peiERzje6vU9vB4bFge6HNA57aRGSIFUrTk12IRJCzC5EIcnYhEkHOLkQiyNmFSAQ5uxCJsJHyT/sA/AWAnVhb2T/i7l83s1EA3wNwAGsloO5zdx4JAKALYIXkyBogwS4AsNoKS00vnThG+0ye5xJJsbqb2lbmuTQ0PRuWSHbv5xJUJc/lpPNnX+E28Dxoh3bzCtnDxPSyc1Gmu8ht5UNcejPnx9YaCwfkYIo/Xy4VeLBLq8Ntc8tcpqzuCsuKxQy/3rzDg3VOrXKZdXGYS6ITBw5QW304PFfFkRrt020Smc83FwjTBvAn7n4bgLsA/LGZ3QbgIQBPuvstAJ7s/S2EuEFZ19ndfdLdf9l7PQ/gJIA9AO4F8EjvbY8A+OQWjVEIcR14W9/ZzewAgDsAPA1gp7tP9kznsfYxXwhxg7JhZzezAQA/APB5f0tmBXd3kF/qmdmDZnbUzI5OTfHv0UKIrWVDzm5meaw5+rfd/bFe8wUzm+jZJwAEf7zs7kfc/bC7H67XeWYTIcTWsq6zm5lhrR77SXf/6lWmxwHc33t9P4AfXf/hCSGuFxuJevsggM8CeM7MjvXavgDgywAeNbMHALwG4L71NpQBUGEqT6RcU64cjlDauXec9ikWw3m9AGA5ks/sxcs8TK3ZXg62W5aPfbrN5ZjnL/JIruoQH0etyqP9FhbCYzl56hzt88rrXG4c2c8lwHxEvhodC0d5LZ3iEWVTc+H5BYDpYS552Tif/5FWWNYaK3H58tQUn6uzba4uV/dymbI2weXBUjV8rjPLfD4WS+HtdRHxI2rp4e4/A+gW7l6vvxDixkC/oBMiEeTsQiSCnF2IRJCzC5EIcnYhEqHv5Z8aFo7WyVJNDmjPhPt0m1xCG97H72O5Dv9xz/D8JLVlSULBMpEGAeDi5UhJoCleampkPzXBi/y0zTXDEVuLC1xCa1e5LUa3yxMsNlfD45ipcjkpVhoKs3yOiysL1LajHpZnxyPy5Ut/c4HaliJJMXfcxqMpB0eGqG21Ej621TaXXztEqlbCSSGEnF2IVJCzC5EIcnYhEkHOLkQiyNmFSIQ+13pzjDDpLcOH0h0JS0PNFk+Ok6nyKKn2DLfN7riZbzMbloY8wyONGrM8omw5EtXUuVzm2xziEtXSajhBSKPRoH2KLS5r5YksBAArKzyCbZYcd3uZ98m2uASYy3BRqVLg18Gu3WHbMngdtcsXXqe2wfeSRJoA6jt44tFMgZ9PXw3PsRf4MZdy4X1lIs9vPdmFSAQ5uxCJIGcXIhHk7EIkgpxdiETo62p8F4ZFkuFqCJEgCEJpJ18ZbUXuY80itx06wHOFtYvhld12nm9vfrJBbZ1Vnp+uVOeBGrkhftpar8wH25stHnSTM77inl3lq+cL5+aobepiuOzS4EUevNSoxp49kXx3Nb7SffjdB4Pt5y/zFfeVSE7BnQffT23lKl/hZyvuAJAph487a/w8F8nmjA9dT3YhUkHOLkQiyNmFSAQ5uxCJIGcXIhHk7EIkwrrSm5ntA/AXWCvJ7ACOuPvXzexLAP4QwKXeW7/g7k/EtpXpAtUlcn8Z5PedJYSDZyqzkcCJYR5kgmyNm/Jc8rJKeLryXS65tJYiOdeWuQyFM5EAmgKXB8uVcH694niN9sktcQktm+V55ry7SG3DHp6TlRK/5DpNvj0UuW3vAM8LV1oN93v1lZO0T3mc5ygs1fi+Vpo8cKUeCcwqMFmxw3W0fDl8DWQiJbk2orO3AfyJu//SzAYBPGNmP+nZvubu/34D2xBCbDMbqfU2CWCy93rezE4C4FUThRA3JG/rO7uZHQBwB4Cne02fM7PjZvawmfGymEKIbWfDzm5mAwB+AODz7j4H4BsADgG4HWtP/q+Qfg+a2VEzOzo1FU6sIITYejbk7GaWx5qjf9vdHwMAd7/g7h137wL4JoA7Q33d/Yi7H3b3w/U6X/gQQmwt6zq7mRmAbwE46e5fvap94qq3fQrAies/PCHE9WIjq/EfBPBZAM+Z2bFe2xcAfMbMbseaHHcawB+tt6GOATP5sDyRneGyhTNTYYn2aXW5bGHdBt9XMSI1zYwF2zs8+A61EX5cq6t8/CdePUZtlTqXcWq1sHQ4WuSSUWEvn6v6IJci20u87FImE/7KVhnnY69f4PORn+BRY+/Z/Q5qa2fDEmC1Fj6XAFDZX6O2coZH2C0U+XxkO7xfaTA8/82Y9Gbh5zRpBrCx1fifAcG41KimLoS4sdAv6IRIBDm7EIkgZxciEeTsQiSCnF2IROhv+ScDamSPkUA0Lr3NcRnHMpHoNXCJZ8a4RIVKWAqJBBqhXq9R2ztv4uWCbn3/YWrbWQ8nUQSACxcvBds7bZ7QcwhD1FYaGqS2c7MvUptlwhJmscMvucUmH+OOSHmwSrNBbXPnzwfbh3ZzKc+GuFyay/IEnMNdPleZAo+MzJBrLl+q0D6lofC1b3l+MerJLkQiyNmFSAQ5uxCJIGcXIhHk7EIkgpxdiEToq/QGB0ByLDZ4UBCGSZ+5Lk/Y6LO8jlq2UqO2jDX4OIrhePwl8OSQN72DR1ftH/571GYF3m9umUtD7UY42qxzid/XR/bwsL18mUdercxzCbNaDp/Q7CLfXrPAL8eBMpdZ51fC9e0AYLkQPu4BRGSyVb6v/DiXKQdJHUMAKLa5JNYshyW2HQWe/MlbM8H2DL809GQXIhXk7EIkgpxdiESQswuRCHJ2IRJBzi5EIvRXejPAimF5YsS4bOEkOCxWzm2WKyQY7nCpbDHDx4GlRrC5XOV9qkT6AQDfs4vaOrN8m7k2r3E3WQ1LPO1RLjWVSQ07AMjMRpJ6tnkEWK0U3l8zxyPbbhnhJ61Y51JqZpGPY3xHuHhRZZBHvVWGa9w2tJPa5ue5BFjs8Ki3eoVEvWW5XNduhuXSWK03PdmFSAQ5uxCJIGcXIhHk7EIkgpxdiERYdzXezEoAngJQ7L3/++7+RTM7COC7AOoAngHwWXfnkSkAOh2gMRe2ZSOL4CwHXS2yr1rkNjbHF01BUnsBABqkvRA56jJfzAaGuCqQHeb56aqD3Dbiu4Ptt7X5hNSKA9T26oU3qG1+lq8wV3Ph1e5miSsJA5lIAEo7ok6Q3IAAkBkNr06Pj/Egk2KF5y/M5LkqkBuuU1t+lh93Ph+eq2xkNb5UCG8vu8nV+FUAH3X392OtPPM9ZnYXgD8F8DV3fweAGQAPbGBbQohtYl1n9zWuVKzL9/45gI8C+H6v/REAn9yKAQohrg8brc+e7VVwvQjgJwBeBtBw9yufac4CCP96QQhxQ7AhZ3f3jrvfDmAvgDsB3LrRHZjZg2Z21MyOTk+HEysIIbaet7Ua7+4NAH8N4HcA1MzsygLfXgDnSJ8j7n7Y3Q+PjoYzvQghtp51nd3MdphZrfe6DOBjAE5izen/ce9t9wP40RaNUQhxHdhIIMwEgEfMLIu1m8Oj7v7fzewFAN81s38L4FcAvrXehrJZYHAwrKPlszx5ls8Q20hEr4sQiZEBunybRvLk5RuRxF+liCwUCbpxrmrBslzPG6sRWW4fzzO3NMkDSVZWLlNbZc8OamucvRBsH5ngs18ocg3TVnkATW2UJzCs18MS28Awnw/LcrdoLfPnY7XCpTeUeb/MQrjduhGfqIZlW8vy/azr7O5+HMAdgfZXsPb9XQjxG4B+QSdEIsjZhUgEObsQiSBnFyIR5OxCJII5Cynbip2ZXQLwWu/PMQBc1+kfGseb0TjezG/aOG5y96Am2ldnf9OOzY66++Ft2bnGoXEkOA59jBciEeTsQiTCdjr7kW3c99VoHG9G43gzvzXj2Lbv7EKI/qKP8UIkwrY4u5ndY2a/NrNTZvbQdoyhN47TZvacmR0zs6N93O/DZnbRzE5c1TZqZj8xs5d6//OMiFs7ji+Z2bnenBwzs4/3YRz7zOyvzewFM3vezP5Fr72vcxIZR1/nxMxKZvZzM3u2N45/02s/aGZP9/zme2YWSY8awN37+g9AFmtprW4GUADwLIDb+j2O3lhOAxjbhv1+GMAHAJy4qu3fAXio9/ohAH+6TeP4EoB/2ef5mADwgd7rQQAvArit33MSGUdf5wSAARjovc4DeBrAXQAeBfDpXvt/BPDP3852t+PJfieAU+7+iq+lnv4ugHu3YRzbhrs/BeCtObruxVriTqBPCTzJOPqOu0+6+y97r+exlhxlD/o8J5Fx9BVf47oned0OZ98D4MxVf29nskoH8GMze8bMHtymMVxhp7tP9l6fB8DLhW49nzOz472P+Vv+deJqzOwA1vInPI1tnJO3jAPo85xsRZLX1BfoPuTuHwDwDwH8sZl9eLsHBKzd2bF2I9oOvgHgENZqBEwC+Eq/dmxmAwB+AODz7v6mciL9nJPAOPo+J76JJK+M7XD2cwD2XfU3TVa51bj7ud7/FwH8ENubeeeCmU0AQO//i9sxCHe/0LvQugC+iT7NiZnlseZg33b3x3rNfZ+T0Di2a056+27gbSZ5ZWyHs/8CwC29lcUCgE8DeLzfgzCzqpkNXnkN4PcBnIj32lIex1riTmAbE3heca4en0If5sTMDGs5DE+6+1evMvV1Ttg4+j0nW5bktV8rjG9Zbfw41lY6Xwbwr7ZpDDdjTQl4FsDz/RwHgO9g7eNgC2vfvR7AWs28JwG8BOCnAEa3aRx/CeA5AMex5mwTfRjHh7D2Ef04gGO9fx/v95xExtHXOQHwPqwlcT2OtRvLv77qmv05gFMA/guA4tvZrn5BJ0QipL5AJ0QyyNmFSAQ5uxCJIGcXIhHk7EIkgpxdiESQswuRCHJ2IRLh/wINNmzCmbywfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = p_adv[0][1].cpu().detach()\n",
    "plt.imshow(p.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0314)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x - f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0314)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054901960784313725"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*2/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5669921875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_adv, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6051"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_adv, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = PyTorchModel(model, bounds=(0, 1))\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean accuracy:  59.2 %\n"
     ]
    }
   ],
   "source": [
    "clean_acc = accuracy(fmodel, images, labels)\n",
    "print(f\"clean accuracy:  {clean_acc * 100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = FGSM()\n",
    "epsilon = 8/255\n",
    "raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_accuracy = 1 - success.float().mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0254, device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_stats = model.fit_fast(train_loader, test_loader, 10, device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_stats = model.fit_fast(train_loader, test_loader, 10, device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_stats = model.fit_fast(train_loader, test_loader, 10, device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_stats = model.fit_fast(train_loader, test_loader, 10, device, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import clamp\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters())\n",
    "\n",
    "alpha=10\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "mu = torch.tensor(cifar10_mean).view(3,1,1).cuda()\n",
    "std = torch.tensor(cifar10_std).view(3,1,1).cuda()\n",
    "\n",
    "upper_limit = ((1 - mu)/ std)\n",
    "lower_limit = ((0 - mu)/ std)\n",
    "\n",
    "def loc_train(model, epochs, epsilon):\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_n = 0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            if i == 0:\n",
    "                first_batch = (X, y)\n",
    "            delta = torch.zeros_like(X).cuda()\n",
    "            \n",
    "\n",
    "            delta.uniform_(-epsilon, epsilon)\n",
    "            delta.data = clamp(delta, 0,1)\n",
    "            \n",
    "            delta.requires_grad = True\n",
    "            output = model(X + delta[:X.size(0)])\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            loss.backward()\n",
    "            grad = delta.grad.detach()\n",
    "            delta.data = clamp(delta + alpha * torch.sign(grad), -epsilon, epsilon)\n",
    "            delta.data[:X.size(0)] = clamp(delta[:X.size(0)], 0,1)\n",
    "            delta = delta.detach()\n",
    "            output = model(X + delta[:X.size(0)])\n",
    "            loss = criterion(output, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "            train_acc += (output.max(1)[1] == y).sum().item()\n",
    "            train_n += y.size(0)\n",
    "            print(train_acc/train_n, train_loss/train_n)\n",
    "        epoch_time = time.time()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.083984375 3.052335500717163\n",
      "0.0859375 10.463098406791687\n",
      "0.09309895833333333 10.319573640823364\n",
      "0.10009765625 9.919875800609589\n",
      "0.11015625 9.819345045089722\n",
      "0.11490885416666667 9.664365728696188\n",
      "0.11830357142857142 9.370375326701573\n",
      "0.114501953125 9.100074023008347\n",
      "0.1174045138888889 8.764120499293009\n",
      "0.11796875 8.358004641532897\n",
      "0.11807528409090909 7.941281968897039\n",
      "0.11979166666666667 7.593418478965759\n",
      "0.12139423076923077 7.339407150561993\n",
      "0.12346540178571429 7.0954293182918\n",
      "0.11953125 6.85154751141866\n",
      "0.1195068359375 6.611902013421059\n",
      "0.12017463235294118 6.442012885037591\n",
      "0.12044270833333333 6.290201465288798\n",
      "0.12099095394736842 6.13347780077081\n",
      "0.12197265625 5.97891389131546\n",
      "0.12276785714285714 5.830017419088454\n",
      "0.12446732954545454 5.711510625752536\n",
      "0.12567934782608695 5.58311012516851\n",
      "0.12744140625 5.465107391277949\n",
      "0.1290625 5.361762580871582\n",
      "0.12935697115384615 5.25775801218473\n",
      "0.12962962962962962 5.153595844904582\n",
      "0.13120814732142858 5.058839687279293\n",
      "0.1328125 4.972027252460348\n",
      "0.13359375 4.88745965162913\n",
      "0.13552167338709678 4.811371710992629\n",
      "0.13629150390625 4.739728204905987\n",
      "0.13594933712121213 4.6678691560571846\n",
      "0.13683363970588236 4.597830961732304\n",
      "0.13705357142857144 4.5324118409838\n",
      "0.1389431423611111 4.469891740216149\n",
      "0.14120565878378377 4.4097876677642\n",
      "0.14314350328947367 4.350757310264989\n",
      "0.14458133012820512 4.2963460347591305\n",
      "0.1453125 4.2468547344207765\n",
      "0.14557926829268292 4.211850974617935\n",
      "0.14671688988095238 4.171210385504223\n",
      "0.1478015988372093 4.1275863481122395\n",
      "0.14799360795454544 4.092486771670255\n",
      "0.14809027777777778 4.059727144241333\n",
      "0.14818274456521738 4.029284637907277\n",
      "0.1496841755319149 3.9948959756404796\n",
      "0.15059407552083334 3.9632994880278907\n",
      "0.15214445153061223 3.927079843015087\n",
      "0.153359375 3.895370464324951\n",
      "0.1539905024509804 3.8675356425491034\n",
      "0.15538611778846154 3.8383585718961863\n",
      "0.1560657429245283 3.81381404624795\n",
      "0.15690104166666666 3.787293884489271\n",
      "0.1569247159090909 3.7600855740633876\n",
      "0.15799386160714285 3.7343744380133495\n",
      "0.15892269736842105 3.7132557693280672\n",
      "0.15944908405172414 3.6891462926206917\n",
      "0.16025556144067796 3.667081905623614\n",
      "0.16100260416666667 3.644276014963786\n",
      "0.16188524590163936 3.6230046553689927\n",
      "0.16248739919354838 3.6018345702079033\n",
      "0.16294642857142858 3.579787155938527\n",
      "0.1636962890625 3.557802442461252\n",
      "0.1641826923076923 3.536284248645489\n",
      "0.16447679924242425 3.5154632077072607\n",
      "0.1652285447761194 3.4943787802511186\n",
      "0.16569967830882354 3.4766774317797493\n",
      "0.1659873188405797 3.4598904001540034\n",
      "0.16668526785714285 3.441457288605826\n",
      "0.16714348591549297 3.422867113435772\n",
      "0.1674533420138889 3.4056935707728067\n",
      "0.1680757705479452 3.387779523248542\n",
      "0.16886613175675674 3.370349768045786\n",
      "0.16958333333333334 3.3540647093455\n",
      "0.17015316611842105 3.337453986469068\n",
      "0.17058137175324675 3.3219124434830305\n",
      "0.17122395833333334 3.3069237012129564\n",
      "0.17140526107594936 3.292955600762669\n",
      "0.1724609375 3.2767936915159224\n",
      "0.17380401234567902 3.2611780843617004\n",
      "0.17449504573170732 3.2465277328723814\n",
      "0.17481645331325302 3.2321953543697495\n",
      "0.17513020833333334 3.2189210028875443\n",
      "0.1758501838235294 3.205455718320959\n",
      "0.17630359738372092 3.1920223735099613\n",
      "0.17685883620689655 3.179057554266919\n",
      "0.1774014559659091 3.16655296087265\n",
      "0.17799771769662923 3.154111685377828\n",
      "0.17847222222222223 3.141792805989583\n",
      "0.17895776098901098 3.1302075674245646\n",
      "0.17939028532608695 3.1183701935021775\n",
      "0.1796875 3.1083635924964823\n",
      "0.18068484042553193 3.0966025717715\n",
      "0.18157894736842106 3.0845832899997108\n",
      "0.18198649088541666 3.0733372047543526\n",
      "0.18294942010309279 3.061447941150862\n",
      "0.18332 3.054388949661255\n",
      "0.2734375 2.0167171955108643\n",
      "0.2724609375 2.0122296810150146\n",
      "0.2740885416666667 1.9913374185562134\n",
      "0.26611328125 1.9854326248168945\n",
      "0.265234375 1.995263433456421\n",
      "0.2662760416666667 2.003137548764547\n",
      "0.26785714285714285 2.0045810767582486\n",
      "0.270751953125 2.004553943872452\n",
      "0.2697482638888889 2.0023899475733438\n",
      "0.2673828125 2.004171407222748\n",
      "0.271484375 2.0012740655378862\n",
      "0.27099609375 1.9981950322786968\n",
      "0.27103365384615385 1.9936592212090125\n",
      "0.27371651785714285 1.9856974823134286\n",
      "0.2733072916666667 1.9889522790908813\n",
      "0.2733154296875 1.9842650070786476\n",
      "0.2721737132352941 1.9784505998387056\n",
      "0.2719184027777778 1.972533192899492\n",
      "0.27230674342105265 1.9684188303194547\n",
      "0.273828125 1.9649203598499299\n",
      "0.2740885416666667 1.961392805689857\n",
      "0.2751242897727273 1.9594064571640708\n",
      "0.274711277173913 1.9607363991115405\n",
      "0.274169921875 1.9631553788979847\n",
      "0.273984375 1.961302318572998\n",
      "0.27501502403846156 1.9573420332028315\n",
      "0.2747395833333333 1.957064814037747\n",
      "0.27462332589285715 1.9570861203329903\n",
      "0.27525592672413796 1.9562982690745387\n",
      "0.27643229166666666 1.9543085972468057\n",
      "0.27639868951612906 1.9517744471949916\n",
      "0.2760009765625 1.9504032246768475\n",
      "0.27580492424242425 1.9484419714320789\n",
      "0.27694163602941174 1.9471772663733538\n",
      "0.2766183035714286 1.9457782472882952\n",
      "0.2771267361111111 1.9444202019108667\n",
      "0.2780827702702703 1.942895083814054\n",
      "0.27816611842105265 1.9411806150486595\n",
      "0.2789463141025641 1.9413407613069584\n",
      "0.278955078125 1.940086156129837\n",
      "0.27848704268292684 1.939819492944857\n",
      "0.2787388392857143 1.9400832511129833\n",
      "0.27875181686046513 1.9398659107296965\n",
      "0.2795188210227273 1.935618210922588\n",
      "0.2798611111111111 1.9332398335138956\n",
      "0.28116508152173914 1.9312090821888135\n",
      "0.2818317819148936 1.9312854979900604\n",
      "0.2818196614583333 1.9298001676797867\n",
      "0.28280452806122447 1.9278488937689333\n",
      "0.2831640625 1.9260618305206298\n",
      "0.28370098039215685 1.9242521173813765\n",
      "0.2842548076923077 1.9233592817416558\n",
      "0.2847877358490566 1.921774223165692\n",
      "0.2845052083333333 1.9210093065544411\n",
      "0.2845880681818182 1.9196535717357288\n",
      "0.2847377232142857 1.919609663741929\n",
      "0.2857730263157895 1.9184249367630273\n",
      "0.28606546336206895 1.9180445465548286\n",
      "0.286083156779661 1.9175909292899955\n",
      "0.2865234375 1.9175457457701366\n",
      "0.2865970799180328 1.9176671133666743\n",
      "0.2872353830645161 1.9159707215524489\n",
      "0.2874503968253968 1.9153576654101174\n",
      "0.2869873046875 1.9172406233847141\n",
      "0.28728966346153845 1.915841480401846\n",
      "0.2879083806818182 1.9149420442003193\n",
      "0.28798390858208955 1.9149841105760033\n",
      "0.2876551011029412 1.9145719654419844\n",
      "0.2880151721014493 1.9137727454088735\n",
      "0.2883091517857143 1.913508711542402\n",
      "0.28823723591549294 1.9134833594443093\n",
      "0.2884657118055556 1.9133961647748947\n",
      "0.2890892551369863 1.9120900794251325\n",
      "0.2892208614864865 1.9117045241433221\n",
      "0.28953125 1.9118163108825683\n",
      "0.28983347039473684 1.9107662815796702\n",
      "0.29048295454545453 1.9086683740863553\n",
      "0.290614983974359 1.9086056382228167\n",
      "0.2906942246835443 1.9083248180679129\n",
      "0.2907958984375 1.9077817127108574\n",
      "0.2914255401234568 1.9066600755408958\n",
      "0.2917778201219512 1.9056569018015048\n",
      "0.29186276355421686 1.9063423800181194\n",
      "0.29227120535714285 1.9054521847338903\n",
      "0.29216452205882354 1.904643999829012\n",
      "0.292469113372093 1.9041672224222228\n",
      "0.29296875 1.9030976870964313\n",
      "0.2934792258522727 1.9018466540358283\n",
      "0.29375877808988765 1.9007317993078339\n",
      "0.29411892361111114 1.8995861185921563\n",
      "0.2947501717032967 1.8979325739891975\n",
      "0.29513417119565216 1.898639870726544\n",
      "0.2952158938172043 1.8985098036386634\n",
      "0.2950465425531915 1.8989382474980456\n",
      "0.2954358552631579 1.8979105133759349\n",
      "0.29522705078125 1.8976471163332462\n",
      "0.29510309278350516 1.8975107411748355\n",
      "0.2953 1.8970306798934937\n"
     ]
    }
   ],
   "source": [
    "model = loc_train(model, 10, 8/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.02, 0.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_clean_accuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7691"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, success = FGSM(model, test_loader, torch.nn.CrossEntropyLoss(), 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, success = PGD(model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device)\n",
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[[0.7490, 0.7490, 0.7529,  ..., 0.6667, 0.6667, 0.6706],\n",
      "          [0.7569, 0.7608, 0.7686,  ..., 0.6706, 0.6667, 0.6706],\n",
      "          [0.7725, 0.7725, 0.7725,  ..., 0.6824, 0.6784, 0.6824],\n",
      "          ...,\n",
      "          [0.5961, 0.6039, 0.6039,  ..., 0.3412, 0.3529, 0.3882],\n",
      "          [0.5490, 0.5569, 0.5451,  ..., 0.3843, 0.3451, 0.3098],\n",
      "          [0.5451, 0.5490, 0.5529,  ..., 0.6000, 0.5961, 0.5725]],\n",
      "\n",
      "         [[0.8588, 0.8588, 0.8588,  ..., 0.7882, 0.7882, 0.7922],\n",
      "          [0.8627, 0.8667, 0.8706,  ..., 0.7882, 0.7843, 0.7882],\n",
      "          [0.8667, 0.8667, 0.8667,  ..., 0.7882, 0.7882, 0.7882],\n",
      "          ...,\n",
      "          [0.5725, 0.5765, 0.5765,  ..., 0.3373, 0.3569, 0.3961],\n",
      "          [0.5294, 0.5333, 0.5294,  ..., 0.3765, 0.3333, 0.3098],\n",
      "          [0.5255, 0.5333, 0.5373,  ..., 0.5686, 0.5608, 0.5451]],\n",
      "\n",
      "         [[0.9490, 0.9529, 0.9569,  ..., 0.9020, 0.8980, 0.9020],\n",
      "          [0.9490, 0.9529, 0.9608,  ..., 0.9020, 0.8941, 0.8980],\n",
      "          [0.9569, 0.9529, 0.9529,  ..., 0.9059, 0.8980, 0.8941],\n",
      "          ...,\n",
      "          [0.5529, 0.5529, 0.5529,  ..., 0.3647, 0.3843, 0.4039],\n",
      "          [0.5137, 0.5137, 0.4980,  ..., 0.3529, 0.3373, 0.3333],\n",
      "          [0.5176, 0.5216, 0.5176,  ..., 0.5412, 0.5294, 0.5176]]],\n",
      "\n",
      "\n",
      "        [[[0.2078, 0.2118, 0.3569,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2196, 0.2196, 0.3647,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.2627, 0.2471, 0.3804,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.4471, 0.4118, 0.4157,  ..., 0.4667, 0.4588, 0.4549],\n",
      "          [0.4549, 0.3412, 0.3059,  ..., 0.4627, 0.4706, 0.4667],\n",
      "          [0.4784, 0.4353, 0.4078,  ..., 0.4588, 0.4667, 0.4745]],\n",
      "\n",
      "         [[0.1961, 0.2039, 0.3529,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2118, 0.2157, 0.3647,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.2510, 0.2431, 0.3804,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.4706, 0.4314, 0.4392,  ..., 0.4784, 0.4667, 0.4588],\n",
      "          [0.4745, 0.3608, 0.3255,  ..., 0.4706, 0.4784, 0.4745],\n",
      "          [0.5020, 0.4588, 0.4314,  ..., 0.4667, 0.4745, 0.4824]],\n",
      "\n",
      "         [[0.2588, 0.2392, 0.3647,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2706, 0.2471, 0.3725,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.3137, 0.2784, 0.3882,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.5725, 0.5373, 0.5412,  ..., 0.5137, 0.5137, 0.5176],\n",
      "          [0.5882, 0.4745, 0.4392,  ..., 0.5098, 0.5255, 0.5294],\n",
      "          [0.6000, 0.5529, 0.5255,  ..., 0.5059, 0.5216, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.6353, 0.6314, 0.6353,  ..., 0.6431, 0.6392, 0.6314],\n",
      "          [0.6196, 0.6157, 0.6196,  ..., 0.6314, 0.6235, 0.6157],\n",
      "          [0.6235, 0.6118, 0.6118,  ..., 0.6275, 0.6196, 0.6157],\n",
      "          ...,\n",
      "          [0.7294, 0.7020, 0.7020,  ..., 0.5765, 0.5804, 0.6039],\n",
      "          [0.7490, 0.7333, 0.6941,  ..., 0.6078, 0.6314, 0.6431],\n",
      "          [0.7490, 0.7176, 0.7098,  ..., 0.6157, 0.6353, 0.6627]],\n",
      "\n",
      "         [[0.6510, 0.6392, 0.6353,  ..., 0.7098, 0.7020, 0.6941],\n",
      "          [0.6353, 0.6235, 0.6275,  ..., 0.6941, 0.6863, 0.6784],\n",
      "          [0.6353, 0.6235, 0.6235,  ..., 0.6902, 0.6824, 0.6784],\n",
      "          ...,\n",
      "          [0.8000, 0.7647, 0.7569,  ..., 0.6196, 0.6275, 0.6549],\n",
      "          [0.8196, 0.8039, 0.7647,  ..., 0.6588, 0.6824, 0.6902],\n",
      "          [0.8196, 0.7922, 0.7882,  ..., 0.6863, 0.7020, 0.7098]],\n",
      "\n",
      "         [[0.6118, 0.6039, 0.6078,  ..., 0.7686, 0.7608, 0.7490],\n",
      "          [0.5922, 0.5843, 0.5882,  ..., 0.7529, 0.7451, 0.7333],\n",
      "          [0.5922, 0.5804, 0.5804,  ..., 0.7490, 0.7412, 0.7294],\n",
      "          ...,\n",
      "          [0.8000, 0.7725, 0.7686,  ..., 0.6078, 0.6157, 0.6471],\n",
      "          [0.8353, 0.8157, 0.7725,  ..., 0.6549, 0.6784, 0.6824],\n",
      "          [0.8431, 0.8078, 0.8000,  ..., 0.6863, 0.7020, 0.7059]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4784, 0.4980, 0.5255,  ..., 0.4863, 0.4275, 0.4118],\n",
      "          [0.5098, 0.5333, 0.5647,  ..., 0.6275, 0.5373, 0.4980],\n",
      "          [0.5451, 0.5686, 0.6000,  ..., 0.6745, 0.6039, 0.5569],\n",
      "          ...,\n",
      "          [0.2353, 0.2235, 0.2510,  ..., 0.3294, 0.3216, 0.3098],\n",
      "          [0.2471, 0.2314, 0.2078,  ..., 0.2314, 0.2314, 0.2353],\n",
      "          [0.2196, 0.2118, 0.2078,  ..., 0.1765, 0.1725, 0.1686]],\n",
      "\n",
      "         [[0.7216, 0.7412, 0.7686,  ..., 0.6863, 0.6510, 0.6314],\n",
      "          [0.7412, 0.7647, 0.7882,  ..., 0.7804, 0.7176, 0.6824],\n",
      "          [0.7725, 0.7961, 0.8118,  ..., 0.8118, 0.7647, 0.7216],\n",
      "          ...,\n",
      "          [0.3255, 0.3294, 0.3608,  ..., 0.3686, 0.3569, 0.3412],\n",
      "          [0.3294, 0.3294, 0.3255,  ..., 0.2784, 0.2824, 0.2784],\n",
      "          [0.2902, 0.2941, 0.3059,  ..., 0.2157, 0.2157, 0.2118]],\n",
      "\n",
      "         [[0.8392, 0.8549, 0.8627,  ..., 0.7922, 0.7804, 0.7647],\n",
      "          [0.8431, 0.8549, 0.8667,  ..., 0.8392, 0.8039, 0.7804],\n",
      "          [0.8588, 0.8667, 0.8784,  ..., 0.8510, 0.8314, 0.8039],\n",
      "          ...,\n",
      "          [0.4078, 0.4157, 0.4314,  ..., 0.3647, 0.3608, 0.3490],\n",
      "          [0.3922, 0.4039, 0.4118,  ..., 0.3098, 0.3098, 0.3020],\n",
      "          [0.3647, 0.3804, 0.3961,  ..., 0.2706, 0.2627, 0.2549]]],\n",
      "\n",
      "\n",
      "        [[[0.3020, 0.3059, 0.3137,  ..., 0.9059, 0.9255, 0.9451],\n",
      "          [0.3373, 0.3412, 0.3451,  ..., 0.8941, 0.9176, 0.9333],\n",
      "          [0.3686, 0.3765, 0.3804,  ..., 0.8902, 0.9059, 0.9255],\n",
      "          ...,\n",
      "          [0.4627, 0.4863, 0.5176,  ..., 0.5608, 0.5804, 0.6235],\n",
      "          [0.4863, 0.5451, 0.5608,  ..., 0.4588, 0.4902, 0.5255],\n",
      "          [0.3608, 0.4039, 0.4510,  ..., 0.2980, 0.3843, 0.4235]],\n",
      "\n",
      "         [[0.3529, 0.3569, 0.3608,  ..., 0.7569, 0.7922, 0.8314],\n",
      "          [0.3882, 0.3961, 0.4000,  ..., 0.7569, 0.7961, 0.8314],\n",
      "          [0.4196, 0.4275, 0.4392,  ..., 0.7647, 0.7922, 0.8314],\n",
      "          ...,\n",
      "          [0.3765, 0.3961, 0.4157,  ..., 0.5020, 0.5412, 0.5961],\n",
      "          [0.3843, 0.4431, 0.4549,  ..., 0.4353, 0.4745, 0.5176],\n",
      "          [0.3529, 0.3882, 0.4235,  ..., 0.3255, 0.4078, 0.4510]],\n",
      "\n",
      "         [[0.3608, 0.3647, 0.3765,  ..., 0.8000, 0.8549, 0.8941],\n",
      "          [0.4078, 0.4118, 0.4235,  ..., 0.8000, 0.8549, 0.8902],\n",
      "          [0.4471, 0.4549, 0.4667,  ..., 0.8039, 0.8471, 0.8941],\n",
      "          ...,\n",
      "          [0.2353, 0.2431, 0.2627,  ..., 0.4039, 0.4275, 0.4706],\n",
      "          [0.2510, 0.3020, 0.3098,  ..., 0.3451, 0.3686, 0.4000],\n",
      "          [0.3020, 0.3294, 0.3529,  ..., 0.2784, 0.3333, 0.3529]]],\n",
      "\n",
      "\n",
      "        [[[0.7843, 0.7529, 0.7608,  ..., 0.7569, 0.7529, 0.7961],\n",
      "          [0.7333, 0.7020, 0.7098,  ..., 0.7059, 0.6980, 0.7490],\n",
      "          [0.7255, 0.6902, 0.6980,  ..., 0.6902, 0.6824, 0.7412],\n",
      "          ...,\n",
      "          [0.6863, 0.6549, 0.6078,  ..., 0.7176, 0.7098, 0.7490],\n",
      "          [0.6627, 0.6667, 0.6667,  ..., 0.7216, 0.7137, 0.7529],\n",
      "          [0.8039, 0.8000, 0.8000,  ..., 0.8000, 0.7922, 0.8118]],\n",
      "\n",
      "         [[0.8157, 0.8000, 0.8039,  ..., 0.7922, 0.7843, 0.8235],\n",
      "          [0.7765, 0.7608, 0.7647,  ..., 0.7529, 0.7451, 0.7804],\n",
      "          [0.7765, 0.7608, 0.7647,  ..., 0.7529, 0.7451, 0.7804],\n",
      "          ...,\n",
      "          [0.6471, 0.6000, 0.5608,  ..., 0.6588, 0.6471, 0.7059],\n",
      "          [0.6235, 0.6118, 0.6118,  ..., 0.6667, 0.6588, 0.7137],\n",
      "          [0.7725, 0.7529, 0.7608,  ..., 0.7647, 0.7569, 0.7804]],\n",
      "\n",
      "         [[0.8824, 0.8588, 0.8549,  ..., 0.8510, 0.8471, 0.8902],\n",
      "          [0.8941, 0.8745, 0.8667,  ..., 0.8667, 0.8588, 0.8745],\n",
      "          [0.8941, 0.8745, 0.8706,  ..., 0.8706, 0.8627, 0.8745],\n",
      "          ...,\n",
      "          [0.6157, 0.5490, 0.5059,  ..., 0.6078, 0.5961, 0.6706],\n",
      "          [0.5882, 0.5647, 0.5725,  ..., 0.6235, 0.6157, 0.6784],\n",
      "          [0.7451, 0.7255, 0.7373,  ..., 0.7333, 0.7294, 0.7490]]]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([[[[0.7490, 0.7490, 0.7529,  ..., 0.6667, 0.6667, 0.6706],\n",
      "          [0.7569, 0.7608, 0.7686,  ..., 0.6706, 0.6667, 0.6706],\n",
      "          [0.7725, 0.7725, 0.7725,  ..., 0.6824, 0.6784, 0.6824],\n",
      "          ...,\n",
      "          [0.5961, 0.6039, 0.6039,  ..., 0.3412, 0.3529, 0.3882],\n",
      "          [0.5490, 0.5569, 0.5451,  ..., 0.3843, 0.3451, 0.3098],\n",
      "          [0.5451, 0.5490, 0.5529,  ..., 0.6000, 0.5961, 0.5725]],\n",
      "\n",
      "         [[0.8588, 0.8588, 0.8588,  ..., 0.7882, 0.7882, 0.7922],\n",
      "          [0.8627, 0.8667, 0.8706,  ..., 0.7882, 0.7843, 0.7882],\n",
      "          [0.8667, 0.8667, 0.8667,  ..., 0.7882, 0.7882, 0.7882],\n",
      "          ...,\n",
      "          [0.5725, 0.5765, 0.5765,  ..., 0.3373, 0.3569, 0.3961],\n",
      "          [0.5294, 0.5333, 0.5294,  ..., 0.3765, 0.3333, 0.3098],\n",
      "          [0.5255, 0.5333, 0.5373,  ..., 0.5686, 0.5608, 0.5451]],\n",
      "\n",
      "         [[0.9490, 0.9529, 0.9569,  ..., 0.9020, 0.8980, 0.9020],\n",
      "          [0.9490, 0.9529, 0.9608,  ..., 0.9020, 0.8941, 0.8980],\n",
      "          [0.9569, 0.9529, 0.9529,  ..., 0.9059, 0.8980, 0.8941],\n",
      "          ...,\n",
      "          [0.5529, 0.5529, 0.5529,  ..., 0.3647, 0.3843, 0.4039],\n",
      "          [0.5137, 0.5137, 0.4980,  ..., 0.3529, 0.3373, 0.3333],\n",
      "          [0.5176, 0.5216, 0.5176,  ..., 0.5412, 0.5294, 0.5176]]],\n",
      "\n",
      "\n",
      "        [[[0.2078, 0.2118, 0.3569,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2196, 0.2196, 0.3647,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.2627, 0.2471, 0.3804,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.4471, 0.4118, 0.4157,  ..., 0.4667, 0.4588, 0.4549],\n",
      "          [0.4549, 0.3412, 0.3059,  ..., 0.4627, 0.4706, 0.4667],\n",
      "          [0.4784, 0.4353, 0.4078,  ..., 0.4588, 0.4667, 0.4745]],\n",
      "\n",
      "         [[0.1961, 0.2039, 0.3529,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2118, 0.2157, 0.3647,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.2510, 0.2431, 0.3804,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.4706, 0.4314, 0.4392,  ..., 0.4784, 0.4667, 0.4588],\n",
      "          [0.4745, 0.3608, 0.3255,  ..., 0.4706, 0.4784, 0.4745],\n",
      "          [0.5020, 0.4588, 0.4314,  ..., 0.4667, 0.4745, 0.4824]],\n",
      "\n",
      "         [[0.2588, 0.2392, 0.3647,  ..., 0.2824, 0.2824, 0.2941],\n",
      "          [0.2706, 0.2471, 0.3725,  ..., 0.3176, 0.2863, 0.2941],\n",
      "          [0.3137, 0.2784, 0.3882,  ..., 0.3373, 0.3255, 0.3098],\n",
      "          ...,\n",
      "          [0.5725, 0.5373, 0.5412,  ..., 0.5137, 0.5137, 0.5176],\n",
      "          [0.5882, 0.4745, 0.4392,  ..., 0.5098, 0.5255, 0.5294],\n",
      "          [0.6000, 0.5529, 0.5255,  ..., 0.5059, 0.5216, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.6353, 0.6314, 0.6353,  ..., 0.6431, 0.6392, 0.6314],\n",
      "          [0.6196, 0.6157, 0.6196,  ..., 0.6314, 0.6235, 0.6157],\n",
      "          [0.6235, 0.6118, 0.6118,  ..., 0.6275, 0.6196, 0.6157],\n",
      "          ...,\n",
      "          [0.7294, 0.7020, 0.7020,  ..., 0.5765, 0.5804, 0.6039],\n",
      "          [0.7490, 0.7333, 0.6941,  ..., 0.6078, 0.6314, 0.6431],\n",
      "          [0.7490, 0.7176, 0.7098,  ..., 0.6157, 0.6353, 0.6627]],\n",
      "\n",
      "         [[0.6510, 0.6392, 0.6353,  ..., 0.7098, 0.7020, 0.6941],\n",
      "          [0.6353, 0.6235, 0.6275,  ..., 0.6941, 0.6863, 0.6784],\n",
      "          [0.6353, 0.6235, 0.6235,  ..., 0.6902, 0.6824, 0.6784],\n",
      "          ...,\n",
      "          [0.8000, 0.7647, 0.7569,  ..., 0.6196, 0.6275, 0.6549],\n",
      "          [0.8196, 0.8039, 0.7647,  ..., 0.6588, 0.6824, 0.6902],\n",
      "          [0.8196, 0.7922, 0.7882,  ..., 0.6863, 0.7020, 0.7098]],\n",
      "\n",
      "         [[0.6118, 0.6039, 0.6078,  ..., 0.7686, 0.7608, 0.7490],\n",
      "          [0.5922, 0.5843, 0.5882,  ..., 0.7529, 0.7451, 0.7333],\n",
      "          [0.5922, 0.5804, 0.5804,  ..., 0.7490, 0.7412, 0.7294],\n",
      "          ...,\n",
      "          [0.8000, 0.7725, 0.7686,  ..., 0.6078, 0.6157, 0.6471],\n",
      "          [0.8353, 0.8157, 0.7725,  ..., 0.6549, 0.6784, 0.6824],\n",
      "          [0.8431, 0.8078, 0.8000,  ..., 0.6863, 0.7020, 0.7059]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4784, 0.4980, 0.5255,  ..., 0.4863, 0.4275, 0.4118],\n",
      "          [0.5098, 0.5333, 0.5647,  ..., 0.6275, 0.5373, 0.4980],\n",
      "          [0.5451, 0.5686, 0.6000,  ..., 0.6745, 0.6039, 0.5569],\n",
      "          ...,\n",
      "          [0.2353, 0.2235, 0.2510,  ..., 0.3294, 0.3216, 0.3098],\n",
      "          [0.2471, 0.2314, 0.2078,  ..., 0.2314, 0.2314, 0.2353],\n",
      "          [0.2196, 0.2118, 0.2078,  ..., 0.1765, 0.1725, 0.1686]],\n",
      "\n",
      "         [[0.7216, 0.7412, 0.7686,  ..., 0.6863, 0.6510, 0.6314],\n",
      "          [0.7412, 0.7647, 0.7882,  ..., 0.7804, 0.7176, 0.6824],\n",
      "          [0.7725, 0.7961, 0.8118,  ..., 0.8118, 0.7647, 0.7216],\n",
      "          ...,\n",
      "          [0.3255, 0.3294, 0.3608,  ..., 0.3686, 0.3569, 0.3412],\n",
      "          [0.3294, 0.3294, 0.3255,  ..., 0.2784, 0.2824, 0.2784],\n",
      "          [0.2902, 0.2941, 0.3059,  ..., 0.2157, 0.2157, 0.2118]],\n",
      "\n",
      "         [[0.8392, 0.8549, 0.8627,  ..., 0.7922, 0.7804, 0.7647],\n",
      "          [0.8431, 0.8549, 0.8667,  ..., 0.8392, 0.8039, 0.7804],\n",
      "          [0.8588, 0.8667, 0.8784,  ..., 0.8510, 0.8314, 0.8039],\n",
      "          ...,\n",
      "          [0.4078, 0.4157, 0.4314,  ..., 0.3647, 0.3608, 0.3490],\n",
      "          [0.3922, 0.4039, 0.4118,  ..., 0.3098, 0.3098, 0.3020],\n",
      "          [0.3647, 0.3804, 0.3961,  ..., 0.2706, 0.2627, 0.2549]]],\n",
      "\n",
      "\n",
      "        [[[0.3020, 0.3059, 0.3137,  ..., 0.9059, 0.9255, 0.9451],\n",
      "          [0.3373, 0.3412, 0.3451,  ..., 0.8941, 0.9176, 0.9333],\n",
      "          [0.3686, 0.3765, 0.3804,  ..., 0.8902, 0.9059, 0.9255],\n",
      "          ...,\n",
      "          [0.4627, 0.4863, 0.5176,  ..., 0.5608, 0.5804, 0.6235],\n",
      "          [0.4863, 0.5451, 0.5608,  ..., 0.4588, 0.4902, 0.5255],\n",
      "          [0.3608, 0.4039, 0.4510,  ..., 0.2980, 0.3843, 0.4235]],\n",
      "\n",
      "         [[0.3529, 0.3569, 0.3608,  ..., 0.7569, 0.7922, 0.8314],\n",
      "          [0.3882, 0.3961, 0.4000,  ..., 0.7569, 0.7961, 0.8314],\n",
      "          [0.4196, 0.4275, 0.4392,  ..., 0.7647, 0.7922, 0.8314],\n",
      "          ...,\n",
      "          [0.3765, 0.3961, 0.4157,  ..., 0.5020, 0.5412, 0.5961],\n",
      "          [0.3843, 0.4431, 0.4549,  ..., 0.4353, 0.4745, 0.5176],\n",
      "          [0.3529, 0.3882, 0.4235,  ..., 0.3255, 0.4078, 0.4510]],\n",
      "\n",
      "         [[0.3608, 0.3647, 0.3765,  ..., 0.8000, 0.8549, 0.8941],\n",
      "          [0.4078, 0.4118, 0.4235,  ..., 0.8000, 0.8549, 0.8902],\n",
      "          [0.4471, 0.4549, 0.4667,  ..., 0.8039, 0.8471, 0.8941],\n",
      "          ...,\n",
      "          [0.2353, 0.2431, 0.2627,  ..., 0.4039, 0.4275, 0.4706],\n",
      "          [0.2510, 0.3020, 0.3098,  ..., 0.3451, 0.3686, 0.4000],\n",
      "          [0.3020, 0.3294, 0.3529,  ..., 0.2784, 0.3333, 0.3529]]],\n",
      "\n",
      "\n",
      "        [[[0.7843, 0.7529, 0.7608,  ..., 0.7569, 0.7529, 0.7961],\n",
      "          [0.7333, 0.7020, 0.7098,  ..., 0.7059, 0.6980, 0.7490],\n",
      "          [0.7255, 0.6902, 0.6980,  ..., 0.6902, 0.6824, 0.7412],\n",
      "          ...,\n",
      "          [0.6863, 0.6549, 0.6078,  ..., 0.7176, 0.7098, 0.7490],\n",
      "          [0.6627, 0.6667, 0.6667,  ..., 0.7216, 0.7137, 0.7529],\n",
      "          [0.8039, 0.8000, 0.8000,  ..., 0.8000, 0.7922, 0.8118]],\n",
      "\n",
      "         [[0.8157, 0.8000, 0.8039,  ..., 0.7922, 0.7843, 0.8235],\n",
      "          [0.7765, 0.7608, 0.7647,  ..., 0.7529, 0.7451, 0.7804],\n",
      "          [0.7765, 0.7608, 0.7647,  ..., 0.7529, 0.7451, 0.7804],\n",
      "          ...,\n",
      "          [0.6471, 0.6000, 0.5608,  ..., 0.6588, 0.6471, 0.7059],\n",
      "          [0.6235, 0.6118, 0.6118,  ..., 0.6667, 0.6588, 0.7137],\n",
      "          [0.7725, 0.7529, 0.7608,  ..., 0.7647, 0.7569, 0.7804]],\n",
      "\n",
      "         [[0.8824, 0.8588, 0.8549,  ..., 0.8510, 0.8471, 0.8902],\n",
      "          [0.8941, 0.8745, 0.8667,  ..., 0.8667, 0.8588, 0.8745],\n",
      "          [0.8941, 0.8745, 0.8706,  ..., 0.8706, 0.8627, 0.8745],\n",
      "          ...,\n",
      "          [0.6157, 0.5490, 0.5059,  ..., 0.6078, 0.5961, 0.6706],\n",
      "          [0.5882, 0.5647, 0.5725,  ..., 0.6235, 0.6157, 0.6784],\n",
      "          [0.7451, 0.7255, 0.7373,  ..., 0.7333, 0.7294, 0.7490]]]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-4b5089a8e630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0madv_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DL/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "model, data_loader, criterion, steps, max_stepsize, eps, device = model, test_loader, torch.nn.CrossEntropyLoss(), 7, 2/255, 8/255, device\n",
    "\n",
    "model.eval()\n",
    "advs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(data_loader):\n",
    "    print(i)\n",
    "    if i==0:\n",
    "        inputs, labels = data\n",
    "        inputs, labels =inputs.to(device), labels.to(device)\n",
    "        adv_examples = inputs\n",
    "        #adv_inputs = inputs\n",
    "        adv_examples.requires_grad = True\n",
    "        #perturbation = torch.zeros_like(adv_inputs, requires_grad=True).to(device)\n",
    "        for step in range(steps):\n",
    "\n",
    "            perturbation = torch.zeros_like(inputs, requires_grad=True).to(device)\n",
    "            preds = model(adv_examples)\n",
    "            loss = criterion(preds, labels)\n",
    "            adv_examples.retain_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            perturbation = torch.sign(adv_examples.grad).clamp_(-eps, eps)\n",
    "            print(inputs)\n",
    "            adv_examples = adv_examples + perturbation\n",
    "            #adv_examples, pert = FGSM_step(model, adv_examples, labels, criterion, max_stepsize, device)\n",
    "            adv_examples.clamp_(-eps, eps)\n",
    "\n",
    "        advs.append(adv_examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
